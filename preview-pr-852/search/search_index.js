var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Cloud Game Development Toolkit","text":"<p>The Cloud Game Development Toolkit (a.k.a. CGD Toolkit) is a collection of templates and configurations for deploying game development infrastructure and tools on AWS.</p> <p>The project is designed for piecemeal usage:</p> <ul> <li>Already have a CI/CD pipeline deployed but need a build machine image? </li> <li>Looking to migrate your Perforce server from on-premise to AWS? </li> <li>Starting your new studio from the ground up and looking for pre-built templates to deploy common infrastructure? </li> </ul> <p>The Toolkit consists of three key components:</p> Component Description Assets Reusable scripts, pipeline definitions, Dockerfiles, Packer templates, Ansible Playbooks to configure workloads after deployment, and other resources that might prove useful or are dependencies of any of the modules. Modules Highly configurable and extensible Terraform modules for simplified deployment of key game development infrastructure on AWS with best-practices by default. Samples Complete Terraform configurations for expedited studio setup that demonstrate module usage and integration with other AWS services."},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>\ud83d\udcd6 Documentation  |  \ud83d\udcbb Contribute to the Project  |  \ud83d\udcac Ask Questions  |  \ud83d\udea7 Roadmap</p>"},{"location":"index.html#security","title":"Security","text":"<p>If you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions here or email AWS security directly.</p>"},{"location":"index.html#license","title":"License","text":"<p>This project is licensed under the MIT-0 License. See the LICENSE file.</p>"},{"location":"AGENTS.html","title":"Cloud Game Development Toolkit - AI Agent Guide","text":""},{"location":"AGENTS.html#project-overview","title":"Project Overview","text":"<p>The Cloud Game Development Toolkit (CGD Toolkit) is a collection of Terraform modules, scripts, and configurations for deploying game development infrastructure and tools on AWS. The project enables game studios to deploy production-ready infrastructure for Perforce, Jenkins, Unreal Engine Horde, VDI workstations, and other game development tools.</p>"},{"location":"AGENTS.html#project-structure","title":"Project Structure","text":"<pre><code>cloud-game-development-toolkit/\n\u251c\u2500\u2500 assets/              # Reusable scripts, Packer templates, Ansible playbooks\n\u251c\u2500\u2500 modules/             # Terraform modules for game dev infrastructure\n\u2502   \u251c\u2500\u2500 jenkins/        # Jenkins CI/CD infrastructure\n\u2502   \u251c\u2500\u2500 perforce/       # Perforce version control\n\u2502   \u251c\u2500\u2500 teamcity/       # TeamCity CI/CD infrastructure\n\u2502   \u251c\u2500\u2500 unity/          # Unity-specific tools\n\u2502   \u251c\u2500\u2500 unreal/         # Unreal Engine tools (Horde, Cloud DDC)\n\u2502   \u2514\u2500\u2500 vdi/            # Virtual desktop infrastructure\n\u251c\u2500\u2500 samples/            # Complete Terraform configurations\n\u2514\u2500\u2500 docs/               # Documentation source\n</code></pre>"},{"location":"AGENTS.html#key-technologies","title":"Key Technologies","text":"<ul> <li>Terraform: Infrastructure as Code (IaC) for AWS resource provisioning</li> <li>AWS: Cloud infrastructure provider</li> <li>Packer: Machine image building</li> <li>Ansible: Configuration management</li> <li>Docker: Container images for services</li> </ul>"},{"location":"AGENTS.html#design-philosophy","title":"Design Philosophy","text":""},{"location":"AGENTS.html#1-modularity-and-flexibility","title":"1. Modularity and Flexibility","text":"<p>Modules are designed as building blocks, not complete solutions. Users compose modules to fit their specific needs rather than being forced into opinionated architectures.</p> <p>Key Principles:</p> <ul> <li>Modules provide infrastructure components, not complete solutions</li> <li>Configuration decisions happen in examples, not module internals</li> <li>Support multiple deployment patterns through simple variables</li> <li>Enable customization without requiring module forking</li> </ul>"},{"location":"AGENTS.html#2-conservative-variable-exposure","title":"2. Conservative Variable Exposure","text":"<p>Every exposed variable is a commitment to backward compatibility. We start with minimal variables based on known use cases and add more when users request them.</p> <p>Guidelines:</p> <ul> <li>Start with minimal variables</li> <li>Add variables based on user demand (demand-driven)</li> <li>Default values should work for 80% of use cases</li> <li>Easier to add than remove (breaking changes are painful)</li> </ul>"},{"location":"AGENTS.html#3-security-by-default","title":"3. Security by Default","text":"<p>Game development infrastructure often handles sensitive assets and player data. Security mistakes are costly and hard to fix later.</p> <p>Security Patterns:</p> <ul> <li>No <code>0.0.0.0/0</code> ingress rules in module code (users explicitly define allowed access)</li> <li>Private-first architecture with controlled external access</li> <li>HTTPS enforcement for internet-facing services</li> <li>User-controlled security groups with their own rules</li> </ul>"},{"location":"AGENTS.html#4-readability-first","title":"4. Readability First","text":"<p>Game development teams often include infrastructure newcomers. Clear, understandable code reduces onboarding time and prevents misconfigurations.</p> <p>Code Standards:</p> <ul> <li>Prefer explicit over implicit configurations</li> <li>Use descriptive variable names that explain purpose</li> <li>Self-documenting code over clever abstractions</li> <li>Comment complex logic with business context</li> </ul>"},{"location":"AGENTS.html#module-design-standards","title":"Module Design Standards","text":""},{"location":"AGENTS.html#naming-conventions","title":"Naming Conventions","text":"<p>Use descriptive, purpose-driven resource names:</p> <pre><code># \u2705 GOOD - Descriptive names\nresource \"aws_lb\" \"nlb\" { }                    # Network Load Balancer\nresource \"aws_lb\" \"alb\" { }                    # Application Load Balancer\nresource \"aws_security_group\" \"internal\" { }   # Internal communication\n\n# \u274c BAD - Generic names\nresource \"aws_lb\" \"this\" { }\nresource \"aws_lb\" \"this2\" { }\n</code></pre>"},{"location":"AGENTS.html#variable-structure","title":"Variable Structure","text":"<p>Use a hybrid approach:</p> <ul> <li>Flat variables for simple, common settings</li> <li>Complex objects for logical grouping when they provide clear value</li> <li>Submodule alignment - Complex objects that map directly to submodules</li> </ul> <pre><code># Flat for simple settings\nvariable \"vpc_id\" {\n  type        = string\n  description = \"VPC ID for deployment\"\n}\n\n# Complex objects for logical grouping\nvariable \"load_balancer_config\" {\n  type = object({\n    nlb = object({\n      enabled         = optional(bool, true)\n      internet_facing = optional(bool, true)\n      subnets        = list(string)\n    })\n  })\n}\n</code></pre>"},{"location":"AGENTS.html#resource-patterns","title":"Resource Patterns","text":"<p>Prefer direct resources over remote modules:</p> <pre><code># \u2705 PREFERRED - Direct resource creation\nresource \"aws_eks_cluster\" \"main\" {\n  name     = local.cluster_name\n  role_arn = aws_iam_role.cluster.arn\n  # Direct configuration gives full control\n}\n\n# \u274c AVOID - Remote module dependency\nmodule \"eks\" {\n  source = \"registry.terraform.io/example/eks/aws\"\n  # Adds complexity, version dependencies, limited customization\n}\n</code></pre> <p>When remote modules are needed, fork them first for full control over changes and updates.</p>"},{"location":"AGENTS.html#testing-strategy","title":"Testing Strategy","text":""},{"location":"AGENTS.html#unit-tests-with-mocked-providers","title":"Unit Tests with Mocked Providers","text":"<p>All modules use Terraform's native test framework with mocked providers to validate module logic without creating actual AWS resources.</p> <p>Benefits:</p> <ul> <li>Zero AWS costs (no resources created)</li> <li>No cleanup required</li> <li>No AWS credentials needed</li> <li>Fast execution (seconds)</li> <li>CI-ready without authentication</li> </ul> <p>Test Structure:</p> <pre><code># Mock AWS provider\nmock_provider \"aws\" {\n  mock_data \"aws_region\" {\n    defaults = { name = \"us-east-1\" }\n  }\n}\n\n# Unit test\nrun \"unit_test_scenario\" {\n  command = plan\n\n  variables {\n    vpc_id = \"vpc-test123\"\n    # Test values directly in file\n  }\n\n  assert {\n    condition     = length(aws_resource.main) &gt; 0\n    error_message = \"Resource should be created\"\n  }\n}\n</code></pre> <p>What Gets Tested:</p> <ul> <li>\u2705 Variable validation logic</li> <li>\u2705 Conditional resource creation</li> <li>\u2705 Resource count calculations</li> <li>\u2705 Dynamic block logic</li> <li>\u274c Actual AWS resource creation</li> <li>\u274c AWS API behavior</li> </ul> <p>Running Tests:</p> <pre><code>cd modules/{module-name}\nterraform init\nterraform test\n</code></pre> <p>See <code>TERRAFORM_TESTING_STRATEGY.md</code> for comprehensive testing guidelines.</p>"},{"location":"AGENTS.html#documentation-standards","title":"Documentation Standards","text":""},{"location":"AGENTS.html#module-documentation","title":"Module Documentation","text":"<p>Each module must include:</p> <ol> <li>README.md - Module overview, usage examples, input/output documentation</li> <li>examples/ - Working example configurations</li> <li>tests/ - Terraform test files with mocked providers</li> </ol>"},{"location":"AGENTS.html#test-documentation","title":"Test Documentation","text":"<p>Test directories should include:</p> <ul> <li>README.md - Test scenarios, what gets tested, how to run</li> <li>QUICKSTART.md - 2-minute quick start guide (optional for complex modules)</li> </ul>"},{"location":"AGENTS.html#avoid-documentation-proliferation","title":"Avoid Documentation Proliferation","text":"<ul> <li>Don't create multiple README files for the same purpose</li> <li>Consolidate related documentation</li> <li>Use existing documentation locations (CONTRIBUTING.md, DESIGN_STANDARDS.md, etc.)</li> <li>Reference external documentation rather than duplicating it</li> </ul>"},{"location":"AGENTS.html#development-workflow","title":"Development Workflow","text":""},{"location":"AGENTS.html#making-changes","title":"Making Changes","text":"<ol> <li>Fork and Branch: Create a feature branch from <code>main</code></li> <li>Follow Standards: Adhere to design standards in <code>modules/DESIGN_STANDARDS.md</code></li> <li>Write Tests: Add unit tests with mocked providers</li> <li>Test Locally: Run <code>terraform test</code> to validate changes</li> <li>Document: Update README and examples as needed</li> <li>Commit: Use conventional commit messages</li> <li>Pull Request: Submit PR with clear description</li> </ol>"},{"location":"AGENTS.html#conventional-commits","title":"Conventional Commits","text":"<p>PR titles must follow conventional commit format:</p> <pre><code>feat(module): add new feature\nfix(module): resolve issue\ndocs(module): update documentation\ntest(module): add tests\n</code></pre>"},{"location":"AGENTS.html#pre-commit-checks","title":"Pre-commit Checks","text":"<p>The project uses pre-commit hooks for:</p> <ul> <li>Terraform formatting (<code>terraform fmt</code>)</li> <li>Terraform validation</li> <li>Security scanning (Checkov)</li> <li>Documentation generation</li> </ul>"},{"location":"AGENTS.html#common-patterns","title":"Common Patterns","text":""},{"location":"AGENTS.html#networking","title":"Networking","text":"<p>3-Tier Architecture:</p> <ul> <li><code>application_subnets</code> - Primary business applications</li> <li><code>service_subnets</code> - Supporting services (databases, caches)</li> <li><code>load_balancer_config</code> - Load balancer configuration</li> </ul> <p>Load Balancer Strategy:</p> <ul> <li>Default to NLB for most services</li> <li>ALB when needed for HTTP/HTTPS routing</li> <li>User controls creation via boolean flags</li> </ul> <p>DNS Patterns:</p> <ul> <li>Regional endpoints by default (<code>us-east-1.service.company.com</code>)</li> <li>Private zones for internal service discovery</li> <li>Global endpoints optional for advanced routing</li> </ul>"},{"location":"AGENTS.html#security-groups","title":"Security Groups","text":"<pre><code>variable \"security_groups\" {\n  type        = list(string)\n  description = \"Security group IDs for external access\"\n  default     = []\n}\n\n# Module creates internal security groups\nresource \"aws_security_group\" \"internal\" {\n  name_prefix = \"${local.name_prefix}-internal-\"\n  vpc_id      = var.vpc_id\n}\n</code></pre>"},{"location":"AGENTS.html#iam-roles-and-policies","title":"IAM Roles and Policies","text":"<pre><code>variable \"create_default_role\" {\n  type        = bool\n  description = \"Create default IAM role\"\n  default     = true\n}\n\nvariable \"custom_role_arn\" {\n  type        = string\n  description = \"Custom IAM role ARN (if not using default)\"\n  default     = null\n}\n</code></pre>"},{"location":"AGENTS.html#ai-agent-guidelines","title":"AI Agent Guidelines","text":""},{"location":"AGENTS.html#when-helping-with-module-development","title":"When Helping with Module Development","text":"<ol> <li>Read Design Standards First: Always reference <code>modules/DESIGN_STANDARDS.md</code></li> <li>Follow Naming Conventions: Use descriptive resource names, not generic ones</li> <li>Test with Mocked Providers: Create unit tests, not integration tests</li> <li>Avoid Remote Modules: Prefer direct resources unless there's a compelling reason</li> <li>Security First: Never add <code>0.0.0.0/0</code> ingress rules without explicit user request</li> </ol>"},{"location":"AGENTS.html#when-writing-tests","title":"When Writing Tests","text":"<ol> <li>Use Mocked Providers: Always use <code>mock_provider</code> blocks</li> <li>Test Logic, Not AWS: Focus on conditionals, counts, validation</li> <li>Provide Test Values: Include test values directly in test files</li> <li>Add Assertions: Test expected behavior with clear error messages</li> <li>No Integration Tests: Don't create tests that require actual AWS resources</li> </ol>"},{"location":"AGENTS.html#when-creating-documentation","title":"When Creating Documentation","text":"<ol> <li>Consolidate: Don't create multiple README files for the same purpose</li> <li>Reference Existing: Link to existing documentation rather than duplicating</li> <li>Be Concise: Focus on essential information</li> <li>Use Examples: Show, don't just tell</li> <li>Update Existing: Enhance existing docs rather than creating new ones</li> </ol>"},{"location":"AGENTS.html#when-reviewing-code","title":"When Reviewing Code","text":"<ol> <li>Check Standards: Verify adherence to design standards</li> <li>Validate Tests: Ensure tests use mocked providers</li> <li>Review Security: Check for security anti-patterns</li> <li>Assess Complexity: Ensure code is readable and maintainable</li> <li>Verify Documentation: Confirm README and examples are updated</li> </ol>"},{"location":"AGENTS.html#resources","title":"Resources","text":"<ul> <li>Documentation: https://aws-games.github.io/cloud-game-development-toolkit/</li> <li>Design Standards: <code>modules/DESIGN_STANDARDS.md</code></li> <li>Contributing: <code>CONTRIBUTING.md</code></li> <li>Discussions: https://github.com/aws-games/cloud-game-development-toolkit/discussions/</li> <li>Roadmap: https://github.com/orgs/aws-games/projects/1/views/1</li> </ul>"},{"location":"AGENTS.html#agent-specific-context","title":"Agent-Specific Context","text":"<p>Different AI agents may have additional context files:</p> <ul> <li>Kiro:</li> <li><code>.kiro/steering/*.md</code> - Automatically loaded steering files (project context)</li> <li><code>.kiro/rules/*.md</code> - Kiro-specific rules and workflows</li> </ul>"},{"location":"AGENTS.html#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Use GitHub Discussions</li> <li>Bugs: File GitHub Issues</li> <li>Security: Follow AWS vulnerability reporting process</li> <li>Contributing: See CONTRIBUTING.md</li> </ul> <p>Note for AI Agents: This project prioritizes readability, security, and modularity. When in doubt, favor explicit over implicit, simple over clever, and secure by default. Always test with mocked providers and follow the established design patterns.</p>"},{"location":"CHANGELOG.html","title":"CHANGELOG","text":""},{"location":"CHANGELOG.html#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG.html#latest-2025-07-29","title":"latest - 2025-07-29","text":""},{"location":"CHANGELOG.html#v115-2025-07-29","title":"v1.1.5 - 2025-07-29","text":""},{"location":"CHANGELOG.html#bug-fixes","title":"Bug Fixes","text":"<ul> <li>fixes button/link to getting started page in homepage</li> <li>update perforce tf test to support branches on forks as well as source repo</li> <li>Minor Perforce module copy/paste naming resolution (#645)</li> <li>Fixes typo in code block in samples DDC readme</li> <li>hardcode protocol to appease checkov</li> <li>Update SG reference in P4 FSxN Example (#640)</li> <li>p4: Fix typo'ed output.shared_application_load_balancer_arn</li> </ul>"},{"location":"CHANGELOG.html#chore","title":"Chore","text":"<ul> <li>remove kevon from description :( (#672)</li> <li>update dependabot configuration</li> <li>deps: bump the random-provider group across 9 directories with 1 update (#653)</li> <li>deps: bump NetApp/netapp-ontap</li> <li>deps: bump NetApp/netapp-ontap in /samples/simple-build-pipeline</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump the aws-provider group across 8 directories with 1 update</li> <li>deps: bump the aws-provider group across 8 directories with 1 update (#662)</li> <li>deps: bump hashicorp/local in /modules/perforce/modules/p4-server</li> <li>deps: bump hashicorp/local in /modules/perforce</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump ossf/scorecard-action from 2.4.1 to 2.4.2</li> <li>deps: bump aquasecurity/trivy-action from 0.30.0 to 0.31.0</li> </ul>"},{"location":"CHANGELOG.html#docs","title":"Docs","text":"<ul> <li>fixed broken links in getting started guide</li> <li>horde: add alb http listeners to README.md</li> </ul>"},{"location":"CHANGELOG.html#features","title":"Features","text":"<ul> <li>Packer template for Cloud Game Development virtual workstation AMI (#651)</li> <li>Unity Accelerator asset caching proxy</li> <li>horde: allow users to bring their own horde-server images (#643)</li> <li>horde: add HTTP redirect listeners to ALBs</li> <li>p4: allow users to specify a private ip (#665)</li> <li>p4: p4_configure.sh attempts to use --fqdn if passed (#666)</li> </ul>"},{"location":"CHANGELOG.html#v114-2025-06-09","title":"v1.1.4 - 2025-06-09","text":""},{"location":"CHANGELOG.html#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Remove commented out NetApp volume resources and cleanup IAM managed policies</li> <li>Resolved EC2 DNS self-signed certificate bug in P4 Server packer template</li> <li>Adding cloud DDC sample for mkdocs.yml</li> <li>helix swarm: helix swarm does not support horizontal scaling, so helix swarm container count is now set to 1</li> </ul>"},{"location":"CHANGELOG.html#chore_1","title":"Chore","text":"<ul> <li>Add Terraform tests for new Perforce module (#604)</li> <li>regenerate CHANGELOG.md for 2025-03-19</li> <li>Minor maintenance to Helix Core module</li> <li>Minor Helix Authentication fixes</li> <li>regenerate CHANGELOG.md for 2025-06-09</li> <li>Addressed IAM policy warnings for Helix Swarm</li> <li>deps: bump actions/github-script from 6 to 7</li> <li>deps: bump mkdocs-material from 9.6.11 to 9.6.12 in /docs</li> <li>deps: bump xt0rted/pull-request-comment-branch from 1 to 3</li> <li>deps: bump actions/checkout from 3 to 4</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump mkdocs-material from 9.6.9 to 9.6.11 in /docs</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump aquasecurity/trivy-action from 0.29.0 to 0.30.0</li> <li>deps: bump mkdocs-material from 9.6.8 to 9.6.9 in /docs</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.8 to 9.6.9 in /docs</li> <li>deps: bump actions/upload-artifact from 4.6.1 to 4.6.2</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.12 to 9.6.14 in /docs</li> </ul>"},{"location":"CHANGELOG.html#code-refactoring","title":"Code Refactoring","text":"<ul> <li>Update Simple Build Pipeline sample to use new Perforce parent module (#608)</li> <li>Perforce modules consolidated to simplify shared resource creation (#585)</li> <li>Updated Perforce complete example to remove NLB front for Helix Core</li> <li>reorganize unreal cloud ddc module structure</li> </ul>"},{"location":"CHANGELOG.html#docs_1","title":"Docs","text":"<ul> <li>Adjustments to mkdocs structure, and updates to \"getting started\" and Perforce documentation. (#612)</li> <li>updates and expands on <code>unreal-cloud-ddc-intra-cluster</code> installation and usage docs</li> <li>fixes relative path for <code>unreal-cloud-ddc-infra</code> and <code>unreal-cloud-ddc-intra-cluster</code> Terraform module docs</li> <li>add unreal fest video to horde module</li> <li>TeamCity: Adding TeamCity module docs and example architecture</li> </ul>"},{"location":"CHANGELOG.html#features_1","title":"Features","text":"<ul> <li>Adds debug variable and flag</li> <li>Simple example deployment of Helix Core backed by FSxN</li> <li>FSxN ISCSI provisioning for Helix Core module</li> <li>Modified p4_configure.sh to mount ISCSI volumes from FSxN</li> </ul>"},{"location":"CHANGELOG.html#v113-alpha-2025-03-19","title":"v1.1.3-alpha - 2025-03-19","text":""},{"location":"CHANGELOG.html#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>create_external_alb shouldn't block internal SG Ingress rules</li> <li>alb_subnet variables should not be required if create boolean is false</li> <li>Attaching perforce web service ALB to target group</li> <li>use provided admin password secret for Helix Authentication Service ADMIN_PASSWD, instead of the username secret</li> <li>AMI version bump for Helix Core, region variable made optional</li> </ul>"},{"location":"CHANGELOG.html#chore_2","title":"Chore","text":"<ul> <li>update dependabot configuration to include unreal modules</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.4 to 9.6.7 in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.4 to 9.6.7 in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump actions/upload-artifact from 4.6.0 to 4.6.1</li> <li>deps: bump ossf/scorecard-action from 2.4.0 to 2.4.1</li> <li>deps: bump hashicorp/random</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.7 to 9.6.8 in /docs</li> <li>deps: bump the random-provider group across 4 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.3 to 9.6.4 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.2 to 9.6.4 in /docs</li> <li>deps: bump actions/upload-artifact from 4.5.0 to 4.6.0</li> <li>deps: bump mkdocs-material from 9.6.7 to 9.6.8 in /docs</li> <li>deps: bump hashicorp/aws</li> <li>deps: bump mkdocs-material from 9.6.2 to 9.6.3 in /docs</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.1 to 9.6.2 in /docs</li> <li>deps: bump mkdocs-material from 9.5.50 to 9.6.2 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump release-drafter/release-drafter from 6.0.0 to 6.1.0</li> <li>deps: bump mkdocs-material from 9.5.49 to 9.5.50 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> </ul>"},{"location":"CHANGELOG.html#docs_2","title":"Docs","text":"<ul> <li>fix broken link in readme</li> <li>Perforce: Updating documentation for Perforce Complete example reference architecture</li> </ul>"},{"location":"CHANGELOG.html#features_2","title":"Features","text":"<ul> <li>Helix Authentication Service: Shifting ALB creation to support external networking configuration</li> <li>Helix Core: Plaintext support for Helix Core, optional EIP creation</li> <li>Helix Core: Adding plaintext variable to p4_configre.sh</li> <li>Helix Swarm: Shifting ALB creation to support external networking configuration</li> <li>Perforce Example: Update complete example for shared networking configuration across services</li> <li>TeamCity Example: example terraform configuration for deploying TeamCity module</li> <li>TeamCity Server: terraform module for deploying TeamCity server on ECS Fargate</li> </ul>"},{"location":"CHANGELOG.html#v112-alpha-2024-12-20","title":"v1.1.2-alpha - 2024-12-20","text":""},{"location":"CHANGELOG.html#chore_3","title":"Chore","text":"<ul> <li>regenerate CHANGELOG.md for 2024-12-20</li> <li>ignore tf backend.tf files in .gitignore</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump actions/upload-artifact from 4.4.3 to 4.5.0</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> </ul>"},{"location":"CHANGELOG.html#docs_3","title":"Docs","text":"<ul> <li>removed READMEs from source directories and moved them to their own dedicated docs pages in docs/ dir</li> <li>update contributor documentation to include table of contents</li> <li>updates to doc formatting and fixed broken links</li> </ul>"},{"location":"CHANGELOG.html#v111-alpha-2024-12-17","title":"v1.1.1-alpha - 2024-12-17","text":""},{"location":"CHANGELOG.html#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>Added service target group ARNs as outputs for HAS and Swarm</li> <li>Adds defaults to <code>vpc_id</code> and <code>subnet_id</code> variables</li> <li>bash error causing build failure when running p4_configure.sh (#367)</li> <li>horde: add JwtIssuer to ensure container retains agents on restart</li> <li>horde: allow inbound access to horde agents on ports 7000-7010 from other horde agents</li> <li>perforce: fixed minor issues in p4_configure.sh</li> <li>perforce: add Unicode support and fix main module to handle existing security groups</li> </ul>"},{"location":"CHANGELOG.html#chore_4","title":"Chore","text":"<ul> <li>make SELinux label updates configurable</li> <li>remove packer assets .ci directory (#337)</li> <li>fix tag names so that they match recommended best practices (#343)</li> <li>define nat gateway routes for private route tables outside of aws_route_table resources in samples and modules (#354)</li> <li>adds triage label to our issue templates</li> <li>regenerate CHANGELOG.md for 2024-12-17</li> <li>document parameter values for '--unicode' flag</li> <li>provide appropriate association name for configuring Helix Core via SSM</li> <li>fix naming</li> <li>checkov: Suppresses CKV_AWS_378 rule (#339)</li> <li>deps: bump mkdocs-material from 9.5.42 to 9.5.44 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump aquasecurity/trivy-action from 0.28.0 to 0.29.0</li> <li>deps: bump mkdocs-material from 9.5.45 to 9.5.46 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.5.44 to 9.5.45 in /docs</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.7 to 1.0.8 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump actions/checkout from 3.0.0 to 4.2.2</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump hashicorp/setup-terraform from 1 to 3</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump mkdocs-material from 9.5.41 to 9.5.42 in /docs</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.6 to 1.0.7 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump aquasecurity/trivy-action from 0.24.0 to 0.28.0</li> <li>deps: bump mkdocs-material from 9.5.40 to 9.5.41 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump python from 3.12.7 to 3.13.0 in /docs (#349)</li> <li>deps: bump actions/upload-artifact from 4.4.0 to 4.4.3 (#356)</li> <li>deps: bump mkdocs-material from 9.5.39 to 9.5.40 in /docs (#359)</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.5 to 1.0.6 in /docs (#345)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.5.37 to 9.5.39 in /docs (#335)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#344)</li> <li>deps: bump mkdocs-material from 9.5.46 to 9.5.48 in /docs</li> <li>deps: bump python from 3.12.6 to 3.12.7 in /docs (#340)</li> <li>deps: bump mkdocs-material from 9.5.48 to 9.5.49 in /docs</li> <li>deps: bump python from 3.13.0 to 3.13.1 in /docs</li> </ul>"},{"location":"CHANGELOG.html#docs_4","title":"Docs","text":"<ul> <li>clarify that modules are intended to be depended on, and samples are reference implementations meant to be copied and modified</li> <li>fix formatting of simple build pipeline docs</li> <li>fix formatting of local.tf in simple build pipeline docs</li> <li>fix formatting of jenkins pipeline assets page</li> <li>clarify use case of Ansible playbooks vs Packer templates</li> <li>clarify that deploying multiple samples independently is not supported</li> <li>point users explicitly to a Classic GitHub Personal Access Token</li> <li>fix typo in getting started guide</li> <li>Updates the getting started instructions for the simple build pipeline sample</li> </ul>"},{"location":"CHANGELOG.html#features_3","title":"Features","text":"<ul> <li>perforce: implement Helix Core setup playbook</li> </ul>"},{"location":"CHANGELOG.html#v110-alpha-2024-10-01","title":"v1.1.0-alpha - 2024-10-01","text":""},{"location":"CHANGELOG.html#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>improve stability of build agent packer scripts, adjust winrm timeout to 15 minutes, remove packer variables that aren't needed (#318)</li> </ul>"},{"location":"CHANGELOG.html#chore_5","title":"Chore","text":"<ul> <li>update changelog (#305)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#323)</li> <li>deps: bump mkdocs-material from 9.5.35 to 9.5.37 in /docs (#314)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#324)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#298)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#291)</li> <li>deps: bump the random-provider group across 5 directories with 1 update (#310)</li> <li>deps: bump mkdocs-material from 9.5.34 to 9.5.35 in /docs (#287)</li> </ul>"},{"location":"CHANGELOG.html#docs_5","title":"Docs","text":"<ul> <li>add perforce complete example in docs (#333)</li> <li>updates to documentation (#329)</li> </ul>"},{"location":"CHANGELOG.html#features_4","title":"Features","text":"<ul> <li>install requirements for (auto)mounting FSx volumes on Jenkins Windows build agents (#319)</li> <li>helix-core: add ARM64 support (#239)</li> </ul>"},{"location":"CHANGELOG.html#v101-alpha-2024-09-16","title":"v1.0.1-alpha - 2024-09-16","text":""},{"location":"CHANGELOG.html#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>changelog automation (#261)</li> <li>adding branch creation to workflow (#259)</li> <li>dependabot grouping terraform providers (#228)</li> <li>wait for cloud-init to complete prior to installing packages during Perforce Helix Core AMI creation (#193)</li> <li>changelog: GHA bot committer (#255)</li> <li>changelog: Add automated PR creation (#252)</li> <li>fsx_automounter: when FSx automounter can't list tags for an FSx volume, the AccessDenied exception is now treated as a warning (#226)</li> <li>p4_configure: resolve script execution errors and repair broken \u2026 (#232)</li> </ul>"},{"location":"CHANGELOG.html#chore_6","title":"Chore","text":"<ul> <li>adjusting changelog automation to leverage GH api (#266)</li> <li>update changelog workflow (#284)</li> <li>update changelog (#285)</li> <li>deps: bump hashicorp/awscc from 1.10.0 to 1.11.0 in /samples/simple-build-pipeline (#220)</li> <li>deps: bump hashicorp/awscc from 1.9.0 to 1.10.0 in /modules/perforce/helix-core (#207)</li> <li>deps: bump mkdocs-material from 9.5.33 to 9.5.34 in /docs (#236)</li> <li>deps: bump actions/upload-artifact from 4.3.6 to 4.4.0 (#235)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#241)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#242)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#233)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#231)</li> <li>deps: bump mkdocs-material from 9.5.32 to 9.5.33 in /docs (#229)</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.3 to 1.0.5 in /docs (#263)</li> <li>deps: bump mkdocs-material from 9.5.31 to 9.5.32 in /docs (#211)</li> <li>deps: bump python from 3.12 to 3.12.6 in /docs (#243)</li> <li>deps: bump hashicorp/awscc from 1.9.0 to 1.10.0 in /modules/perforce/helix-authentication-service (#205)</li> <li>deps: bump hashicorp/aws from 5.62.0 to 5.63.1 in /samples/simple-build-pipeline (#216)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /modules/perforce/helix-authentication-service (#196)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-authentication-service (#197)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /modules/perforce/helix-core (#198)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-core (#199)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-swarm (#200)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /samples/simple-build-pipeline (#201)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /samples/simple-build-pipeline (#202)</li> <li>deps: bump mike from 2.1.2 to 2.1.3 in /docs (#189)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/jenkins (#195)</li> </ul>"},{"location":"CHANGELOG.html#docs_6","title":"Docs","text":"<ul> <li>add openssf scorecard badge to readme (#219)</li> <li>link to installation instructions for required tools, fix packer command invocation instructions (#194)</li> <li>Windows Build AMI README (#187)</li> </ul>"},{"location":"CHANGELOG.html#v100-alpha-2024-08-07","title":"v1.0.0-alpha - 2024-08-07","text":""},{"location":"CHANGELOG.html#staging-2024-08-07","title":"staging - 2024-08-07","text":""},{"location":"CHANGELOG.html#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>fix issue where SSH public key was not baked into the Windows Jenkins build agent AMI (#150)</li> <li>bug fixes for FSxZ storage in build farm (#152)</li> <li>allow Jenkins build agents to discover FSx volumes/snapshots and make outbound Internet connections (#147)</li> </ul>"},{"location":"CHANGELOG.html#chore_7","title":"Chore","text":"<ul> <li>add CODEOWNERS file (#132)</li> <li>Updates to docs (#63)</li> <li>fix makefile (#65)</li> <li>Modify version handling in Docs (#66)</li> <li>deps: bump mkdocs-material from 9.5.27 to 9.5.28 in /docs (#135)</li> <li>deps: bump mkdocs-material from 9.5.26 to 9.5.27 in /docs (#77)</li> <li>deps: bump aquasecurity/trivy-action from 0.23.0 to 0.24.0 (#137)</li> <li>deps: bump actions/upload-artifact from 4.3.3 to 4.3.4 (#136)</li> <li>deps: bump actions/upload-artifact from 4.3.5 to 4.3.6 (#178)</li> <li>deps: bump mkdocs-material from 9.5.29 to 9.5.30 in /docs (#153)</li> <li>deps: bump mike from 2.1.1 to 2.1.2 in /docs (#110)</li> <li>deps: bump mkdocs-material from 9.5.28 to 9.5.29 in /docs (#144)</li> <li>deps: bump github/codeql-action from 3.25.8 to 3.25.10 (#69)</li> <li>deps: bump ossf/scorecard-action from 2.3.3 to 2.4.0 (#167)</li> <li>deps: bump actions/upload-artifact from 4.3.4 to 4.3.5 (#171)</li> <li>deps: bump mkdocs-material from 9.5.30 to 9.5.31 in /docs (#172)</li> <li>deps: bump github/codeql-action from 3.24.9 to 3.25.8 (#53)</li> <li>deps: bump mkdocs-material from 9.5.25 to 9.5.26 in /docs (#54)</li> </ul>"},{"location":"CHANGELOG.html#code-refactoring_1","title":"Code Refactoring","text":"<ul> <li>Perforce Helix Core AMI revamp, simple build pipeline DNS (#73)</li> </ul>"},{"location":"CHANGELOG.html#docs_7","title":"Docs","text":"<ul> <li>update changelog (#181)</li> <li>update main docs page (#179)</li> <li>update layout of documentation main page theme (#175)</li> <li>update documentation (#163)</li> <li>update workflow for docs (#129)</li> <li>update workflow (#128)</li> <li>fix workflow to use gh inputs from workflow (#127)</li> <li>update to docs and flip release workflow to manual (#126)</li> <li>fix commit depth (#125)</li> <li>modify the workflow for docs release and update documentation (#124)</li> <li>fix docs ci (#123)</li> <li>modify git fetch-depth for docs ci (#121)</li> <li>update README.md (#119)</li> <li>consolidate Ansible playbooks under assets (#117)</li> <li>fix url to documentation to point to /latest (#80)</li> <li>add GH Pull Request template (#67)</li> <li>updates workflow and adds changelog automation (#61)</li> <li>add issue template for RFCs (#57)</li> <li>add git-chglog for changelog generation (#49)</li> <li>enable workflow dispatch (#36)</li> <li>fix docs release workflow (#34)</li> <li>convert docs releases to use mike (#33)</li> <li>adds markdown docs for assets, modules, playbooks, and samples (#32)</li> <li>adds issue template for submitting maintenance issues (#31)</li> <li>Adds documentation and GH workflow for build/publish of docs (#21)</li> <li>Updates to project README (#20)</li> <li>Adds project docs (#13)</li> </ul>"},{"location":"CHANGELOG.html#features_5","title":"Features","text":"<ul> <li>Added getting-started documentation for quickstart with Simple Build Pipeline (#177)</li> <li>Updates to CI configurations for pre-commit and GHA (#154)</li> <li>Helix Authentication Extension (#82)</li> <li>enable web based administration through variables for HAS (#79)</li> <li>complete sample with both Jenkins and Perforce modules (#60)</li> <li>Add packer build agent templates for Linux (Ubuntu Jammy 22.04, Amazon Linux 2023) (#46)</li> <li>devops: Add new DevOps playbook files (#76)</li> <li>packer: switch AMI from Rocky Linux to Amazon Linux 2023 and up\u2026 (#141)</li> </ul>"},{"location":"CODE_OF_CONDUCT.html","title":"CODE OF CONDUCT","text":""},{"location":"CODE_OF_CONDUCT.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"CONTRIBUTING.html","title":"Table of contents","text":"<ul> <li>Contributing Guidelines</li> <li>Reporting Bugs/Feature Requests</li> <li>Contributing via Pull Requests</li> <li>Conventional Commits</li> <li>Finding contributions to work on</li> <li>Code of Conduct</li> <li>Security issue notifications</li> <li>Licensing</li> </ul>"},{"location":"CONTRIBUTING.html#contributing-guidelines","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"CONTRIBUTING.html#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"CONTRIBUTING.html#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages. See the conventional commits section for more details.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p>"},{"location":"CONTRIBUTING.html#conventional-commits","title":"Conventional Commits","text":"<p>This project uses Conventional Commits following the release of v1.0.0-alpha. These conventions ensure that the commit history of the project remains readable, and supports extensive automation around pull request creation, release cadence, and documentation.</p> <p>We do not enforce conventional commits on contributors. We do require that pull request titles follow convention so that the changelog and release automation work as expected.</p>"},{"location":"CONTRIBUTING.html#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"CONTRIBUTING.html#building-and-testing-project-documentation","title":"Building and Testing Project Documentation","text":"<p>This project uses Material for MkDocs to generate documentation. Content is sourced from markdown files throughout the project, including module README.md files.</p>"},{"location":"CONTRIBUTING.html#local-development","title":"Local Development","text":"<p>For day-to-day documentation work, use the container-based local development server:</p> <pre><code>make docs-serve\n</code></pre> <p>This command:</p> <ul> <li>Builds a container image with all required dependencies</li> <li>Starts a local development server at http://127.0.0.1:8000</li> <li>Provides live reload - changes appear automatically in your browser</li> <li>Requires no Python installation or VERSION/ALIAS parameters</li> <li>Works with Docker (default) or Finch by setting <code>CONTAINER_RUNTIME=finch</code> in your environment.</li> </ul> <p>To build the documentation without serving it:</p> <pre><code>make docs-build\n</code></pre> <p>This validates that your changes build successfully using <code>mkdocs build --strict</code> in a container. The built site will be in the <code>./site</code> directory.</p>"},{"location":"CONTRIBUTING.html#pre-commit-validation","title":"Pre-commit Validation","text":"<p>Documentation is validated automatically before commit:</p> <ul> <li>Link checking: Verifies all links in staged markdown files</li> <li>Markdown linting: Ensures consistent markdown formatting</li> </ul> <p>Install pre-commit hooks:</p> <pre><code>pre-commit install\n</code></pre> <p>Optional - Pretty Formatter for Markdownlint:</p> <p>For better-looking markdown linting output, you can install the pretty formatter:</p> <pre><code>npm install -g markdownlint-cli2-formatter-pretty\n</code></pre> <p>This is optional but provides cleaner, color-coded output. The linting will work without it.</p>"},{"location":"CONTRIBUTING.html#pull-request-validation","title":"Pull Request Validation","text":"<p>When you open a PR with documentation changes:</p> <ol> <li>Changed files are validated (links and markdown linting)</li> <li>Full documentation site is built to ensure integrity</li> <li>Preview deployment is created at <code>preview/pr-{number}</code></li> <li>Comment is posted with preview URL</li> </ol>"},{"location":"CONTRIBUTING.html#main-branch-deployment","title":"Main Branch Deployment","text":"<p>When your PR is merged to main:</p> <ul> <li>Documentation automatically deploys to \"latest\"</li> <li>Full regression testing runs (all files validated)</li> <li>Issues are created if validation fails</li> </ul>"},{"location":"CONTRIBUTING.html#versioned-releases","title":"Versioned Releases","text":"<p>Versioned documentation is created only for official releases:</p> <ol> <li>Use the \"Build Docs and Publish to gh-pages\" GitHub Action</li> <li>Manually specify VERSION and ALIAS</li> <li>This is done rarely for major/minor releases</li> </ol>"},{"location":"CONTRIBUTING.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"CONTRIBUTING.html#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"CONTRIBUTING.html#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"SECURITY.html","title":"SECURITY","text":""},{"location":"SECURITY.html#overview","title":"Overview","text":"<p>This project is maintained by members of the AWS for Games technical community within AWS (i.e. Solutions Architects, Technical Account Managers, Software Engineers) who support the gaming industry. Design decisions and tradeoffs made throughout this project reflect our experiences working with game studios to build and maintain their development infrastructure and tools in the cloud. We encourage contributions from the community via Pull Requests, which are manually reviewed and tested by the core maintainers of the project before they are merged.</p> <p>We rely on Open Source Security (OpenSSF) Scorecard assessments to validate our project's security posture against a common set of open source project risks. We also self-certify our security practices against the standards defined by OpenSSF Best Practices Badge Program. The badges above are hyperlinks that you can follow to review the results of each.</p>"},{"location":"SECURITY.html#reporting-a-vulnerability","title":"Reporting a vulnerability","text":"<p>If you discover a potential security issue in this project, we ask that you notify AWS/Amazon Security via our vulnerability reporting page or directly via email to aws-security@amazon.com.</p> <p>Please do not create a public GitHub issue.</p>"},{"location":"assets/ansible-playbooks/perforce/p4-server/index.html","title":"Ansible Playbooks | P4 Server","text":"<p>This page includes documentation for reusable Ansible Playbooks for game development on AWS.</p> <p>Currently the project provides the following playbooks:</p> Playbook Description P4 Server (formerly Helix Core) Commit Playbook Sets up a server as a P4 Server Commit Server P4 Server (formerly Helix Core) Replica Playbook Sets up a server as a P4 Server Replica Server"},{"location":"assets/jenkins-pipelines/index.html","title":"Jenkins example pipelines","text":"<p>This folder contains example Jenkins pipelines for building various pieces of software. To use them, create a new Jenkins pipeline project, and then copy-and-paste the contents of a sample file into the \"Pipeline\" section in the configuration page.</p> <p>You will likely need to change the pipelines slightly to suit your needs, for example to alter the agent node labels on which steps run. Many pipelines also depend on a global Jenkins environment variable to be set: <code>FSX_WORKSPACE_VOLUME_ID</code>, which should be set to the FSx for OpenZFS volume ID of the Workspace volume.</p> <p>The pipelines are primarily written as Declarative Pipelines, with sections of scripted pipeline blocks used to pass variables between stages or to implement try/catch behavior.</p>"},{"location":"assets/jenkins-pipelines/index.html#ue5_build_pipelinegroovy","title":"<code>ue5_build_pipeline.groovy</code>","text":"<p>This pipeline builds Unreal Engine 5 on Linux from its Git repository on GitHub, using an FSx volume as workspace cache and another FSx volume to (optionally) store build cache artifacts to speed up subsequent builds.</p> Note <p>This pipeline requires that you configure GitHub credentials in Jenkins. You also need to get access to the Unreal Engine 5 source code.</p> Note <p>Although this pipeline supports octobuild for caching build artifacts out-of-the-box, octobuild for Linux requires a patch to the Unreal Engine 5 source code. We recommend forking Unreal Engine 5, applying the necessary patch to your fork, and then building from your own fork instead of from the upstream repository. Please refer to the octobuild readme for instructions on which patches to apply.</p> Note <p>You will need to run this on a build node with large /tmp space.</p> <p>The pipeline is divided in two stages:</p> <ol> <li> <p>Prepare - Clones or pulls the Git repository to the FSx workspace volume, then creates an FSx snapshot and waits for it to be available. This stage is skipped if the <code>source_path</code> parameter is provided.</p> </li> <li> <p>Build - Builds Unreal Engine 5 from the snapshot location on x86_64 Linux. Because FSx for OpenZFS snapshots are read-only, on Linux a temporary overlay file system is created.</p> </li> </ol>"},{"location":"assets/jenkins-pipelines/index.html#godotgroovy","title":"<code>godot.groovy</code>","text":"<p>This pipeline builds the Godot engine from its public Git repository, using an FSx volume as workspace cache and another FSx volume to store sccache artifacts to speed up subsequent builds.</p> <p>The pipeline is divided in two stages:</p> <ol> <li> <p>Prepare - Clones or pulls the Git repository to the FSx workspace volume, then creates an FSx snapshot and waits for it to be available. This stage is skipped if the <code>source_path</code> parameter is provided.</p> </li> <li> <p>Build - Builds Godot from the snapshot location on x86_64 Linux and arm64 Linux. Because FSx for OpenZFS snapshots are read-only, and Godot does not build from a read-only filesystem, on Linux a temporary overlay file system is created.</p> </li> </ol>"},{"location":"assets/jenkins-pipelines/index.html#gamelift_sdkgroovy","title":"<code>gamelift_sdk.groovy</code>","text":"<p>This pipeline builds the GameLift Server C++ SDK in 8 different configurations. It uses an FSx volume as workspace cache and (optionally) another FSx volume to store sccache artifacts to speed up subsequent builds.</p> <p>The build configurations are:</p> Operating sytem CPU architecture Build configuration Ubuntu Jammy 22.04 x86_64 Standard build Ubuntu Jammy 22.04 x86_64 Built for Unreal Engine Ubuntu Jammy 22.04 arm64 Standard build Ubuntu Jammy 22.04 arm64 Built for Unreal Engine Amazon Linux 2023 x86_64 Standard build Amazon Linux 2023 x86_64 Built for Unreal Engine Amazon Linux 2023 arm64 Standard build Amazon Linux 2023 arm64 Built for Unreal Engine Note <p>You will most likely not need each of these build configurations to compile the GameLift Server SDK for your game. We recommend that you delete those you don't need from the pipeline manually.</p> <p>The pipeline is divided in two stages:</p> <ol> <li> <p>Prepare - Downloads the GameLift Server SDK .zip to the FSx workspace volume, then creates an FSx snapshot and waits for it to be available. This stage is skipped if the <code>source_path</code> parameter is provided.</p> </li> <li> <p>Build - Builds the SDK from the snapshot location on aforementioned operating systems and configurations.</p> </li> </ol>"},{"location":"assets/jenkins-pipelines/index.html#delete_oldest_snapshotgroovy","title":"<code>delete_oldest_snapshot.groovy</code>","text":"<p>This parameterized pipeline deletes the oldest FSx snapshot that's older than 7 days. Use this to clean up automatically-created snapshots for workspace volumes that you no longer need.</p> <p>This pipeline has the following input parameters:</p> <ol> <li><code>FSX_VOLUME_ID</code> - FSx volume ID of the volume to delete the oldest snapshot from.</li> </ol> <p>Warning</p> <p>This pipeline performs no logic to check whether a snapshot was created automatically, so do not run this pipeline against FSx volumes where you use snapshots for data backup purposes.</p>"},{"location":"assets/jenkins-pipelines/index.html#multiplatform_buildgroovy","title":"<code>multiplatform_build.groovy</code>","text":"<p>This simple multi-stage pipeline demonstrates how to build for multiple platforms by running multiple stages, and how to paralellize steps in a single stage across different build nodes. It can be used to verify that build nodes for various platforms work correctly, and is a great starting point for creating new pipelines.</p>"},{"location":"assets/packer/build-agents/linux/index.html","title":"Packer templates for Unreal Engine Linux build agents","text":"<p>The following templates provide Unreal Engine Linux build agents:</p> Operating system CPU architecture file location Ubuntu Jammy 22.04 x86_64 (a.k.a. amd64) <code>x86_64/ubuntu-jammy-22.04-amd64-server.pkr.hcl</code> Ubuntu Jammy 22.04 aarch64 (a.k.a. arm64) <code>aarch64/ubuntu-jammy-22.04-arm64-server.pkr.hcl</code> Amazon Linux 2023 x86_64 (a.k.a. amd64) <code>x86_64/amazon-linux-2023-x86_64.pkr.hcl</code> Amazon Linux 2023 aarch64 (a.k.a. arm64) <code>aarch64/amazon-linux-2023-arm64.pkr.hcl</code>"},{"location":"assets/packer/build-agents/linux/index.html#usage","title":"Usage","text":"<ol> <li>Make a copy of <code>example.pkrvars.hcl</code> and adjust the input variables as needed</li> <li>Ensure you have active AWS credentials</li> <li>Invoke <code>packer build --var-file=&lt;your .pkrvars.hcl file&gt; &lt;path to .pkr.hcl file&gt;</code>, then wait for the build to complete.</li> </ol>"},{"location":"assets/packer/build-agents/linux/index.html#software-packages-included","title":"Software packages included","text":"<p>The templates install various software packages:</p>"},{"location":"assets/packer/build-agents/linux/index.html#common-tools","title":"common tools","text":"<p>Some common tools are installed to enable installing other software, performing maintenance tasks, and compile some C++ software:</p> <ul> <li>git</li> <li>curl</li> <li>jq</li> <li>unzip</li> <li>dos2unix</li> <li>AWS CLI v2</li> <li>AWS Systems Manager Agent</li> <li>Amazon Corretto</li> <li>mount.nfs, to be able to mount FSx volumes over NFS</li> <li>python3</li> <li>python3 packages: 'pip', 'requests', 'boto3' and 'botocore'</li> <li>clang</li> <li>cmake3</li> <li>scons</li> <li>Development libraries for compiling the Amazon GameLift Server SDK for C++</li> <li>Development libraries for compiling the Godot 4 game engine (if available in the OS's package manager)</li> </ul>"},{"location":"assets/packer/build-agents/linux/index.html#mold","title":"mold","text":"<p>The 'mold' linker is installed to enable faster linking.</p>"},{"location":"assets/packer/build-agents/linux/index.html#fsx-automounter-service","title":"FSx automounter service","text":"<p>The FSx automounter systemd service is a service written in Python that automatically mounts FSx for OpenZFS volumes on instance bootup. The service uses resource tags on FSx volumes to determine if and where to mount volumes on.</p> <p>You can use the following tags on FSx volumes:</p> <ul> <li>'automount-fsx-volume-name' tag: specifies the name of the local mount point. The mount point specified will be prefixed with 'fsx_' by the service.</li> <li>'automount-fsx-volume-on' tag: This tag contains a space-delimited list of EC2 instance names on which the volume will be automatically mounted by this service (if it is running on that instance).</li> </ul> <p>For example, if the FSx automounter service is running on an EC2 instance with Name tag 'ubuntu-builder', and an FSx volume has tag <code>automount-fsx-volume-on</code>=<code>al2023-builder ubuntu-builder</code> and tag <code>automount-fsx-volume-name</code>=<code>workspace</code>, then the automounter will automatically mount that volume on <code>/mnt/fsx_workspace</code>.</p> <p>Note that the automounter service makes use of the ListTagsForResource FSx API call, which is rate-limited. If you intend to scale up hundreds of EC2 instances that are running this service, then we recommend automatically mounting FSx volumes using <code>/etc/fstab</code>.</p>"},{"location":"assets/packer/build-agents/linux/index.html#mount_ephemeral-service","title":"mount_ephemeral service","text":"<p>The mount_ephemeral service is a systemd service written as a simple bash script that mounts NVMe attached instance storage volume automatically as temporary storage. It does this by formatting <code>/dev/nvme1n1</code> as xfs and then mounting it on <code>/tmp</code>. This service runs on instance bootup.</p>"},{"location":"assets/packer/build-agents/linux/index.html#create_swap-service","title":"create_swap service","text":"<p>The create_swap service is a systemd service written as a simple bash script that creates a 1GB swap file on <code>/swapfile</code>. This service runs on instance bootup.</p>"},{"location":"assets/packer/build-agents/linux/index.html#sccache","title":"sccache","text":"<p>'sccache' is installed to cache c/c++ compilation artefacts, which can speed up builds by avoiding unneeded work.</p> <p>sccache is installed as a systemd service, and configured to use <code>/mnt/fsx_cache/sccache</code> as its cache folder. The service expects this folder to be available or set up by another service.</p>"},{"location":"assets/packer/build-agents/linux/index.html#octobuild","title":"octobuild","text":"<p>'Octobuild' is installed to act as a compilation cache for Unreal Engine.</p> <p>Octobuild is configured in octobuild.conf to use <code>/mnt/fsx_cache/octobuild_cache</code> as its cache folder, and expects this folder to be available or set up by another service.</p> <p>NOTE: Octobuild is not supported on aarch64, and therefore not installed there.</p>"},{"location":"assets/packer/build-agents/linux/index.html#processor-architectures-and-naming-conventions","title":"Processor architectures and naming conventions","text":"<p>Within this folder, the processor architecture naming conventions as reported by <code>uname -m</code> are used, hence why there are scripts here with names containing \"x86_64\" or \"aarch64\". The packer template <code>.hcl</code> files are named following the naming conventions of the operating system that they are based on. Unfortunately, because some operating systems don't use the same terminology in their naming conventions throughout, this means that you'll see this lack of consistency here has well.</p>"},{"location":"assets/packer/build-agents/windows/index.html","title":"Packer Templates for Unreal Engine Windows Build Agents","text":"<p>The following template builds a Windows based AMI capable of Unreal Engine 5.4 compilation jobs. Please customize it to your needs.</p>"},{"location":"assets/packer/build-agents/windows/index.html#usage","title":"Usage","text":"<p>This Amazon Machine Image is provisioned using the Windows Server 2022 base operating system. It installs all required tooling for Unreal Engine 5 compilation by default. Please consult the release notes for Unreal Engine 5.7 for details on what tools are used for compiling this version of the engine.</p> <p>The only required variable for building this Amazon Machine Image is a public SSH key.</p> <pre><code>packer build windows.pkr.hcl \\\n    -var \"public_key=&lt;include public ssh key here&gt;\"\n</code></pre> Note <p>The above command assumes you are running <code>packer</code> from the <code>/assets/packer/build-agents/windows</code> directory.</p> <p>You will then want to upload the private SSH key to AWS Secrets Manager so that the Jenkins orchestration service can use it to connect to this build agent.</p> <pre><code>aws secretsmanager create-secret \\\n    --name JenkinsPrivateSSHKey \\\n    --description \"Private SSH key for Jenkins build agent access.\" \\\n    --secret-string \"&lt;insert private SSH key here&gt;\" \\\n    --tags 'Key=jenkins:credentials:type,Value=sshUserPrivateKey' 'Key=jenkins:credentials:username,Value=jenkins'\n</code></pre> <p>Take note of the output of this CLI command. You will need the ARN later.</p> <p>Currently this AMI is designed to work with our Jenkins module. This is why it creates a <code>jenkins</code> user and the associated SSH username for the key you upload is that same <code>jenkins</code> user. Expanded customization of this AMI is currently on the Cloud Game Development Toolkit roadmap.</p>"},{"location":"assets/packer/build-agents/windows/index.html#installed-tooling","title":"Installed Tooling","text":"<ul> <li>Chocolatey package manager</li> <li>OpenJDK used by Jenkins agents</li> <li>Git</li> <li>OpenSSH</li> <li>Python3</li> <li>Botocore</li> <li>Boto3</li> <li>Client for Network File System (NFS)</li> <li>Windows Development Kit and Debugging Tools</li> <li>Visual Studio 2022 Build Tools</li> <li>VCTools Workload; Include Recommended</li> <li>ManagedDesktopBuild Tools; Include Recommended</li> <li>MSVC v143 - VS 2022 C++ x64/x86 build tools</li> <li>Microsoft.Net.Component.4.6.2TargetingPack</li> </ul> <p>Consult the Visual Studio Build Tools component directory for details.</p>"},{"location":"assets/packer/perforce/p4-code-review/index.html","title":"P4 Code Review Packer Template","text":"<p>This Packer template creates an Amazon Machine Image (AMI) for P4 Code Review (Helix Swarm) on Ubuntu 24.04 LTS. The AMI includes all necessary software pre-installed, with runtime configuration handled automatically during instance launch.</p>"},{"location":"assets/packer/perforce/p4-code-review/index.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prerequisites</li> <li>Quick Start</li> <li>What Gets Installed</li> <li>Building the AMI</li> <li>Finding Your AMI</li> <li>Next Steps</li> <li>Troubleshooting</li> </ul>"},{"location":"assets/packer/perforce/p4-code-review/index.html#prerequisites","title":"Prerequisites","text":"<p>Before building the AMI, ensure you have:</p> <ol> <li>AWS CLI configured with valid credentials:</li> </ol> <pre><code>aws configure\n# Verify access\naws sts get-caller-identity\n</code></pre> <ol> <li>Packer installed (version &gt;= 1.8.0):</li> </ol> <pre><code>packer version\n</code></pre> <p>If not installed, download from https://www.packer.io/downloads</p> <ol> <li>VPC Access:</li> <li>Default VPC in your region (Default behaviour)</li> <li> <p>OR custom VPC with a public subnet (Can be configured by passing the VPC id through the <code>vpc_id</code> variable)</p> </li> <li> <p>IAM Permissions: Your AWS credentials need permissions to:</p> </li> <li>Launch EC2 instances</li> <li>Create AMIs</li> <li>Create/delete security groups</li> <li>Create/delete key pairs</li> </ol>"},{"location":"assets/packer/perforce/p4-code-review/index.html#quick-start","title":"Quick Start","text":"<p>From the repository root, run:</p> <pre><code># 1. Navigate to Packer template directory\ncd assets/packer/perforce/p4-code-review\n\n# 2. Initialize Packer (downloads required plugins)\npacker init p4_code_review_x86.pkr.hcl\n\n# 3. Validate the template\npacker validate p4_code_review_x86.pkr.hcl\n\n# 4. Build the AMI (takes ~10-15 minutes)\npacker build p4_code_review_x86.pkr.hcl\n</code></pre> <p>At the end of the build, Packer will output the AMI ID:</p> <pre><code>==&gt; amazon-ebs.ubuntu2404: AMI: ami-0abc123def456789\n</code></pre> <p>Save this AMI ID - you'll need it for Terraform deployment.</p>"},{"location":"assets/packer/perforce/p4-code-review/index.html#what-gets-installed","title":"What Gets Installed","text":"<p>The AMI includes a complete P4 Code Review installation:</p>"},{"location":"assets/packer/perforce/p4-code-review/index.html#software-components","title":"Software Components","text":"<ol> <li>Perforce Repository: Official Perforce package repository (Ubuntu jammy/22.04 compatible)</li> <li>PHP 8.x: PHP runtime with all required extensions:<ol> <li>Core: curl, mbstring, xml, intl, ldap, bcmath</li> <li>Database: mysql</li> <li>Graphics: gd</li> <li>Archive: zip</li> <li>PECL: igbinary, msgpack, redis</li> </ol> </li> <li>Helix Swarm: Native DEB installation via <code>helix-swarm</code> package</li> <li>Apache2: Web server with mod_php and required modules (rewrite, proxy, proxy_fcgi)</li> <li>PHP-FPM: FastCGI Process Manager for PHP</li> <li>helix-swarm-optional (optional, installed by default): LibreOffice for document preview (.docx, .xlsx, .pptx) and ImageMagick for image preview (.png, .jpg, .tiff, etc.) (~500MB)</li> <li>AWS CLI v2: Required for Secrets Manager access and EBS volume operations at runtime</li> <li>Configuration Script: <code>/home/ubuntu/swarm_scripts/swarm_instance_init.sh</code> for runtime setup (see Runtime Configuration Details below)</li> </ol>"},{"location":"assets/packer/perforce/p4-code-review/index.html#system-configuration","title":"System Configuration","text":"<ul> <li>AppArmor: Ubuntu's security module (less restrictive by default for <code>/opt</code>)</li> <li>Services: Apache2 and PHP-FPM enabled for automatic startup</li> <li>User: <code>swarm</code> system user created with proper permissions</li> <li>Directories: <code>/opt/perforce/swarm</code> prepared with correct ownership</li> </ul>"},{"location":"assets/packer/perforce/p4-code-review/index.html#whats-not-configured-yet","title":"What's NOT Configured Yet","text":"<p>The following are configured at deployment when you launch an instance:</p> <ul> <li>P4 Server connection details</li> <li>P4 user credentials (fetched from AWS Secrets Manager)</li> <li>Redis cache connection</li> <li>External hostname/URL</li> <li>SSO settings</li> <li>EBS volume mounting for persistent data</li> <li>Queue worker configuration (cron job and endpoint)</li> <li>File permissions for worker processes</li> <li>P4 Server extension installation (Swarm triggers)</li> </ul>"},{"location":"assets/packer/perforce/p4-code-review/index.html#runtime-configuration-details","title":"Runtime Configuration Details","text":"<p>When an EC2 instance launches, the user-data script performs the following steps:</p> <ol> <li>EBS Volume Attachment: Finds and attaches the persistent data volume by tags</li> <li>Filesystem Setup: Creates ext4 filesystem (first launch) or mounts existing one</li> <li>Swarm Configuration: Executes <code>/home/ubuntu/swarm_scripts/swarm_instance_init.sh</code> which:</li> <li>Retrieves P4 credentials from AWS Secrets Manager</li> <li>Runs Perforce's official <code>configure-swarm.sh</code> to:<ul> <li>Connect to P4 Server and validate credentials</li> <li>Install Swarm extension on P4 Server (enables event triggers)</li> <li>Create initial configuration file</li> <li>Set up Apache VirtualHost</li> <li>Create cron job for queue workers</li> </ul> </li> <li>Configures file permissions for queue worker functionality</li> <li>Updates configuration with Redis connection details</li> <li>Configures queue workers to use localhost endpoint</li> <li>Starts Apache and PHP-FPM services</li> </ol> <p>Queue Workers: P4 Code Review requires background workers to process events, send notifications, and index files. These are spawned by a cron job (created by <code>configure-swarm.sh</code>) that runs every minute. The runtime configuration ensures workers have proper permissions and connect to the correct endpoint.</p>"},{"location":"assets/packer/perforce/p4-code-review/index.html#building-the-ami","title":"Building the AMI","text":""},{"location":"assets/packer/perforce/p4-code-review/index.html#option-1-using-default-vpc-recommended","title":"Option 1: Using Default VPC (Recommended)","text":"<p>If your AWS region has a default VPC:</p> <pre><code>cd assets/packer/perforce/p4-code-review\npacker init p4_code_review_x86.pkr.hcl\npacker build p4_code_review_x86.pkr.hcl\n</code></pre>"},{"location":"assets/packer/perforce/p4-code-review/index.html#option-2-using-custom-vpc","title":"Option 2: Using Custom VPC","text":"<p>If you don't have a default VPC, specify your own:</p> <pre><code>packer build \\\n  -var=\"region=us-west-2\" \\\n  -var=\"vpc_id=vpc-xxxxx\" \\\n  -var=\"subnet_id=subnet-xxxxx\" \\\n  -var=\"associate_public_ip_address=true\" \\\n  -var=\"ssh_interface=public_ip\" \\\n  p4_code_review_x86.pkr.hcl\n</code></pre> <p>Requirements for custom VPC:</p> <ul> <li>Subnet must be in a public subnet (has route to Internet Gateway)</li> <li><code>associate_public_ip_address=true</code> if subnet doesn't auto-assign public IPs</li> <li>Security group allows outbound internet access (for package downloads)</li> </ul>"},{"location":"assets/packer/perforce/p4-code-review/index.html#option-3-using-variables-file","title":"Option 3: Using Variables File","text":"<p>Create a <code>my-vars.pkrvars.hcl</code>:</p> <pre><code>region                       = \"us-west-2\"\nvpc_id                       = \"vpc-xxxxx\"\nsubnet_id                    = \"subnet-xxxxx\"\nassociate_public_ip_address  = true\nssh_interface                = \"public_ip\"\n</code></pre> <p>Then build:</p> <pre><code>packer build -var-file=\"my-vars.pkrvars.hcl\" p4_code_review_x86.pkr.hcl\n</code></pre>"},{"location":"assets/packer/perforce/p4-code-review/index.html#build-output","title":"Build Output","text":"<p>Successful build output looks like:</p> <pre><code>==&gt; amazon-ebs.ubuntu2404: Stopping the source instance...\n==&gt; amazon-ebs.ubuntu2404: Waiting for the instance to stop...\n==&gt; amazon-ebs.ubuntu2404: Creating AMI p4_code_review_ubuntu-20231209123456 from instance i-xxxxx\n==&gt; amazon-ebs.ubuntu2404: AMI: ami-0abc123def456789\n==&gt; amazon-ebs.ubuntu2404: Waiting for AMI to become ready...\n==&gt; amazon-ebs.ubuntu2404: Terminating the source AWS instance...\nBuild 'amazon-ebs.ubuntu2404' finished after 12 minutes 34 seconds.\n\n==&gt; Wait completed after 12 minutes 34 seconds\n\n==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; amazon-ebs.ubuntu2404: AMIs were created:\nus-west-2: ami-0abc123def456789\n</code></pre> <p>Copy the AMI ID (e.g., <code>ami-0abc123def456789</code>) - you'll need this for Terraform.</p>"},{"location":"assets/packer/perforce/p4-code-review/index.html#finding-your-ami","title":"Finding Your AMI","text":""},{"location":"assets/packer/perforce/p4-code-review/index.html#list-all-p4-code-review-amis","title":"List All P4 Code Review AMIs","text":"<pre><code>aws ec2 describe-images \\\n  --owners self \\\n  --filters \"Name=name,Values=p4_code_review_ubuntu-*\" \\\n  --query 'Images[*].[ImageId,Name,CreationDate]' \\\n  --output table\n</code></pre> <p>Output:</p> <pre><code>+-----------------------------------------------------------------------+\n|                         DescribeImages                                |\n+----------------------+---------------------------------------+--------+\n|  ami-0abc123def456   | p4_code_review_ubuntu-20231209    | 2023...|\n|  ami-0def456abc789   | p4_code_review_ubuntu-20231208    | 2023...|\n+----------------------+---------------------------------------+--------+\n</code></pre>"},{"location":"assets/packer/perforce/p4-code-review/index.html#get-the-latest-ami","title":"Get the Latest AMI","text":"<pre><code>aws ec2 describe-images \\\n  --owners self \\\n  --filters \"Name=name,Values=p4_code_review_ubuntu-*\" \\\n  --query 'Images | sort_by(@, &amp;CreationDate) | [-1].[ImageId,Name,CreationDate]' \\\n  --output table\n</code></pre>"},{"location":"assets/packer/perforce/p4-code-review/index.html#get-details-about-a-specific-ami","title":"Get Details About a Specific AMI","text":"<pre><code>aws ec2 describe-images --image-ids ami-0abc123def456789\n</code></pre>"},{"location":"assets/packer/perforce/p4-code-review/index.html#next-steps","title":"Next Steps","text":"<p>Now that you have an AMI, proceed to deploy P4 Code Review infrastructure:</p> <ol> <li> <p>Read the P4 Code Review Module Documentation</p> </li> <li> <p>Follow the deployment guide in the module README, which covers:</p> </li> <li>Creating AWS Secrets Manager secrets for P4 credentials</li> <li>Writing Terraform configuration</li> <li>Deploying the infrastructure</li> <li>Accessing the P4 Code Review web console</li> </ol>"},{"location":"assets/packer/perforce/p4-code-review/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"assets/packer/perforce/p4-code-review/index.html#no-default-vpc-available","title":"\"No default VPC available\"","text":"<p>Error: Packer fails with \"No default VPC for this user\"</p> <p>Solution: Use Option 2 or 3 above to specify your VPC and subnet:</p> <pre><code>packer build \\\n  -var=\"vpc_id=vpc-xxxxx\" \\\n  -var=\"subnet_id=subnet-xxxxx\" \\\n  p4_code_review_x86.pkr.hcl\n</code></pre>"},{"location":"assets/packer/perforce/p4-code-review/index.html#unable-to-connect-to-instance","title":"\"Unable to connect to instance\"","text":"<p>Error: Packer times out connecting to the instance</p> <p>Possible causes:</p> <ol> <li>Subnet is not public (no route to Internet Gateway)</li> <li>Security group blocks SSH (port 22)</li> <li>No public IP assigned to instance</li> </ol> <p>Solution: Verify your subnet has:</p> <pre><code># Check if subnet has route to IGW\naws ec2 describe-route-tables \\\n  --filters \"Name=association.subnet-id,Values=subnet-xxxxx\" \\\n  --query 'RouteTables[*].Routes[?GatewayId!=`local`]'\n</code></pre>"},{"location":"assets/packer/perforce/p4-code-review/index.html#package-installation-failed","title":"\"Package installation failed\"","text":"<p>Error: APT/DEB errors during build</p> <p>Possible causes:</p> <ol> <li>No internet access from instance</li> <li>Perforce repository temporarily unavailable</li> <li>Package version conflicts</li> </ol> <p>Solution:</p> <ul> <li>Check build instance has outbound internet access</li> <li>Try rebuilding (temporary outages resolve themselves)</li> <li>Review <code>/var/log/swarm_setup.log</code> on build instance</li> </ul>"},{"location":"assets/packer/perforce/p4-code-review/index.html#ami-already-exists-with-that-name","title":"\"AMI already exists with that name\"","text":"<p>Error: \"AMI name 'p4_code_review_ubuntu-TIMESTAMP' already exists\"</p> <p>This shouldn't happen (timestamp should be unique), but if it does:</p> <pre><code># List your AMIs\naws ec2 describe-images --owners self \\\n  --filters \"Name=name,Values=p4_code_review_ubuntu-*\"\n\n# Deregister old AMI if no longer needed\naws ec2 deregister-image --image-id ami-xxxxx\n</code></pre>"},{"location":"assets/packer/perforce/p4-code-review/index.html#build-is-slow","title":"Build is slow","text":"<p>Normal build time: 10-15 minutes</p> <p>If taking longer:</p> <ul> <li>Package downloads can be slow depending on region</li> <li>Perforce repository might be experiencing high load</li> <li>This is normal - be patient</li> </ul>"},{"location":"assets/packer/perforce/p4-code-review/index.html#need-to-debug-the-build","title":"Need to debug the build?","text":"<p>Enable debug mode to step through each provisioner:</p> <pre><code>packer build -debug p4_code_review_x86.pkr.hcl\n</code></pre> <p>This will pause before each provisioner step, allowing you to:</p> <ul> <li>SSH into the build instance</li> <li>Inspect the current state</li> <li>Verify installation progress</li> <li>Press Enter to continue to the next step</li> </ul> <p>Enable detailed logging:</p> <pre><code>PACKER_LOG=1 packer build p4_code_review_x86.pkr.hcl\n</code></pre>"},{"location":"assets/packer/perforce/p4-code-review/index.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Packer Documentation</li> <li>Perforce Helix Swarm Admin Guide</li> <li>Ubuntu 24.04 LTS Documentation</li> </ul>"},{"location":"assets/packer/perforce/p4-code-review/index.html#questions-or-issues","title":"Questions or Issues?","text":"<p>If you encounter problems:</p> <ol> <li>Check the troubleshooting section above</li> <li>Review Packer logs with <code>PACKER_LOG=1</code></li> <li>Use <code>packer build -debug</code> to step through the build process</li> <li>Verify AWS credentials and permissions</li> </ol>"},{"location":"assets/packer/perforce/p4-server/index.html","title":"P4 Server Packer Template","text":"<p>This Packer template creates an Amazon Machine Image for installing and configuring P4 Server on Linux. It supports both <code>x86</code> and <code>ARM</code> architectures.</p> <p>The <code>p4_configure.sh</code> script contains the majority of P4 Server setup. It performs the following operations:</p> <ol> <li>Pre-Flight Checks: Ensures the script is run with root privileges.</li> <li>Environment Setup: Defines paths and necessary constants for the installation.</li> <li>SELinux Handling: Checks if SELinux is enabled and installs required packages.</li> <li>User and Group Verification: Ensures the 'perforce' user and group exist.</li> <li>Directory Creation and Ownership: Ensures necessary directories exist and have correct ownership.</li> <li>P4 Binaries and SDP Installation: Downloads and extracts SDP, checks for P4 binaries, and downloads them if missing.</li> <li>Systemd Service Configuration: Sets up a systemd service for the p4d server.</li> <li>SSL Configuration: Updates SSL certificate configuration with the EC2 instance DNS name.</li> <li>SELinux Context Management: Updates SELinux context for p4d.</li> <li>Crontab Initialization: Sets up crontab for the 'perforce' user.</li> <li>SDP Verification: Runs a script to verify the SDP installation.</li> <li>P4Auth Extension: Installs the P4Auth Extension and validates successful communication with P4Auth.</li> </ol>"},{"location":"assets/packer/perforce/p4-server/index.html#how-to-use","title":"How to Use","text":"<p>Building this AMI is as easy as running (x86 example):</p> <pre><code>packer init ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\n</code></pre> <pre><code>packer validate ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\n</code></pre> <pre><code>packer build ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\n</code></pre> <p>Packer will attempt to leverage the default VPC available in the AWS account and Region specified by your CLI credentials. It will provision an instance in a public subnet and communicate with that instance over the public internet. If a default VPC is not available or otherwise provided, the above command will fail. This Packer template can take a number of variables as specified in <code>example.pkrvars.hcl</code>. Variables can be passed individually through the <code>-var</code> command line flag or through a configuration file with the <code>-var-file</code> command line flag.</p> <p>An instance that is provisioned with this AMI will not automatically deploy a P4 Server. Instead, the required installation and configuration scripts are loaded onto this AMI by Packer, and then invoked at boot through EC2 user data. The P4 Server module does this through Terraform, but you can also manually provision an instance off of this AMI and specify the user data yourself:</p> <pre><code>#!/bin/bash\n/home/ec2-user/cloud-game-development-toolkit/p4_configure.sh \\\n   --p4d_type p4d_master \\\n   --hx_depots /dev/sdf \\\n   --hx_metadata /dev/sdg \\\n   --hx_logs /dev/sdh \\\n   --super_password &lt;AWS Secrets Manager secret ID for service account password&gt; \\\n   --admin_username &lt;AWS Secrets Manager secret ID for admin username&gt; \\\n   --admin_password &lt;AWS Secrets Manager secret ID for admin password&gt; \\\n   --fqdn perforce.example.com \\\n   --auth https://auth.perforce.example.com\n</code></pre>"},{"location":"assets/packer/perforce/p4-server/index.html#script-options","title":"Script Options","text":"Option Description <code>--p4d_type</code> P4 Server type: <code>p4d_master</code>, <code>p4d_replica</code>, or <code>p4d_edge</code> <code>--hx_depots</code> Path/device for P4 Server depots volume <code>--hx_metadata</code> Path/device for P4 Server metadata volume <code>--hx_logs</code> Path/device for P4 Server logs volume <code>--super_password</code> AWS Secrets Manager secret ID for service account (super) password <code>--admin_username</code> AWS Secrets Manager secret ID for admin account username <code>--admin_password</code> AWS Secrets Manager secret ID for admin account password <code>--fqdn</code> Fully Qualified Domain Name for the P4 Server <code>--auth</code> P4Auth URL (optional) <code>--case_sensitive</code> Case sensitivity: <code>0</code> (insensitive) or <code>1</code> (sensitive, default) <code>--unicode</code> Enable Unicode mode: <code>true</code> or <code>false</code> <code>--selinux</code> Update SELinux labels: <code>true</code> or <code>false</code> <code>--plaintext</code> Disable SSL: <code>true</code> or <code>false</code> <code>--fsxn_password</code> AWS Secrets Manager secret ID for FSxN password <code>--fsxn_svm_name</code> FSxN Storage Virtual Machine name <code>--fsxn_management_ip</code> FSxN management IP address"},{"location":"assets/packer/perforce/p4-server/index.html#user-configuration","title":"User Configuration","text":"<p>The script creates two Perforce users:</p> <ol> <li> <p>Service Account (<code>super</code>): Always created with username \"super\". Used internally by P4 Code Review (Helix Swarm) and other tooling. Password provided via <code>--super_password</code>.</p> </li> <li> <p>Admin Account: Created with the username provided via <code>--admin_username</code>. This is the account for human administrators. Password provided via <code>--admin_password</code>.</p> </li> </ol> <p>Both users have full super privileges and are added to the <code>unlimited_timeout</code> group.</p> <p>We recommend using the Perforce module to manage these configurations through Terraform.</p>"},{"location":"assets/packer/perforce/p4-server/index.html#important-notes","title":"Important Notes","text":"<ul> <li>This script is designed for a specific use-case and might require modifications for different environments or requirements.</li> <li>Ensure you have a backup of your system before running the script, as it makes significant changes to users, groups, and services.</li> <li>The script assumes an internet connection for downloading packages and binaries.</li> </ul>"},{"location":"assets/packer/virtual-workstations/index.html","title":"Virtual Workstations Packer Templates","text":""},{"location":"assets/packer/virtual-workstations/index.html#critical-requirements","title":"\ud83d\udea8 CRITICAL REQUIREMENTS","text":""},{"location":"assets/packer/virtual-workstations/index.html#gpu-instance-types-required","title":"GPU Instance Types Required","text":""},{"location":"assets/packer/virtual-workstations/index.html#all-templates-build-nvidia-optimized-amis","title":"\u26a0\ufe0f ALL TEMPLATES BUILD NVIDIA-OPTIMIZED AMIs","text":""},{"location":"assets/packer/virtual-workstations/index.html#for-packer-build","title":"For Packer Build:","text":"<ul> <li>\u2705 GPU instances: <code>g4dn.*</code>, <code>g5.*</code>, <code>p3.*</code>, <code>p4.*</code> (full functionality)</li> <li>\u26a0\ufe0f Non-GPU instances: <code>t3.*</code>, <code>m5.*</code>, <code>c5.*</code>, <code>r5.*</code> (builds succeed, skips NVIDIA drivers)</li> <li>\ud83d\udd27 Current defaults: <code>g4dn.2xlarge</code> (recommended for production)</li> <li>\ud83c\udf93 Workshop friendly: C instances work fine for learning/demos</li> </ul> <p>For Final VDI Deployment:</p> <ul> <li>\u2705 Recommended: GPU instances for full functionality</li> <li>\u26a0\ufe0f Will boot but degraded: Non-GPU instances (software rendering only)</li> <li>\u274c GPU apps will fail: Unreal Engine, CUDA applications</li> </ul> <p>Instance Compatibility Matrix:</p> Packer Build Final Instance Result <code>g4dn.2xlarge</code> <code>g4dn.xlarge</code> \u2705 Full GPU acceleration <code>g4dn.2xlarge</code> <code>g4dn.4xlarge</code> \u2705 Full GPU acceleration <code>g4dn.2xlarge</code> <code>m5.2xlarge</code> \u26a0\ufe0f Boots, no GPU, slow DCV <code>g4dn.2xlarge</code> <code>t3.medium</code> \u274c Poor performance, apps fail"},{"location":"assets/packer/virtual-workstations/index.html#directory-structure-required","title":"Directory Structure Required","text":"<p>\u26a0\ufe0f CRITICAL: These templates require the complete directory structure and cannot be used standalone without customization.</p>"},{"location":"assets/packer/virtual-workstations/index.html#directory-structure","title":"Directory Structure","text":"<pre><code>assets/packer/virtual-workstations/\n\u251c\u2500\u2500 shared/                    # REQUIRED - Base infrastructure scripts\n\u2502   \u251c\u2500\u2500 base_infrastructure.ps1    # NVIDIA + DCV + AWS tools + dev tools\n\u2502   \u251c\u2500\u2500 sysprep.ps1               # EC2Launch configuration\n\u2502   \u2514\u2500\u2500 userdata.ps1              # Packer WinRM setup\n\u251c\u2500\u2500 lightweight/               # Base VDI AMI\n\u2514\u2500\u2500 ue-gamedev/               # Unreal Engine development AMI\n</code></pre>"},{"location":"assets/packer/virtual-workstations/index.html#prerequisites","title":"Prerequisites","text":"<p>You MUST have the complete CGD Toolkit repository:</p> <pre><code># Clone the entire repository\ngit clone https://github.com/aws-games/cloud-game-development-toolkit.git\ncd cloud-game-development-toolkit\n\n# Verify structure exists\nls assets/packer/virtual-workstations/shared/\n# Should show: base_infrastructure.ps1  sysprep.ps1  userdata.ps1\n</code></pre> <p>Build with defaults:</p> <p>Packer will use your current AWS session and the defaults defined in the template:</p> <pre><code># Navigate to the template directory\ncd assets/packer/virtual-workstations/lightweight/\n\n# Build with defaults (recommended)\npacker build windows-server-2025-lightweight.pkr.hcl\n</code></pre> <p>To override default instance type (optional):</p> <pre><code># Create variables file (optional)\ncp variables.pkrvars.hcl.example variables.pkrvars.hcl\n\n# Edit variables.pkrvars.hcl\ninstance_type = \"g4dn.4xlarge\"  # Must be GPU-enabled\n\n# Build with custom variables\npacker build -var-file=\"variables.pkrvars.hcl\" windows-server-2025-lightweight.pkr.hcl\n</code></pre> <p>On-Demand Capacity Reservations (ODCR):</p> <p>Use existing capacity reservations during AMI builds:</p> <pre><code># Use ODCR if available, fall back to On-Demand if not\npacker build -var capacity_reservation_preference=open windows-server-2025-lightweight.pkr.hcl\n\n# Never use ODCR, always On-Demand\npacker build -var capacity_reservation_preference=none windows-server-2025-lightweight.pkr.hcl\n</code></pre>"},{"location":"assets/packer/virtual-workstations/index.html#available-templates","title":"Available Templates","text":""},{"location":"assets/packer/virtual-workstations/index.html#lightweight-ami","title":"Lightweight AMI","text":"<p>Best for: Runtime software customization via VDI Terraform module</p> <pre><code># Navigate to lightweight template directory\ncd assets/packer/virtual-workstations/lightweight/\n\n# Build lightweight AMI\npacker build windows-server-2025-lightweight.pkr.hcl\n</code></pre> <p>Includes: Windows Server 2025 + DCV + AWS CLI + PowerShell + Git + Perforce + Python + Chocolatey Build Time: ~25 minutes</p>"},{"location":"assets/packer/virtual-workstations/index.html#ue-gamedev-ami","title":"UE GameDev AMI","text":"<p>Best for: Immediate Unreal Engine development</p> <pre><code># Navigate to UE GameDev template directory\ncd assets/packer/virtual-workstations/ue-gamedev/\n\n# Build UE GameDev AMI\npacker build windows-server-2025-ue-gamedev.pkr.hcl\n</code></pre> <p>Includes: Lightweight base + Visual Studio 2022 + Epic Games Launcher (UE requires manual install) Build Time: ~45 minutes</p>"},{"location":"assets/packer/virtual-workstations/index.html#shared-infrastructure","title":"Shared Infrastructure","text":"<p>All templates use the shared base infrastructure script that provides:</p> <ul> <li>NVIDIA GRID drivers (GPU instances)</li> <li>Amazon DCV remote desktop server</li> <li>AWS CLI and PowerShell modules</li> <li>Git, Perforce, Python development tools</li> <li>Chocolatey package manager</li> <li>Active Directory management tools</li> <li>System PATH configuration</li> </ul>"},{"location":"assets/packer/virtual-workstations/index.html#template-dependencies","title":"Template Dependencies","text":"<p>Each template references shared scripts:</p> <ul> <li><code>../shared/base_infrastructure.ps1</code> - Common infrastructure setup</li> <li><code>../shared/sysprep.ps1</code> - EC2Launch configuration</li> <li><code>../shared/userdata.ps1</code> - Packer WinRM connectivity</li> </ul> <p>This is why the complete directory structure is required.</p>"},{"location":"assets/packer/virtual-workstations/index.html#usage-with-vdi-module","title":"Usage with VDI Module","text":"<p>After building an AMI, use it with the VDI Terraform module:</p> <pre><code>module \"vdi\" {\n  source = \"path/to/vdi/module\"\n\n  # Core configuration\n  project_prefix = \"gamedev\"\n  environment    = \"dev\"\n  vpc_id         = aws_vpc.vdi_vpc.id\n\n  # Templates reference your built AMIs\n  templates = {\n    \"developer\" = {\n      instance_type = \"g4dn.2xlarge\"\n      ami           = \"ami-0d22cd2c73f6b623\"  # Use AMI ID from Packer build output\n      volumes = {\n        Root = { capacity = 256, type = \"gp3\", windows_drive = \"C:\" }\n        Projects = { capacity = 1024, type = \"gp3\", windows_drive = \"D:\" }\n      }\n    }\n  }\n\n  # Workstations and users configuration\n  workstations = { /* ... */ }\n  users = { /* ... */ }\n  workstation_assignments = { /* ... */ }\n}\n</code></pre>"},{"location":"assets/packer/virtual-workstations/index.html#troubleshooting","title":"Troubleshooting","text":"<p>\"Script not found\" errors:</p> <ul> <li>Ensure you're running from the correct subdirectory</li> <li>Verify the <code>shared/</code> directory exists at the same level</li> <li>Check that you have the complete repository structure</li> </ul> <p>Build failures:</p> <ul> <li>Verify AWS credentials are configured</li> <li>Check VPC/subnet configuration in variables</li> <li>Ensure instance type supports GPU drivers (g4dn.* recommended)</li> </ul>"},{"location":"assets/packer/virtual-workstations/index.html#contributing","title":"Contributing","text":"<p>When adding new templates:</p> <ol> <li>Create new subdirectory (e.g., <code>audio/</code>)</li> <li>Reference shared scripts: <code>../shared/base_infrastructure.ps1</code></li> <li>Add template-specific scripts in the subdirectory</li> <li>Update this README with the new template</li> <li>Add dependency warnings to the template file</li> </ol>"},{"location":"assets/packer/virtual-workstations/lightweight/index.html","title":"VDI Lightweight Windows AMI","text":"<p>This Packer template creates a lightweight Windows Server 2025 AMI optimized for VDI workloads with runtime software customization.</p>"},{"location":"assets/packer/virtual-workstations/lightweight/index.html#whats-included","title":"What's Included","text":"<p>Base Infrastructure (from shared/base_infrastructure.ps1):</p> <ul> <li>Windows Server 2025 base</li> <li>NVIDIA GRID drivers (GPU instances)</li> <li>Amazon DCV remote desktop server</li> <li>AWS CLI and PowerShell modules</li> <li>Git, Perforce, Python, Chocolatey</li> <li>Active Directory management tools</li> <li>System PATH configuration</li> </ul> <p>Runtime Customization:</p> <ul> <li>Software packages installed via VDI Terraform module</li> <li>User accounts created at deployment time</li> <li>DCV sessions configured per user</li> <li>Domain join (optional)</li> </ul>"},{"location":"assets/packer/virtual-workstations/lightweight/index.html#build-instructions","title":"Build Instructions","text":"<ol> <li>Copy variables file:</li> </ol> <pre><code>cp variables.pkrvars.hcl.example variables.pkrvars.hcl\n</code></pre> <ol> <li>Edit variables.pkrvars.hcl:</li> <li>Set your AWS region</li> <li>Configure VPC/subnet (optional)</li> <li> <p>Adjust instance type if needed</p> </li> <li> <p>Build AMI:</p> </li> </ol> <pre><code>packer build -var-file=\"variables.pkrvars.hcl\" windows-server-2025-lightweight.pkr.hcl\n</code></pre>"},{"location":"assets/packer/virtual-workstations/lightweight/index.html#build-time","title":"Build Time","text":"<ul> <li>Estimated: 20-30 minutes</li> <li>Instance Type: g4dn.2xlarge (for GPU driver testing)</li> <li>Storage: 80GB root volume</li> </ul>"},{"location":"assets/packer/virtual-workstations/lightweight/index.html#using-the-ami","title":"Using the AMI","text":""},{"location":"assets/packer/virtual-workstations/lightweight/index.html#option-1-aws-console-manual-deployment","title":"Option 1: AWS Console (Manual Deployment)","text":"<ol> <li>Launch Instance:</li> <li>Go to EC2 Console \u2192 Launch Instance</li> <li>Search for your AMI ID (from Packer build output)</li> <li>Select GPU instance type: <code>g4dn.xlarge</code> or larger</li> <li> <p>Configure security group to allow RDP (port 3389) or DCV (port 8443)</p> </li> <li> <p>Connect via DCV:</p> </li> <li>Install DCV Client: https://download.nice-dcv.com/</li> <li>Connect to: <code>https://&lt;instance-ip&gt;:8443</code></li> <li> <p>Login with Administrator account</p> </li> <li> <p>Install Additional Software:</p> </li> <li>Use Chocolatey: <code>choco install vscode</code></li> <li>Use PowerShell: Install modules as needed</li> <li>Manual installers: Download and install directly</li> </ol>"},{"location":"assets/packer/virtual-workstations/lightweight/index.html#option-2-terraform-infrastructure-as-code","title":"Option 2: Terraform (Infrastructure as Code)","text":"<pre><code>resource \"aws_instance\" \"workstation\" {\n  ami           = \"ami-0123456789abcdef0\"  # Your AMI ID\n  instance_type = \"g4dn.xlarge\"\n  subnet_id     = var.subnet_id\n\n  vpc_security_group_ids = [aws_security_group.workstation.id]\n\n  tags = {\n    Name = \"Developer Workstation\"\n  }\n}\n</code></pre>"},{"location":"assets/packer/virtual-workstations/lightweight/index.html#option-3-vdi-module-advanced-automation","title":"Option 3: VDI Module (Advanced Automation)","text":"<p>For multi-user environments with automated user management:</p> <pre><code>module \"vdi\" {\n  source = \"path/to/vdi/module\"\n\n  presets = {\n    \"developer\" = {\n      ami           = \"ami-0123456789abcdef0\"  # Your AMI ID\n      instance_type = \"g4dn.xlarge\"\n      software_packages = [\"vscode\", \"git\"]\n    }\n  }\n}\n</code></pre>"},{"location":"assets/packer/virtual-workstations/lightweight/index.html#alternative-amis","title":"Alternative AMIs","text":"<p>For faster boot times with pre-installed software, consider:</p> <ul> <li>UE GameDev AMI - Visual Studio 2022 + Epic Games Launcher (UE requires manual install)</li> </ul>"},{"location":"assets/packer/virtual-workstations/lightweight/index.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Build fails: Check VPC/subnet configuration and internet connectivity</li> <li>GPU drivers: Ensure g4dn instance type for proper driver testing</li> <li>WinRM timeout: Increase timeout in Packer template if needed</li> </ul>"},{"location":"assets/packer/virtual-workstations/ue-gamedev/index.html","title":"VDI Unreal Engine GameDev AMI","text":"<p>This Packer template creates a Windows Server 2025 AMI optimized for Unreal Engine game development with pre-installed development tools.</p>"},{"location":"assets/packer/virtual-workstations/ue-gamedev/index.html#whats-included","title":"What's Included","text":"<p>Base Infrastructure (from shared/base_infrastructure.ps1):</p> <ul> <li>Windows Server 2025 base</li> <li>NVIDIA GRID drivers (GPU instances)</li> <li>Amazon DCV remote desktop server</li> <li>AWS CLI and PowerShell modules</li> <li>Git, Python, Chocolatey</li> <li>Perforce (P4 command line + P4V visual client)</li> <li>Active Directory management tools</li> </ul> <p>Unreal Engine Development Stack (from unreal_development_stack.ps1):</p> <ul> <li>Visual Studio 2022 Community with game development workloads:</li> <li>Managed Desktop (.NET)</li> <li>Native Desktop (C++)</li> <li>.NET Cross-Platform Development</li> <li>Visual C++ Diagnostic Tools</li> <li>Address Sanitizer</li> <li>Windows 10 SDK (10.0.18362.0)</li> <li>Unreal Engine Component</li> <li>Epic Games Launcher</li> <li>Desktop shortcut for Epic Games Launcher</li> </ul>"},{"location":"assets/packer/virtual-workstations/ue-gamedev/index.html#build-instructions","title":"Build Instructions","text":"<ol> <li>Navigate to template directory:</li> </ol> <pre><code>cd assets/packer/virtual-workstations/ue-gamedev/\n</code></pre> <ol> <li>Build AMI:</li> </ol> <pre><code>packer build windows-server-2025-ue-gamedev.pkr.hcl\n</code></pre> <ol> <li>Optional - Custom variables:</li> </ol> <pre><code># Override instance type or other settings\npacker build -var 'instance_type=g4dn.4xlarge' windows-server-2025-ue-gamedev.pkr.hcl\n</code></pre>"},{"location":"assets/packer/virtual-workstations/ue-gamedev/index.html#build-time-requirements","title":"Build Time &amp; Requirements","text":"<ul> <li>Estimated Build Time: 45-60 minutes</li> <li>Instance Type: g4dn.2xlarge (default, GPU required)</li> <li>Storage: 150GB root volume (larger for Visual Studio + UE)</li> <li>Network: Requires internet access for software downloads</li> </ul>"},{"location":"assets/packer/virtual-workstations/ue-gamedev/index.html#post-build-setup","title":"Post-Build Setup","text":"<p>Unreal Engine Installation:</p> <ul> <li>Epic Games Launcher is pre-installed</li> <li>Unreal Engine must be installed manually after first login:</li> <li>Launch Epic Games Launcher from desktop</li> <li>Create/sign in to Epic Games account</li> <li>Navigate to Unreal Engine tab</li> <li>Click \"Install Engine\" and select desired version</li> <li>Choose installation location (recommend D:\\ drive)</li> </ul> <p>Why Manual UE Installation:</p> <ul> <li>Requires Epic Games account authentication</li> <li>Requires EULA acceptance</li> <li>No silent installation method available</li> <li>User-specific licensing requirements</li> </ul>"},{"location":"assets/packer/virtual-workstations/ue-gamedev/index.html#usage-with-vdi-module","title":"Usage with VDI Module","text":"<p>Use the resulting AMI with the VDI Terraform module:</p> <pre><code>module \"vdi\" {\n  source = \"./modules/vdi\"\n\n  templates = {\n    \"ue-developer\" = {\n      instance_type = \"g4dn.xlarge\"\n      ami = \"ami-0123456789abcdef0\"  # Your built AMI ID\n      # No need for software_packages - already installed\n    }\n  }\n\n  users = {\n    \"game-dev\" = {\n      given_name = \"Game\"\n      family_name = \"Developer\"\n      type = \"user\"\n    }\n  }\n}\n</code></pre>"},{"location":"assets/packer/virtual-workstations/ue-gamedev/index.html#development-ready-features","title":"Development Ready Features","text":"<p>Immediate Development:</p> <ul> <li>Visual Studio 2022 with all UE workloads</li> <li>Git for version control</li> <li>Perforce for enterprise VCS</li> <li>Python for scripting</li> <li>AWS CLI for cloud integration</li> </ul> <p>GPU Acceleration:</p> <ul> <li>NVIDIA GRID drivers pre-installed</li> <li>DCV hardware acceleration enabled</li> <li>Ready for UE rendering and CUDA development</li> </ul> <p>User Experience:</p> <ul> <li>Desktop shortcuts for all tools</li> <li>Optimized PATH configuration</li> <li>Development-friendly PowerShell profile</li> </ul>"},{"location":"assets/packer/virtual-workstations/ue-gamedev/index.html#troubleshooting","title":"Troubleshooting","text":"<p>Build Failures:</p> <ul> <li>Ensure GPU instance type (g4dn.* required)</li> <li>Check internet connectivity for downloads</li> <li>Verify AWS credentials and permissions</li> </ul> <p>Visual Studio Issues:</p> <ul> <li>Build may take 45+ minutes (normal)</li> <li>Large download sizes require stable connection</li> <li>Workload installation is comprehensive</li> </ul> <p>Epic Games Launcher:</p> <ul> <li>Pre-installed but requires user login</li> <li>UE installation is user-initiated</li> <li>Account creation may be required</li> </ul>"},{"location":"assets/packer/virtual-workstations/ue-gamedev/index.html#alternative-templates","title":"Alternative Templates","text":"<ul> <li>Lightweight AMI - Runtime software customization</li> </ul> <p>For faster iteration during development, consider the lightweight template with runtime software installation via the VDI module.</p>"},{"location":"docs/index.html","title":"Welcome to the Cloud Game Development Toolkit","text":"<p>Info</p> <p>This project is under active development and community contributions are welcomed!. If you would like to see something in this repository please create a feature request in the Issues tab. If you'd like to contribute, raise a pull request. You'll find our contribution guidelines here.</p> <p>The Cloud Game Development Toolkit (a.k.a. CGD Toolkit) is a collection of templates and configurations for deploying game development infrastructure and tools on AWS.</p> <p>Below are key tenets driving project's focus:</p> <ul> <li>This is a fork-first, open-source project. We know that every game project is unique, so fork the repo, create your own branches for customization, and sync as appropriate. If you build something that can benefit other game developers, feel free to share via PR, as we encourage contributions!</li> <li>Meet game developers where they are. We aim to minimize learning curves and introducing new technologies where possible by building solutions that incorporate tools and software that are already widely used across the game industry and among our existing AWS for Games customers.</li> <li>Solutions are built for AWS. This project is focused on improving the game development experience on AWS and does not try to standardize solutions for deployment across many hosting platforms. In our experience, doing so is generally difficult, unecessary, and fraught with tradeoffs. If AWS is not your jam, you're welcome to fork and customize as needed (see above)!</li> </ul>"},{"location":"docs/index.html#getting-started","title":"Getting Started","text":"<p>Getting Started</p>"},{"location":"docs/index.html#license","title":"License","text":"<p>This project is licensed under the MIT-0 License.</p>"},{"location":"docs/changelog.html","title":"Changelog","text":""},{"location":"docs/changelog.html#unreleased","title":"Unreleased","text":""},{"location":"docs/changelog.html#latest-2025-07-29","title":"latest - 2025-07-29","text":""},{"location":"docs/changelog.html#v115-2025-07-29","title":"v1.1.5 - 2025-07-29","text":""},{"location":"docs/changelog.html#bug-fixes","title":"Bug Fixes","text":"<ul> <li>fixes button/link to getting started page in homepage</li> <li>update perforce tf test to support branches on forks as well as source repo</li> <li>Minor Perforce module copy/paste naming resolution (#645)</li> <li>Fixes typo in code block in samples DDC readme</li> <li>hardcode protocol to appease checkov</li> <li>Update SG reference in P4 FSxN Example (#640)</li> <li>p4: Fix typo'ed output.shared_application_load_balancer_arn</li> </ul>"},{"location":"docs/changelog.html#chore","title":"Chore","text":"<ul> <li>remove kevon from description :( (#672)</li> <li>update dependabot configuration</li> <li>deps: bump the random-provider group across 9 directories with 1 update (#653)</li> <li>deps: bump NetApp/netapp-ontap</li> <li>deps: bump NetApp/netapp-ontap in /samples/simple-build-pipeline</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump the aws-provider group across 8 directories with 1 update</li> <li>deps: bump the aws-provider group across 8 directories with 1 update (#662)</li> <li>deps: bump hashicorp/local in /modules/perforce/modules/p4-server</li> <li>deps: bump hashicorp/local in /modules/perforce</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump ossf/scorecard-action from 2.4.1 to 2.4.2</li> <li>deps: bump aquasecurity/trivy-action from 0.30.0 to 0.31.0</li> </ul>"},{"location":"docs/changelog.html#docs","title":"Docs","text":"<ul> <li>fixed broken links in getting started guide</li> <li>horde: add alb http listeners to README.md</li> </ul>"},{"location":"docs/changelog.html#features","title":"Features","text":"<ul> <li>Packer template for Cloud Game Development virtual workstation AMI (#651)</li> <li>Unity Accelerator asset caching proxy</li> <li>horde: allow users to bring their own horde-server images (#643)</li> <li>horde: add HTTP redirect listeners to ALBs</li> <li>p4: allow users to specify a private ip (#665)</li> <li>p4: p4_configure.sh attempts to use --fqdn if passed (#666)</li> </ul>"},{"location":"docs/changelog.html#v114-2025-06-09","title":"v1.1.4 - 2025-06-09","text":""},{"location":"docs/changelog.html#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Remove commented out NetApp volume resources and cleanup IAM managed policies</li> <li>Resolved EC2 DNS self-signed certificate bug in P4 Server packer template</li> <li>Adding cloud DDC sample for mkdocs.yml</li> <li>helix swarm: helix swarm does not support horizontal scaling, so helix swarm container count is now set to 1</li> </ul>"},{"location":"docs/changelog.html#chore_1","title":"Chore","text":"<ul> <li>Add Terraform tests for new Perforce module (#604)</li> <li>regenerate CHANGELOG.md for 2025-03-19</li> <li>Minor maintenance to Helix Core module</li> <li>Minor Helix Authentication fixes</li> <li>regenerate CHANGELOG.md for 2025-06-09</li> <li>Addressed IAM policy warnings for Helix Swarm</li> <li>deps: bump actions/github-script from 6 to 7</li> <li>deps: bump mkdocs-material from 9.6.11 to 9.6.12 in /docs</li> <li>deps: bump xt0rted/pull-request-comment-branch from 1 to 3</li> <li>deps: bump actions/checkout from 3 to 4</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump mkdocs-material from 9.6.9 to 9.6.11 in /docs</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump aquasecurity/trivy-action from 0.29.0 to 0.30.0</li> <li>deps: bump mkdocs-material from 9.6.8 to 9.6.9 in /docs</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.8 to 9.6.9 in /docs</li> <li>deps: bump actions/upload-artifact from 4.6.1 to 4.6.2</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.12 to 9.6.14 in /docs</li> </ul>"},{"location":"docs/changelog.html#code-refactoring","title":"Code Refactoring","text":"<ul> <li>Update Simple Build Pipeline sample to use new Perforce parent module (#608)</li> <li>Perforce modules consolidated to simplify shared resource creation (#585)</li> <li>Updated Perforce complete example to remove NLB front for Helix Core</li> <li>reorganize unreal cloud ddc module structure</li> </ul>"},{"location":"docs/changelog.html#docs_1","title":"Docs","text":"<ul> <li>Adjustments to mkdocs structure, and updates to \"getting started\" and Perforce documentation. (#612)</li> <li>updates and expands on <code>unreal-cloud-ddc-intra-cluster</code> installation and usage docs</li> <li>fixes relative path for <code>unreal-cloud-ddc-infra</code> and <code>unreal-cloud-ddc-intra-cluster</code> Terraform module docs</li> <li>add unreal fest video to horde module</li> <li>TeamCity: Adding TeamCity module docs and example architecture</li> </ul>"},{"location":"docs/changelog.html#features_1","title":"Features","text":"<ul> <li>Adds debug variable and flag</li> <li>Simple example deployment of Helix Core backed by FSxN</li> <li>FSxN ISCSI provisioning for Helix Core module</li> <li>Modified p4_configure.sh to mount ISCSI volumes from FSxN</li> </ul>"},{"location":"docs/changelog.html#v113-alpha-2025-03-19","title":"v1.1.3-alpha - 2025-03-19","text":""},{"location":"docs/changelog.html#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>create_external_alb shouldn't block internal SG Ingress rules</li> <li>alb_subnet variables should not be required if create boolean is false</li> <li>Attaching perforce web service ALB to target group</li> <li>use provided admin password secret for Helix Authentication Service ADMIN_PASSWD, instead of the username secret</li> <li>AMI version bump for Helix Core, region variable made optional</li> </ul>"},{"location":"docs/changelog.html#chore_2","title":"Chore","text":"<ul> <li>update dependabot configuration to include unreal modules</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.4 to 9.6.7 in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.4 to 9.6.7 in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump actions/upload-artifact from 4.6.0 to 4.6.1</li> <li>deps: bump ossf/scorecard-action from 2.4.0 to 2.4.1</li> <li>deps: bump hashicorp/random</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.7 to 9.6.8 in /docs</li> <li>deps: bump the random-provider group across 4 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.3 to 9.6.4 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.2 to 9.6.4 in /docs</li> <li>deps: bump actions/upload-artifact from 4.5.0 to 4.6.0</li> <li>deps: bump mkdocs-material from 9.6.7 to 9.6.8 in /docs</li> <li>deps: bump hashicorp/aws</li> <li>deps: bump mkdocs-material from 9.6.2 to 9.6.3 in /docs</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.1 to 9.6.2 in /docs</li> <li>deps: bump mkdocs-material from 9.5.50 to 9.6.2 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump release-drafter/release-drafter from 6.0.0 to 6.1.0</li> <li>deps: bump mkdocs-material from 9.5.49 to 9.5.50 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> </ul>"},{"location":"docs/changelog.html#docs_2","title":"Docs","text":"<ul> <li>fix broken link in readme</li> <li>Perforce: Updating documentation for Perforce Complete example reference architecture</li> </ul>"},{"location":"docs/changelog.html#features_2","title":"Features","text":"<ul> <li>Helix Authentication Service: Shifting ALB creation to support external networking configuration</li> <li>Helix Core: Plaintext support for Helix Core, optional EIP creation</li> <li>Helix Core: Adding plaintext variable to p4_configre.sh</li> <li>Helix Swarm: Shifting ALB creation to support external networking configuration</li> <li>Perforce Example: Update complete example for shared networking configuration across services</li> <li>TeamCity Example: example terraform configuration for deploying TeamCity module</li> <li>TeamCity Server: terraform module for deploying TeamCity server on ECS Fargate</li> </ul>"},{"location":"docs/changelog.html#v112-alpha-2024-12-20","title":"v1.1.2-alpha - 2024-12-20","text":""},{"location":"docs/changelog.html#chore_3","title":"Chore","text":"<ul> <li>regenerate CHANGELOG.md for 2024-12-20</li> <li>ignore tf backend.tf files in .gitignore</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump actions/upload-artifact from 4.4.3 to 4.5.0</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> </ul>"},{"location":"docs/changelog.html#docs_3","title":"Docs","text":"<ul> <li>removed READMEs from source directories and moved them to their own dedicated docs pages in docs/ dir</li> <li>update contributor documentation to include table of contents</li> <li>updates to doc formatting and fixed broken links</li> </ul>"},{"location":"docs/changelog.html#v111-alpha-2024-12-17","title":"v1.1.1-alpha - 2024-12-17","text":""},{"location":"docs/changelog.html#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>Added service target group ARNs as outputs for HAS and Swarm</li> <li>Adds defaults to <code>vpc_id</code> and <code>subnet_id</code> variables</li> <li>bash error causing build failure when running p4_configure.sh (#367)</li> <li>horde: add JwtIssuer to ensure container retains agents on restart</li> <li>horde: allow inbound access to horde agents on ports 7000-7010 from other horde agents</li> <li>perforce: fixed minor issues in p4_configure.sh</li> <li>perforce: add Unicode support and fix main module to handle existing security groups</li> </ul>"},{"location":"docs/changelog.html#chore_4","title":"Chore","text":"<ul> <li>make SELinux label updates configurable</li> <li>remove packer assets .ci directory (#337)</li> <li>fix tag names so that they match recommended best practices (#343)</li> <li>define nat gateway routes for private route tables outside of aws_route_table resources in samples and modules (#354)</li> <li>adds triage label to our issue templates</li> <li>regenerate CHANGELOG.md for 2024-12-17</li> <li>document parameter values for '--unicode' flag</li> <li>provide appropriate association name for configuring Helix Core via SSM</li> <li>fix naming</li> <li>checkov: Suppresses CKV_AWS_378 rule (#339)</li> <li>deps: bump mkdocs-material from 9.5.42 to 9.5.44 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump aquasecurity/trivy-action from 0.28.0 to 0.29.0</li> <li>deps: bump mkdocs-material from 9.5.45 to 9.5.46 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.5.44 to 9.5.45 in /docs</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.7 to 1.0.8 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump actions/checkout from 3.0.0 to 4.2.2</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump hashicorp/setup-terraform from 1 to 3</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump mkdocs-material from 9.5.41 to 9.5.42 in /docs</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.6 to 1.0.7 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump aquasecurity/trivy-action from 0.24.0 to 0.28.0</li> <li>deps: bump mkdocs-material from 9.5.40 to 9.5.41 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump python from 3.12.7 to 3.13.0 in /docs (#349)</li> <li>deps: bump actions/upload-artifact from 4.4.0 to 4.4.3 (#356)</li> <li>deps: bump mkdocs-material from 9.5.39 to 9.5.40 in /docs (#359)</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.5 to 1.0.6 in /docs (#345)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.5.37 to 9.5.39 in /docs (#335)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#344)</li> <li>deps: bump mkdocs-material from 9.5.46 to 9.5.48 in /docs</li> <li>deps: bump python from 3.12.6 to 3.12.7 in /docs (#340)</li> <li>deps: bump mkdocs-material from 9.5.48 to 9.5.49 in /docs</li> <li>deps: bump python from 3.13.0 to 3.13.1 in /docs</li> </ul>"},{"location":"docs/changelog.html#docs_4","title":"Docs","text":"<ul> <li>clarify that modules are intended to be depended on, and samples are reference implementations meant to be copied and modified</li> <li>fix formatting of simple build pipeline docs</li> <li>fix formatting of local.tf in simple build pipeline docs</li> <li>fix formatting of jenkins pipeline assets page</li> <li>clarify use case of Ansible playbooks vs Packer templates</li> <li>clarify that deploying multiple samples independently is not supported</li> <li>point users explicitly to a Classic GitHub Personal Access Token</li> <li>fix typo in getting started guide</li> <li>Updates the getting started instructions for the simple build pipeline sample</li> </ul>"},{"location":"docs/changelog.html#features_3","title":"Features","text":"<ul> <li>perforce: implement Helix Core setup playbook</li> </ul>"},{"location":"docs/changelog.html#v110-alpha-2024-10-01","title":"v1.1.0-alpha - 2024-10-01","text":""},{"location":"docs/changelog.html#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>improve stability of build agent packer scripts, adjust winrm timeout to 15 minutes, remove packer variables that aren't needed (#318)</li> </ul>"},{"location":"docs/changelog.html#chore_5","title":"Chore","text":"<ul> <li>update changelog (#305)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#323)</li> <li>deps: bump mkdocs-material from 9.5.35 to 9.5.37 in /docs (#314)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#324)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#298)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#291)</li> <li>deps: bump the random-provider group across 5 directories with 1 update (#310)</li> <li>deps: bump mkdocs-material from 9.5.34 to 9.5.35 in /docs (#287)</li> </ul>"},{"location":"docs/changelog.html#docs_5","title":"Docs","text":"<ul> <li>add perforce complete example in docs (#333)</li> <li>updates to documentation (#329)</li> </ul>"},{"location":"docs/changelog.html#features_4","title":"Features","text":"<ul> <li>install requirements for (auto)mounting FSx volumes on Jenkins Windows build agents (#319)</li> <li>helix-core: add ARM64 support (#239)</li> </ul>"},{"location":"docs/changelog.html#v101-alpha-2024-09-16","title":"v1.0.1-alpha - 2024-09-16","text":""},{"location":"docs/changelog.html#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>changelog automation (#261)</li> <li>adding branch creation to workflow (#259)</li> <li>dependabot grouping terraform providers (#228)</li> <li>wait for cloud-init to complete prior to installing packages during Perforce Helix Core AMI creation (#193)</li> <li>changelog: GHA bot committer (#255)</li> <li>changelog: Add automated PR creation (#252)</li> <li>fsx_automounter: when FSx automounter can't list tags for an FSx volume, the AccessDenied exception is now treated as a warning (#226)</li> <li>p4_configure: resolve script execution errors and repair broken \u2026 (#232)</li> </ul>"},{"location":"docs/changelog.html#chore_6","title":"Chore","text":"<ul> <li>adjusting changelog automation to leverage GH api (#266)</li> <li>update changelog workflow (#284)</li> <li>update changelog (#285)</li> <li>deps: bump hashicorp/awscc from 1.10.0 to 1.11.0 in /samples/simple-build-pipeline (#220)</li> <li>deps: bump hashicorp/awscc from 1.9.0 to 1.10.0 in /modules/perforce/helix-core (#207)</li> <li>deps: bump mkdocs-material from 9.5.33 to 9.5.34 in /docs (#236)</li> <li>deps: bump actions/upload-artifact from 4.3.6 to 4.4.0 (#235)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#241)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#242)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#233)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#231)</li> <li>deps: bump mkdocs-material from 9.5.32 to 9.5.33 in /docs (#229)</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.3 to 1.0.5 in /docs (#263)</li> <li>deps: bump mkdocs-material from 9.5.31 to 9.5.32 in /docs (#211)</li> <li>deps: bump python from 3.12 to 3.12.6 in /docs (#243)</li> <li>deps: bump hashicorp/awscc from 1.9.0 to 1.10.0 in /modules/perforce/helix-authentication-service (#205)</li> <li>deps: bump hashicorp/aws from 5.62.0 to 5.63.1 in /samples/simple-build-pipeline (#216)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /modules/perforce/helix-authentication-service (#196)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-authentication-service (#197)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /modules/perforce/helix-core (#198)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-core (#199)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-swarm (#200)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /samples/simple-build-pipeline (#201)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /samples/simple-build-pipeline (#202)</li> <li>deps: bump mike from 2.1.2 to 2.1.3 in /docs (#189)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/jenkins (#195)</li> </ul>"},{"location":"docs/changelog.html#docs_6","title":"Docs","text":"<ul> <li>add openssf scorecard badge to readme (#219)</li> <li>link to installation instructions for required tools, fix packer command invocation instructions (#194)</li> <li>Windows Build AMI README (#187)</li> </ul>"},{"location":"docs/changelog.html#v100-alpha-2024-08-07","title":"v1.0.0-alpha - 2024-08-07","text":""},{"location":"docs/changelog.html#staging-2024-08-07","title":"staging - 2024-08-07","text":""},{"location":"docs/changelog.html#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>fix issue where SSH public key was not baked into the Windows Jenkins build agent AMI (#150)</li> <li>bug fixes for FSxZ storage in build farm (#152)</li> <li>allow Jenkins build agents to discover FSx volumes/snapshots and make outbound Internet connections (#147)</li> </ul>"},{"location":"docs/changelog.html#chore_7","title":"Chore","text":"<ul> <li>add CODEOWNERS file (#132)</li> <li>Updates to docs (#63)</li> <li>fix makefile (#65)</li> <li>Modify version handling in Docs (#66)</li> <li>deps: bump mkdocs-material from 9.5.27 to 9.5.28 in /docs (#135)</li> <li>deps: bump mkdocs-material from 9.5.26 to 9.5.27 in /docs (#77)</li> <li>deps: bump aquasecurity/trivy-action from 0.23.0 to 0.24.0 (#137)</li> <li>deps: bump actions/upload-artifact from 4.3.3 to 4.3.4 (#136)</li> <li>deps: bump actions/upload-artifact from 4.3.5 to 4.3.6 (#178)</li> <li>deps: bump mkdocs-material from 9.5.29 to 9.5.30 in /docs (#153)</li> <li>deps: bump mike from 2.1.1 to 2.1.2 in /docs (#110)</li> <li>deps: bump mkdocs-material from 9.5.28 to 9.5.29 in /docs (#144)</li> <li>deps: bump github/codeql-action from 3.25.8 to 3.25.10 (#69)</li> <li>deps: bump ossf/scorecard-action from 2.3.3 to 2.4.0 (#167)</li> <li>deps: bump actions/upload-artifact from 4.3.4 to 4.3.5 (#171)</li> <li>deps: bump mkdocs-material from 9.5.30 to 9.5.31 in /docs (#172)</li> <li>deps: bump github/codeql-action from 3.24.9 to 3.25.8 (#53)</li> <li>deps: bump mkdocs-material from 9.5.25 to 9.5.26 in /docs (#54)</li> </ul>"},{"location":"docs/changelog.html#code-refactoring_1","title":"Code Refactoring","text":"<ul> <li>Perforce Helix Core AMI revamp, simple build pipeline DNS (#73)</li> </ul>"},{"location":"docs/changelog.html#docs_7","title":"Docs","text":"<ul> <li>update changelog (#181)</li> <li>update main docs page (#179)</li> <li>update layout of documentation main page theme (#175)</li> <li>update documentation (#163)</li> <li>update workflow for docs (#129)</li> <li>update workflow (#128)</li> <li>fix workflow to use gh inputs from workflow (#127)</li> <li>update to docs and flip release workflow to manual (#126)</li> <li>fix commit depth (#125)</li> <li>modify the workflow for docs release and update documentation (#124)</li> <li>fix docs ci (#123)</li> <li>modify git fetch-depth for docs ci (#121)</li> <li>update README.md (#119)</li> <li>consolidate Ansible playbooks under assets (#117)</li> <li>fix url to documentation to point to /latest (#80)</li> <li>add GH Pull Request template (#67)</li> <li>updates workflow and adds changelog automation (#61)</li> <li>add issue template for RFCs (#57)</li> <li>add git-chglog for changelog generation (#49)</li> <li>enable workflow dispatch (#36)</li> <li>fix docs release workflow (#34)</li> <li>convert docs releases to use mike (#33)</li> <li>adds markdown docs for assets, modules, playbooks, and samples (#32)</li> <li>adds issue template for submitting maintenance issues (#31)</li> <li>Adds documentation and GH workflow for build/publish of docs (#21)</li> <li>Updates to project README (#20)</li> <li>Adds project docs (#13)</li> </ul>"},{"location":"docs/changelog.html#features_5","title":"Features","text":"<ul> <li>Added getting-started documentation for quickstart with Simple Build Pipeline (#177)</li> <li>Updates to CI configurations for pre-commit and GHA (#154)</li> <li>Helix Authentication Extension (#82)</li> <li>enable web based administration through variables for HAS (#79)</li> <li>complete sample with both Jenkins and Perforce modules (#60)</li> <li>Add packer build agent templates for Linux (Ubuntu Jammy 22.04, Amazon Linux 2023) (#46)</li> <li>devops: Add new DevOps playbook files (#76)</li> <li>packer: switch AMI from Rocky Linux to Amazon Linux 2023 and up\u2026 (#141)</li> </ul>"},{"location":"docs/contributing.html","title":"Table of contents","text":"<ul> <li>Contributing Guidelines</li> <li>Reporting Bugs/Feature Requests</li> <li>Contributing via Pull Requests</li> <li>Conventional Commits</li> <li>Finding contributions to work on</li> <li>Code of Conduct</li> <li>Security issue notifications</li> <li>Licensing</li> </ul>"},{"location":"docs/contributing.html#contributing-guidelines","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"docs/contributing.html#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"docs/contributing.html#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages. See the conventional commits section for more details.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p>"},{"location":"docs/contributing.html#conventional-commits","title":"Conventional Commits","text":"<p>This project uses Conventional Commits following the release of v1.0.0-alpha. These conventions ensure that the commit history of the project remains readable, and supports extensive automation around pull request creation, release cadence, and documentation.</p> <p>We do not enforce conventional commits on contributors. We do require that pull request titles follow convention so that the changelog and release automation work as expected.</p>"},{"location":"docs/contributing.html#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"docs/contributing.html#building-and-testing-project-documentation","title":"Building and Testing Project Documentation","text":"<p>This project uses Material for MkDocs to generate documentation. Content is sourced from markdown files throughout the project, including module README.md files.</p>"},{"location":"docs/contributing.html#local-development","title":"Local Development","text":"<p>For day-to-day documentation work, use the container-based local development server:</p> <pre><code>make docs-serve\n</code></pre> <p>This command:</p> <ul> <li>Builds a container image with all required dependencies</li> <li>Starts a local development server at http://127.0.0.1:8000</li> <li>Provides live reload - changes appear automatically in your browser</li> <li>Requires no Python installation or VERSION/ALIAS parameters</li> <li>Works with Docker (default) or Finch by setting <code>CONTAINER_RUNTIME=finch</code> in your environment.</li> </ul> <p>To build the documentation without serving it:</p> <pre><code>make docs-build\n</code></pre> <p>This validates that your changes build successfully using <code>mkdocs build --strict</code> in a container. The built site will be in the <code>./site</code> directory.</p>"},{"location":"docs/contributing.html#pre-commit-validation","title":"Pre-commit Validation","text":"<p>Documentation is validated automatically before commit:</p> <ul> <li>Link checking: Verifies all links in staged markdown files</li> <li>Markdown linting: Ensures consistent markdown formatting</li> </ul> <p>Install pre-commit hooks:</p> <pre><code>pre-commit install\n</code></pre> <p>Optional - Pretty Formatter for Markdownlint:</p> <p>For better-looking markdown linting output, you can install the pretty formatter:</p> <pre><code>npm install -g markdownlint-cli2-formatter-pretty\n</code></pre> <p>This is optional but provides cleaner, color-coded output. The linting will work without it.</p>"},{"location":"docs/contributing.html#pull-request-validation","title":"Pull Request Validation","text":"<p>When you open a PR with documentation changes:</p> <ol> <li>Changed files are validated (links and markdown linting)</li> <li>Full documentation site is built to ensure integrity</li> <li>Preview deployment is created at <code>preview/pr-{number}</code></li> <li>Comment is posted with preview URL</li> </ol>"},{"location":"docs/contributing.html#main-branch-deployment","title":"Main Branch Deployment","text":"<p>When your PR is merged to main:</p> <ul> <li>Documentation automatically deploys to \"latest\"</li> <li>Full regression testing runs (all files validated)</li> <li>Issues are created if validation fails</li> </ul>"},{"location":"docs/contributing.html#versioned-releases","title":"Versioned Releases","text":"<p>Versioned documentation is created only for official releases:</p> <ol> <li>Use the \"Build Docs and Publish to gh-pages\" GitHub Action</li> <li>Manually specify VERSION and ALIAS</li> <li>This is done rarely for major/minor releases</li> </ol>"},{"location":"docs/contributing.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"docs/contributing.html#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"docs/contributing.html#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"docs/getting-started.html","title":"Getting Started","text":"<p>Welcome to the Cloud Game Development Toolkit . There are a number of ways to use this repository depending on your development needs. This guide will introduce some of the key features of the project, and provide detailed instructions for deploying your game studio on AWS.</p>"},{"location":"docs/getting-started.html#introduction-to-repository-structure","title":"Introduction to Repository Structure","text":""},{"location":"docs/getting-started.html#assets","title":"Assets","text":"<p>An asset is a singular template, script, or automation document that may prove useful in isolation. Currently, the Toolkit contains three types of assets: Ansible playbooks, Jenkins pipelines, and Packer templates. Each of these assets can be used in isolation.</p> <p>For more information about assets, consult the detailed documentation.</p>"},{"location":"docs/getting-started.html#modules","title":"Modules","text":"<p>A module is a reusable Terraform configuration encapsulating all of the resources needed to deploy a particular workload on AWS. These modules are highly configurable through variables, and provide necessary outputs for building interconnected architectures. We recommend reviewing the Terraform module documentation if you are unfamiliar with this concept. Modules are designed for you to depend on in your own Terraform modules, and we don't expect you to have to make any modifications to them; that said, if a module doesn't meet your needs, please raise an issue!</p> <p>For more information about modules, consult the detailed documentation.</p>"},{"location":"docs/getting-started.html#samples","title":"Samples","text":"<p>A sample is a complete reference architecture that stitches together modules and first-party AWS services. A sample is deployed with Terraform, and is the best way to get started with the Cloud Game Development Toolkit . Samples are designed for you to copy from and modify as needed to suit your architecture and needs.</p> <p>Note: Because samples may deploy resources that have unique name constraints, we cannot guarantee that two different samples can be deployed into the same AWS account without modifying either of the samples to integrate shared infrastructure or resolve conflicts. If you're interested in using functionality from multiple samples, we recommend that you use them as reference material to base your own infrastructure off of.</p> <p>For more information about samples, consult the detailed documentation.</p> <p>If you're new to the project, we recommend starting by deploying one of the samples, such as the Simple Build Pipeline.</p>"},{"location":"docs/security.html","title":"Security","text":""},{"location":"docs/security.html#overview","title":"Overview","text":"<p>This project is maintained by members of the AWS for Games technical community within AWS (i.e. Solutions Architects, Technical Account Managers, Software Engineers) who support the gaming industry. Design decisions and tradeoffs made throughout this project reflect our experiences working with game studios to build and maintain their development infrastructure and tools in the cloud. We encourage contributions from the community via Pull Requests, which are manually reviewed and tested by the core maintainers of the project before they are merged.</p> <p>We rely on Open Source Security (OpenSSF) Scorecard assessments to validate our project's security posture against a common set of open source project risks. We also self-certify our security practices against the standards defined by OpenSSF Best Practices Badge Program. The badges above are hyperlinks that you can follow to review the results of each.</p>"},{"location":"docs/security.html#reporting-a-vulnerability","title":"Reporting a vulnerability","text":"<p>If you discover a potential security issue in this project, we ask that you notify AWS/Amazon Security via our vulnerability reporting page or directly via email to aws-security@amazon.com.</p> <p>Please do not create a public GitHub issue.</p>"},{"location":"docs/assets/index.html","title":"Assets","text":"<p>Assets are reusable scripts, pipeline definitions, Dockerfiles, Packer templates, and other resources that might prove useful or are dependencies of any of the modules.</p> <p>Info</p> <p>Don't see an asset listed? Create a feature request for a new asset or learn how to contribute new assets to the project</p> Asset Type Description Packer Templates Packer templates provide an easy way to build machine images for commonly used game dev infrastructure. Currently the project includes Packer templates for UE5 build agents for Linux and Windows, as well as a Packer template for building a Perforce Helix Core version control AMI. Jenkins Pipelines Jenkins Pipelines for common game dev automation workflows Ansible Playbooks Automation scripts for reusable system level configurations. Unlike Packer templates, you can use these to add new functionality to existing EC2 instances. Dockerfiles (Coming Soon!) Dockerfiles for creating Docker images of commonly used game dev infrastructure. These are primarily used in scenarios where there aren't openly available pre-built images that address a use case, or significant customization is needed that warrants building an image"},{"location":"docs/assets/dockerfiles.html","title":"Dockerfiles","text":"<p>Coming soon.</p>"},{"location":"docs/assets/packer/index.html","title":"Packer Templates","text":"<p>Packer is a tool for simplifying and automating Amazon Machine Image (AMI) creation with code. It enables developers to create identical images for multiple platforms. The Packer templates provided in the Cloud Game Development Toolkit can be used to provision EC2 instances with common development tools preinstalled.</p> <p>Info</p> <p>Don't see a Packer template that solves your needs? Create a feature request for a new template or learn how to contribute new assets to the project</p> Template Description Linux Build Agents Provision C++ compilation machines on Amazon Linux 2023 and Ubuntu machines on both x86 and ARM based architectures with useful tools like compiler caches such as Octobuild preinstalled. Windows Build Agents Create Windows 2022 based instances capable of Unreal Engine compilation out of the box. P4 Server (formerly Helix Core) An Amazon Machine Image used for provisioning P4 Server on AWS. This AMI is required for deployment of the Perforce module Virtual Workstations (Windows) AWS Virtual Workstation AMI for Unreal Engine development"},{"location":"modules/index.html","title":"Modules","text":""},{"location":"modules/index.html#introduction","title":"Introduction","text":"<p>A module is an automated deployment of a game development workload (i.e. Perforce, Unreal Horde, Unreal Cloud DDC, etc.) that is implemented as a Terraform module. They are designed to provide flexibility and customization via input variables with defaults based on typical deployment architectures. They are designed to be depended on from other modules (including your own root module), easily integrate with each other, and provide relevant outputs to simplify permissions, networking, and access.</p> <p>Note: While the project focuses on Terraform modules today, this project may expand to provide options for implementations built in other IaC tools such as AWS CDK in the future.</p>"},{"location":"modules/index.html#available-modules","title":"Available Modules","text":"<p>Don't see a module listed? Create a feature request for a new module. If you'd like to contribute new modules to the project, see the general docs on contributing.</p> Module Description Status Perforce This module allows for deployment of Perforce resources on AWS. These are currently P4 Server (formerly Helix Core), P4Auth (formerly Helix Authentication Service), and P4 Code Review (formerly Helix Swarm). \u2705 External Access Unreal Horde This module allows for deployment of Unreal Horde on AWS. \u2705 External Access Unreal Cloud DDC This module allows for deployment of Unreal Cloud DDC (Derived Data Cache) on AWS. \u2705 External Access Jenkins This module allows for deployment of Jenkins on AWS. \u2705 External Access TeamCity This module allows for deployment of TeamCity resources on AWS. \u2705 External Access \ud83d\udda5\ufe0f VDI (Virtual Desktop Infrastructure) This module allows for deployment of Virtual Desktop Infrastructure (VDI) workstations on AWS with Amazon DCV remote access for game development teams. \u2705 External Access Monitoring Pending development - will provide unified monitoring across all CGD services. \ud83d\udea7 Future"},{"location":"modules/index.html#getting-started","title":"Getting Started","text":""},{"location":"modules/index.html#module-source-options","title":"Module Source Options","text":"<p>Due to the structure of the toolkit, all modules will have unified incremental versions during our release process. To reference a specific version, simply have a different source on a module-by-module basis. The same applies to the commit hash option (though less clear to track changes so not recommended for most users).</p> <p>Important, currently there is a staggered release process. When a PR is merged to main, that doesn't immediately trigger a new release. \u26a0\ufe0f should we be doing this still?</p> <p>Option 1: Git Release Tag (Recommended \u2705) Use case: Explicit version tracking. Easy to pin to specific versions of modules.</p> <pre><code>module \"example_module\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/path/to/module?ref=v1.1.5\"\n  # ... configuration\n}\n</code></pre> <p>Option 2: Specific Commit Hash Use case: If you want the latest and greatest, even before a release may be cut. As such we recommend caution with this option as well.</p> <pre><code>module \"example_module\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/path/to/module?ref=abc123def456\"\n  # ... configuration (replace abc123def456 with actual commit hash)\n}\n</code></pre> <p>Option 3: Git Branch (\u26a0\ufe0f Caution) Use case: If you don't care about tracking versions and just want to use whatever is the latest. Due to the fact that you cant track versions with this, we recommend high caution using this method.</p> <pre><code>module \"example_module\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/path/to/module?ref=main\"\n  # ... configuration\n}\n</code></pre> <p>Option 4: Submodule Reference You can also include the CGD Toolkit repository as a git submodule in your own infrastructure repository as a way of depending on the modules within an (existing) Terraform root module. Forking the CGD Toolkit and submoduling your fork may be a good approach if you intend to make changes to any modules. We recommend starting with the Terraform module documentation for a crash course in the way patterns that influence the way that the CGD Toolkit is designed. Note how you can use the module source argument to declare modules that use the CGD Toolkit's module source code.</p>"},{"location":"modules/index.html#basic-usage-pattern","title":"Basic Usage Pattern","text":"<pre><code>module \"service\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/service-name?ref=v1.2.0\"\n\n  # Networking (3-tier architecture)\n  application_subnets = var.private_subnets  # Primary business applications\n  service_subnets    = var.private_subnets   # Supporting services (databases, caches)\n\n  # Load balancer configuration\n  load_balancer_config = {\n    nlb = {\n      internet_facing = true               # External access\n      subnets        = var.public_subnets  # Load balancer placement\n    }\n    # ALB optional - module-specific support\n  }\n\n  # Security\n  security_groups = [aws_security_group.office_access.id]\n\n  # Service-specific configuration\n  # ... see individual module documentation\n}\n</code></pre>"},{"location":"modules/index.html#access-patterns","title":"Access Patterns","text":"<p>CGD Toolkit modules support three deployment patterns:</p>"},{"location":"modules/index.html#pattern-1-fully-public-development","title":"Pattern 1: Fully Public (Development)","text":"<p>Use Case: Development, testing, proof-of-concept</p> <pre><code>module \"ddc\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/unreal/unreal-cloud-ddc?ref=v1.2.0\"\n\n  application_subnets = var.public_subnets   # Applications in public subnets\n  service_subnets    = var.public_subnets    # Services in public subnets\n\n  load_balancer_config = {\n    nlb = {\n      internet_facing = true\n      subnets        = var.public_subnets\n    }\n  }\n}\n</code></pre>"},{"location":"modules/index.html#pattern-2-fully-private-high-security","title":"Pattern 2: Fully Private (High Security)","text":"<p>Use Case: High security, compliance, production</p> <pre><code>module \"ddc\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/unreal/unreal-cloud-ddc?ref=v1.2.0\"\n\n  application_subnets = var.private_subnets  # Applications in private subnets\n  service_subnets    = var.private_subnets   # Services in private subnets\n\n  load_balancer_config = {\n    nlb = {\n      internet_facing = false              # Internal load balancer\n      subnets        = var.private_subnets\n    }\n  }\n\n  # Requires VPN, Direct Connect, or bastion for access\n}\n</code></pre>"},{"location":"modules/index.html#pattern-3-hybrid-recommended","title":"Pattern 3: Hybrid (Recommended)","text":"<p>Use Case: Production with external access needs</p> <pre><code>module \"ddc\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/unreal/unreal-cloud-ddc?ref=v1.2.0\"\n\n  application_subnets = var.private_subnets  # Applications secure\n  service_subnets    = var.private_subnets   # Services secure\n\n  load_balancer_config = {\n    nlb = {\n      internet_facing = true               # External access via load balancer\n      subnets        = var.public_subnets\n    }\n  }\n\n  security_groups = [aws_security_group.office_access.id]  # Restricted access\n}\n</code></pre>"},{"location":"modules/index.html#load-balancer-configuration","title":"Load Balancer Configuration","text":"<p>Modules use a standardized <code>load_balancer_config</code> variable:</p>"},{"location":"modules/index.html#network-load-balancer-nlb-always-available","title":"Network Load Balancer (NLB) - Always Available","text":"<pre><code>load_balancer_config = {\n  nlb = {\n    enabled         = true                 # Required for most modules\n    internet_facing = true                 # true = public, false = internal\n    subnets        = var.public_subnets   # Where to place the NLB\n    name_suffix    = \"game-clients\"        # Optional naming\n  }\n}\n</code></pre>"},{"location":"modules/index.html#application-load-balancer-alb-module-specific","title":"Application Load Balancer (ALB) - Module-Specific","text":"<pre><code># Only supported by modules with multiple web services (e.g., Perforce)\nload_balancer_config = {\n  nlb = {\n    internet_facing = false\n    subnets        = var.private_subnets\n  }\n  alb = {\n    enabled         = true                 # Module must support ALB\n    internet_facing = true\n    subnets        = var.public_subnets\n    enable_waf     = true                  # Web Application Firewall\n  }\n}\n</code></pre> <p>Important: All subnet variables are user-defined:</p> <pre><code># Your root module defines subnets\nvariable \"public_subnets\" { type = list(string) }\nvariable \"private_subnets\" { type = list(string) }\n\n# Or use direct resource references\nsubnets = aws_subnet.public[*].id\n</code></pre>"},{"location":"modules/index.html#multi-region-deployment","title":"Multi-Region Deployment","text":"<p>\u26a0\ufe0f Important: Multi-region deployment within a single Terraform state is only recommended for workloads that are inherently benefited by multi-region architecture. For example, Unreal Cloud DDC is designed as a distributed cache where multi-region deployment provides substantial performance benefits for geographically distributed teams.</p> <p>When to use shared-state multi-region:</p> <ul> <li>Distributed caching systems (like DDC) where cross-region replication improves performance</li> <li>Global data synchronization where regions need to coordinate</li> <li>Workloads designed for multi-region as a core architectural pattern</li> </ul> <p>When to use separate deployments per region:</p> <ul> <li>Most applications that don't require cross-region coordination</li> <li>Independent regional services (separate Jenkins, separate Perforce instances)</li> <li>Disaster recovery scenarios where regions are meant to be isolated</li> </ul> <p>With AWS Provider v6, multi-region is simplified:</p> <pre><code># AWS Provider v6 - No provider aliases needed for AWS resources\nmodule \"ddc_us_east_1\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/unreal/unreal-cloud-ddc?ref=v1.2.0\"\n\n  region = \"us-east-1\"  # Automatic region inheritance\n  application_subnets = var.private_subnets_us_east_1\n\n  load_balancer_config = {\n    nlb = {\n      internet_facing = true\n      subnets        = var.public_subnets_us_east_1\n    }\n  }\n\n  # Non-enhanced providers still require explicit aliases\n  providers = {\n    kubernetes = kubernetes.us_east_1\n    helm       = helm.us_east_1\n  }\n}\n\nmodule \"ddc_us_west_2\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/unreal/unreal-cloud-ddc?ref=v1.2.0\"\n\n  region = \"us-west-2\"  # Automatic region inheritance\n  application_subnets = var.private_subnets_us_west_2\n\n  load_balancer_config = {\n    nlb = {\n      internet_facing = true\n      subnets        = var.public_subnets_us_west_2\n    }\n  }\n\n  # Non-enhanced providers still require explicit aliases\n  providers = {\n    kubernetes = kubernetes.us_west_2\n    helm       = helm.us_west_2\n  }\n}\n\n# Provider aliases required for non-enhanced providers\nprovider \"kubernetes\" {\n  alias = \"us_east_1\"\n  # Configuration for us-east-1 cluster\n}\n\nprovider \"kubernetes\" {\n  alias = \"us_west_2\"\n  # Configuration for us-west-2 cluster\n}\n\nprovider \"helm\" {\n  alias = \"us_east_1\"\n  kubernetes {\n    # Reference us-east-1 cluster\n  }\n}\n\nprovider \"helm\" {\n  alias = \"us_west_2\"\n  kubernetes {\n    # Reference us-west-2 cluster\n  }\n}\n</code></pre> <p>Important: While AWS Provider v6 supports enhanced region support, other providers (Kubernetes, Helm, kubectl) still require explicit provider aliases for multi-region deployments.</p>"},{"location":"modules/index.html#security-best-practices","title":"Security Best Practices","text":""},{"location":"modules/index.html#network-security","title":"Network Security","text":"<ul> <li>Private-first: Deploy applications in private subnets</li> <li>Restricted access: Use security groups, never <code>0.0.0.0/0</code> for ingress</li> <li>Load balancer isolation: Separate load balancer and application subnets</li> </ul>"},{"location":"modules/index.html#access-control","title":"Access Control","text":"<pre><code># Create restricted security groups\nresource \"aws_security_group\" \"office_access\" {\n  name_prefix = \"office-access\"\n  vpc_id      = var.vpc_id\n}\n\nresource \"aws_vpc_security_group_ingress_rule\" \"office_https\" {\n  security_group_id = aws_security_group.office_access.id\n  description       = \"HTTPS from office network\"\n  ip_protocol       = \"tcp\"\n  from_port         = 443\n  to_port           = 443\n  cidr_ipv4        = \"203.0.113.0/24\"  # Your office CIDR\n}\n\n# Use in module configuration\nmodule \"service\" {\n  security_groups = [aws_security_group.office_access.id]\n}\n</code></pre>"},{"location":"modules/index.html#contributing","title":"Contributing","text":"<p>For detailed information on contributing new modules or enhancing existing ones:</p> <ul> <li>Module Design Standards - Technical implementation guidelines for module contributors</li> <li>Contributing Guide - General contribution process and requirements</li> </ul> <p>Quick Start for Contributors:</p> <ol> <li>Review existing module patterns and architecture</li> <li>Follow the standardized variable naming and structure</li> <li>Implement comprehensive tests and examples</li> <li>Ensure security best practices are followed</li> </ol>"},{"location":"modules/index.html#module-architecture","title":"Module Architecture","text":"<p>CGD Toolkit modules follow a standardized structure:</p> <pre><code>modules/service-name/\n\u251c\u2500\u2500 main.tf              # Parent module orchestration\n\u251c\u2500\u2500 variables.tf         # Input variables with validation\n\u251c\u2500\u2500 outputs.tf           # Module outputs\n\u251c\u2500\u2500 versions.tf          # Terraform and provider version constraints\n\u251c\u2500\u2500 README.md            # Module documentation\n\u251c\u2500\u2500 modules/             # Submodules for complex components\n\u2502   \u251c\u2500\u2500 infra/          # Infrastructure submodule (AWS resources)\n\u2502   \u2514\u2500\u2500 services/       # Services submodule (Kubernetes/Helm)\n\u251c\u2500\u2500 tests/              # Terraform tests\n\u2502   \u251c\u2500\u2500 setup/          # Shared test configuration\n\u2502   \u2514\u2500\u2500 *.tftest.hcl    # Unit and integration tests\n\u2514\u2500\u2500 examples/           # Working examples\n    \u251c\u2500\u2500 single-region-basic/\n    \u2514\u2500\u2500 multi-region-basic/\n</code></pre>"},{"location":"modules/index.html#key-components","title":"Key Components","text":"<ul> <li>Parent module: Orchestrates submodules and provides user interface</li> <li>Submodules: Separate AWS resources (<code>infra/</code>) from Kubernetes resources (<code>services/</code>)</li> <li>Examples: Complete, deployable configurations showing usage patterns</li> <li>Tests: Automated validation of module functionality</li> </ul>"},{"location":"modules/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"modules/index.html#common-issues","title":"Common Issues","text":""},{"location":"modules/index.html#provider-configuration-errors","title":"Provider Configuration Errors","text":"<pre><code># Error: Invalid provider configuration\n# Solution: Ensure AWS Provider v6+ for enhanced region support\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"&gt;= 6.0.0\"\n    }\n  }\n}\n</code></pre>"},{"location":"modules/index.html#subnet-configuration","title":"Subnet Configuration","text":"<pre><code># Error: Load balancer subnets must be provided\n# Solution: Specify subnets in load_balancer_config\nload_balancer_config = {\n  nlb = {\n    subnets = var.public_subnets  # Must provide subnet IDs\n  }\n}\n</code></pre>"},{"location":"modules/index.html#security-group-access","title":"Security Group Access","text":"<pre><code># Error: Cannot access service\n# Solution: Check security group rules and CIDR blocks\nsecurity_groups = [aws_security_group.office_access.id]\n</code></pre>"},{"location":"modules/index.html#getting-help","title":"Getting Help","text":"<ul> <li>Module Documentation: Each module has detailed README with examples</li> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Ask questions</li> </ul>"},{"location":"modules/index.html#next-steps","title":"Next Steps","text":"<ol> <li>Choose your access pattern (public, private, or hybrid)</li> <li>Review module-specific documentation for detailed configuration options</li> <li>Start with examples to understand usage patterns</li> <li>Deploy in development environment first</li> <li>Scale to production with appropriate security controls</li> </ol>"},{"location":"modules/CONTRIBUTING.html","title":"\ud83d\udccb Module Contribution Guide","text":"<p>This guide defines the gold standards for building excellent CGD Toolkit modules. Follow these patterns to create consistent, maintainable, and secure infrastructure modules.</p>"},{"location":"modules/CONTRIBUTING.html#gold-standards-overview","title":"\u2b50 Gold Standards Overview","text":"<p>Quick reference for what makes an excellent CGD Toolkit module:</p> <ul> <li>\ud83c\udfd7\ufe0f Structure: Simple parent modules with optional submodules only when justified</li> <li>\ud83c\udff7\ufe0f Variables: <code>existing_</code> prefix for external resources, purpose-based naming</li> <li>\ud83c\udfe0 Resources: Random IDs for predictable names, standardized logical names</li> <li>\ud83d\udd12 Security: No 0.0.0.0/0 ingress rules, HTTPS-first policy, user-controlled access</li> <li>\ud83c\udf10 DNS: Always create private zones, regional endpoint patterns</li> <li>\ud83e\uddea Testing: Terraform Test Framework with setup/ directory for CI parameters</li> <li>\ud83d\udcda Examples: Architecture decisions in examples, not module variables</li> </ul>"},{"location":"modules/CONTRIBUTING.html#module-structure-standards","title":"\ud83c\udfd7\ufe0f Module Structure Standards","text":""},{"location":"modules/CONTRIBUTING.html#preferred-structure","title":"Preferred Structure","text":"<pre><code>module-name/\n\u251c\u2500\u2500 README.md            # Parent module documentation\n\u251c\u2500\u2500 data.tf              # Data source definitions\n\u251c\u2500\u2500 locals.tf            # Local value calculations\n\u251c\u2500\u2500 main.tf              # Parent module orchestration\n\u251c\u2500\u2500 outputs.tf           # Standardized outputs\n\u251c\u2500\u2500 variables.tf         # Input variables with validation\n\u251c\u2500\u2500 versions.tf          # Terraform and provider version constraints\n\u251c\u2500\u2500 examples/\n\u2502   \u2514\u2500\u2500 complete/        # Shows parent module usage\n\u2502       \u251c\u2500\u2500 README.md    # Example documentation\n\u2502       \u251c\u2500\u2500 main.tf      # Example configuration using the module\n\u2502       \u251c\u2500\u2500 outputs.tf   # Example outputs\n\u2502       \u251c\u2500\u2500 variables.tf # Example input variables\n\u2502       \u2514\u2500\u2500 versions.tf  # Example version constraints\n\u251c\u2500\u2500 modules/ (OPTIONAL - only when dependency conflicts between providers)\n\u2502   \u251c\u2500\u2500 infra/           # AWS provider submodule (creates EKS cluster)\n\u2502   \u2502   \u251c\u2500\u2500 README.md    # Submodule documentation\n\u2502   \u2502   \u251c\u2500\u2500 main.tf      # AWS resource definitions\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf   # Submodule outputs\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf # Submodule input variables\n\u2502   \u2502   \u2514\u2500\u2500 versions.tf  # Submodule version constraints\n\u2502   \u2514\u2500\u2500 services/        # Kubernetes/Helm provider submodule (requires EKS cluster)\n\u2502       \u251c\u2500\u2500 README.md    # Submodule documentation\n\u2502       \u251c\u2500\u2500 main.tf      # Kubernetes/Helm resource definitions\n\u2502       \u251c\u2500\u2500 outputs.tf   # Submodule outputs\n\u2502       \u251c\u2500\u2500 variables.tf # Submodule input variables\n\u2502       \u2514\u2500\u2500 versions.tf  # Submodule version constraints\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 setup/           # Shared test setup (REQUIRED)\n    \u2502   \u251c\u2500\u2500 ssm.tf       # SSM parameter retrieval\n    \u2502   \u2514\u2500\u2500 versions.tf  # Test setup versions\n    \u251c\u2500\u2500 unit_01_basic_single_region.tftest.hcl          # Unit test (plan only)\n    \u251c\u2500\u2500 unit_02_basic_multi_region.tftest.hcl           # Unit test (plan only)\n    \u251c\u2500\u2500 integration_01_single_region_deploy.tftest.hcl  # Integration/E2E test (apply)\n    \u2514\u2500\u2500 integration_02_multi_region_deploy.tftest.hcl   # Integration/E2E test (apply)\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#when-to-use-submodules","title":"When to Use Submodules","text":"<p>\u2705 Use Submodules When:</p> <ul> <li>Different providers required (AWS vs Kubernetes vs Helm)</li> <li>Dependency conflicts between providers (EKS cluster must exist before Kubernetes resources)</li> <li>Clear logical separation benefits maintenance</li> <li>Reusable across multiple parent modules</li> </ul> <p>\u274c Avoid Submodules When:</p> <ul> <li>Same provider throughout</li> <li>Tightly coupled resources</li> <li>No clear separation benefit</li> <li>Adds complexity without value</li> </ul> <p>Example Excellence Pattern:</p> <pre><code># DDC Module - Different providers justify submodules\nmodule \"ddc_infra\" {\n  source = \"./modules/ddc-infra\"\n  providers = { aws = aws }\n}\n\nmodule \"ddc_services\" {\n  source = \"./modules/ddc-services\"\n  providers = {\n    kubernetes = kubernetes\n    helm = helm\n  }\n  depends_on = [module.ddc_infra]\n}\n</code></pre> <p>Default Excellence: Most modules should be implemented as single parent modules without submodules.</p>"},{"location":"modules/CONTRIBUTING.html#variable-naming-standards","title":"\ud83c\udff7\ufe0f Variable Naming Standards","text":""},{"location":"modules/CONTRIBUTING.html#excellence-pattern-existing_-prefix-for-external-resources","title":"Excellence Pattern: <code>existing_</code> Prefix for External Resources","text":"<pre><code># \u2705 EXCELLENT - Use existing_ prefix for user-provided resources\nvariable \"existing_vpc_id\" {\n  type        = string\n  description = \"VPC ID where resources will be created\"\n}\n\nvariable \"existing_load_balancer_subnets\" {\n  type        = list(string)\n  description = \"Subnets for load balancers (public for internet, private for VPC-only)\"\n}\n\nvariable \"existing_service_subnets\" {\n  type        = list(string)\n  description = \"Subnets for services (EKS, databases, applications)\"\n}\n\nvariable \"existing_security_groups\" {\n  type        = list(string)\n  description = \"Security group IDs for external access\"\n  default     = []\n}\n\n# \u274c AVOID - Topology-based naming\nvariable \"public_subnets\" { }  # Implies specific network topology\nvariable \"private_subnets\" { } # Less flexible than purpose-based naming\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#excellence-pattern-purpose-based-variable-names","title":"Excellence Pattern: Purpose-Based Variable Names","text":"<pre><code># \u2705 EXCELLENT - Purpose-based, configurable\nvariable \"project_prefix\" {\n  type        = string\n  description = \"Prefix for all resource names\"\n  default     = \"cgd\"\n}\n\nvariable \"debug_mode\" {\n  type        = string\n  description = \"Debug mode: 'enabled' relaxes security constraints for troubleshooting, 'disabled' enforces production security\"\n  default     = \"disabled\"\n\n  validation {\n    condition     = contains([\"enabled\", \"disabled\"], var.debug_mode)\n    error_message = \"debug_mode must be 'enabled' or 'disabled'\"\n  }\n}\n\n# \u274c AVOID - Opinionated variables\nvariable \"access_method\" { }  # Let examples show patterns instead\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#excellence-pattern-tiered-security-group-strategy","title":"Excellence Pattern: Tiered Security Group Strategy","text":"<pre><code># General access (applies to all public-facing components)\nvariable \"existing_security_groups\" {\n  type        = list(string)\n  description = \"Security group IDs for general access to public services\"\n  default     = []\n}\n\n# Component-specific access (when needed)\nvariable \"existing_load_balancer_security_groups\" {\n  type        = list(string)\n  description = \"Additional security group IDs for load balancer access\"\n  default     = []\n}\n\nvariable \"existing_eks_security_groups\" {\n  type        = list(string)\n  description = \"Additional security group IDs for EKS API access\"\n  default     = []\n}\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#resource-naming-standards","title":"\ud83c\udfe0 Resource Naming Standards","text":""},{"location":"modules/CONTRIBUTING.html#excellence-pattern-random-ids-for-predictable-names","title":"Excellence Pattern: Random IDs for Predictable Names","text":"<pre><code># \u2705 EXCELLENT - Predictable, conflict-free naming\nresource \"random_id\" \"suffix\" {\n  byte_length = 4\n  keepers = {\n    project_prefix = var.project_prefix\n    name          = local.name\n  }\n}\n\nlocals {\n  name_prefix = \"${var.project_prefix}-${local.name}\"\n  name_suffix = random_id.suffix.hex\n\n  # Predictable, readable names\n  nlb_name = \"${local.name_prefix}-nlb-${local.name_suffix}\"\n  # Result: \"cgd-unreal-cloud-ddc-nlb-a1b2c3d4\"\n}\n\n# \u274c AVOID - Unpredictable name_prefix\nresource \"aws_lb\" \"nlb\" {\n  name_prefix = \"cgd-ddc-\"  # AWS generates random suffix\n}\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#excellence-pattern-standardized-logical-names","title":"Excellence Pattern: Standardized Logical Names","text":"<pre><code># \u2705 EXCELLENT - Use these EXACT logical names in every module\nresource \"aws_lb\" \"nlb\" { }                    # Network Load Balancer\nresource \"aws_lb\" \"alb\" { }                    # Application Load Balancer\nresource \"aws_eks_cluster\" \"main\" { }          # Primary EKS cluster\nresource \"aws_security_group\" \"nlb\" { }        # NLB security group\nresource \"aws_security_group\" \"eks_cluster\" { } # EKS cluster security group\nresource \"aws_security_group\" \"internal\" { }   # Internal service communication\nresource \"aws_s3_bucket\" \"artifacts\" { }       # Artifacts storage\nresource \"aws_s3_bucket\" \"logs\" { }            # Logs storage\nresource \"aws_route53_zone\" \"private\" { }      # Private DNS zone\n\n# \u274c AVOID - Inconsistent naming\nresource \"aws_lb\" \"this\" { }           # Too generic\nresource \"aws_lb\" \"ddc_nlb\" { }        # Module-specific\nresource \"aws_lb\" \"load_balancer\" { }  # Verbose\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#excellence-pattern-provider-configurations","title":"Excellence Pattern: Provider Configurations","text":"<p>CRITICAL: Modules should NOT define provider configurations - only version requirements.</p> <p>Required Versions (Universal):</p> <ul> <li>Terraform &gt;= 1.11 - Required for enhanced testing framework and stability improvements</li> <li>AWS Provider &gt;= 6.0.0 - MANDATORY for enhanced region support in multi-region deployments</li> </ul> <p>Additional Providers (When Needed):</p> <ul> <li>Kubernetes Provider &gt;= 2.33.0 - For modules using Kubernetes resources</li> <li>Helm Provider &gt;= 3.0.0 - For modules deploying Helm charts</li> </ul> <pre><code># \u2705 EXCELLENT - Universal requirements\nterraform {\n  required_version = \"&gt;= 1.11\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"&gt;= 6.0.0\"  # CRITICAL for enhanced region support\n    }\n    # Add additional providers only when needed:\n    # kubernetes = { source = \"hashicorp/kubernetes\", version = \"&gt;= 2.33.0\" }\n    # helm = { source = \"hashicorp/helm\", version = \"&gt;= 3.0.0\" }\n  }\n}\n</code></pre> <p>Why AWS Provider v6.0.0+: Enhanced region support allows automatic region inheritance for multi-region deployments, eliminating the need to explicitly pass AWS providers to submodules.</p>"},{"location":"modules/CONTRIBUTING.html#security-standards","title":"\ud83d\udd12 Security Standards","text":""},{"location":"modules/CONTRIBUTING.html#excellence-pattern-user-controlled-access","title":"Excellence Pattern: User-Controlled Access","text":"<pre><code># \u2705 EXCELLENT - Users control external access, we create internal communication\nvariable \"existing_security_groups\" {\n  type        = list(string)\n  description = \"Security group IDs for external access\"\n  default     = []\n}\n\n# \u2705 EXCELLENT - Internal communication security group\nresource \"aws_security_group\" \"internal\" {\n  name        = \"${local.name_prefix}-internal\"\n  description = \"Internal service communication\"\n  vpc_id      = var.existing_vpc_id\n\n  # Internal service rules only\n  ingress {\n    from_port = 9042\n    to_port   = 9042\n    protocol  = \"tcp\"\n    self      = true  # Only from same security group\n  }\n\n  # \u2705 ACCEPTABLE - Outbound internet access for AWS APIs\n  egress {\n    description = \"All outbound traffic (AWS APIs, updates, container registry)\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]  # Required for AWS API calls\n  }\n}\n\n# \u2705 EXCELLENT - Apply security groups with concat\nresource \"aws_eks_node_group\" \"main\" {\n  security_groups = concat(\n    var.existing_security_groups,              # User-controlled\n    var.existing_load_balancer_security_groups, # Component-specific\n    [aws_security_group.internal.id]          # Internal communication\n  )\n}\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#excellence-pattern-security-validation","title":"Excellence Pattern: Security Validation","text":"<pre><code># \u2705 EXCELLENT - Prevent insecure configurations\nvariable \"allowed_external_cidrs\" {\n  type        = list(string)\n  description = \"CIDR blocks for external access\"\n  default     = []\n\n  validation {\n    condition     = !contains(var.allowed_external_cidrs, \"0.0.0.0/0\")\n    error_message = \"0.0.0.0/0 not allowed for ingress. Specify actual CIDR blocks.\"\n  }\n}\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#excellence-pattern-https-first-policy","title":"Excellence Pattern: HTTPS-First Policy","text":"<pre><code># \u2705 EXCELLENT - Certificate validation for internet-facing services\nvariable \"existing_certificate_arn\" {\n  type        = string\n  description = \"ACM certificate ARN for HTTPS listeners (required for internet-facing)\"\n  default     = null\n\n  validation {\n    condition = var.internet_facing == false || var.existing_certificate_arn != null || var.debug_mode == \"enabled\"\n    error_message = \"Certificate ARN required for internet-facing services unless debug_mode enabled\"\n  }\n}\n\n# \u2705 EXCELLENT - HTTPS listener (production)\nresource \"aws_lb_listener\" \"https\" {\n  count             = var.existing_certificate_arn != null ? 1 : 0\n  load_balancer_arn = aws_lb.nlb.arn\n  port              = \"443\"\n  protocol          = \"TLS\"\n  certificate_arn   = var.existing_certificate_arn\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.main.arn\n  }\n}\n\n# \u2705 EXCELLENT - HTTP redirect to HTTPS or debug mode\nresource \"aws_lb_listener\" \"http\" {\n  load_balancer_arn = aws_lb.nlb.arn\n  port              = \"80\"\n  protocol          = \"TCP\"\n\n  default_action {\n    type = var.debug_mode == \"enabled\" ? \"forward\" : (\n      var.existing_certificate_arn != null ? \"redirect\" : \"forward\"\n    )\n\n    dynamic \"redirect\" {\n      for_each = var.debug_mode == \"disabled\" &amp;&amp; var.existing_certificate_arn != null ? [1] : []\n      content {\n        port        = \"443\"\n        protocol    = \"HTTPS\"\n        status_code = \"HTTP_301\"\n      }\n    }\n\n    target_group_arn = var.debug_mode == \"enabled\" || var.existing_certificate_arn == null ?\n      aws_lb_target_group.main.arn : null\n  }\n}\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#dns-standards","title":"\ud83c\udf10 DNS Standards","text":""},{"location":"modules/CONTRIBUTING.html#excellence-pattern-always-create-private-zones","title":"Excellence Pattern: Always Create Private Zones","text":"<p>Reasoning: Private hosted zones provide essential internal service discovery, enable future multi-region replication, and support operational troubleshooting without additional cost. They're created automatically to ensure consistent DNS patterns across all modules.</p> <p>Note: As with all standards, we evaluate exceptions on a case-by-case basis. If your specific use case doesn't benefit from private DNS, discuss with the team.</p> <pre><code># \u2705 EXCELLENT - Always create private zone\nresource \"aws_route53_zone\" \"private\" {\n  name = var.existing_route53_public_hosted_zone_name != null ?\n    \"${var.project_prefix}.${var.existing_route53_public_hosted_zone_name}\" :\n    \"${var.project_prefix}.internal\"\n\n  vpc {\n    vpc_id = var.existing_vpc_id\n  }\n}\n\n# \u2705 EXCELLENT - Always create service records for internal discovery\nresource \"aws_route53_record\" \"scylla\" {\n  zone_id = aws_route53_zone.private.zone_id\n  name    = \"scylla\"\n  type    = \"A\"\n  ttl     = 300\n  records = aws_instance.scylla[*].private_ip\n}\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#excellence-pattern-regional-endpoint-pattern","title":"Excellence Pattern: Regional Endpoint Pattern","text":"<pre><code># \u2705 EXCELLENT - Multi-region DNS standard\nlocals {\n  # Regional endpoint: {region}.{service}.{domain}\n  public_dns_name = var.existing_route53_public_hosted_zone_name != null ?\n    \"${var.region}.${local.service_name}.${var.existing_route53_public_hosted_zone_name}\" :\n    null\n\n  service_name = \"ddc\"  # or \"perforce\", \"jenkins\", etc.\n}\n\n# Examples:\n# us-east-1.ddc.company.com\n# us-west-2.ddc.company.com\n# eu-west-1.perforce.company.com\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#testing-standards","title":"\ud83e\uddea Testing Standards","text":""},{"location":"modules/CONTRIBUTING.html#excellence-pattern-terraform-test-framework","title":"Excellence Pattern: Terraform Test Framework","text":"<p>All modules must implement comprehensive testing using the Terraform Test Framework:</p> <pre><code># tests/unit_01_basic_single_region.tftest.hcl\nrun \"setup\" {\n  command = plan\n  module {\n    source = \"./tests/setup\"\n  }\n}\n\nrun \"unit_test\" {\n  command = plan\n\n  variables {\n    existing_vpc_id = \"vpc-12345678\"\n    existing_load_balancer_subnets = [\"subnet-12345678\"]\n    existing_service_subnets = [\"subnet-87654321\"]\n  }\n\n  module {\n    source = \"./examples/complete\"\n  }\n}\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#excellence-pattern-test-setup-directory","title":"Excellence Pattern: Test Setup Directory","text":"<pre><code># tests/setup/ssm.tf - REQUIRED for all modules\ndata \"aws_ssm_parameter\" \"route53_public_hosted_zone_name\" {\n  name = \"/cgd-toolkit/tests/route53-public-hosted-zone-name\"\n}\n\ndata \"aws_ssm_parameter\" \"ghcr_credentials_secret_arn\" {\n  name = \"/cgd-toolkit/tests/ghcr-credentials-secret-arn\"\n}\n\noutput \"route53_public_hosted_zone_name\" {\n  value = data.aws_ssm_parameter.route53_public_hosted_zone_name.value\n}\n\noutput \"ghcr_credentials_secret_arn\" {\n  value = data.aws_ssm_parameter.ghcr_credentials_secret_arn.value\n}\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#excellence-checklists","title":"\u2705 Excellence Checklists","text":""},{"location":"modules/CONTRIBUTING.html#for-new-modules","title":"For New Modules","text":"<ul> <li>[ ] Use <code>existing_</code> prefix for external resources</li> <li>[ ] Implement standardized logical names (nlb, alb, main, etc.)</li> <li>[ ] Create private DNS zones automatically</li> <li>[ ] Use random IDs for resource naming</li> <li>[ ] Implement HTTPS-first policy with debug exception</li> <li>[ ] Use tiered security group strategy</li> <li>[ ] Set conservative defaults</li> <li>[ ] Create comprehensive examples</li> <li>[ ] Add Terraform tests with setup/ directory</li> <li>[ ] Document architecture patterns</li> <li>[ ] Use examples for architecture decisions, not module variables</li> </ul>"},{"location":"modules/CONTRIBUTING.html#for-existing-modules-enhancement","title":"For Existing Modules Enhancement","text":"<ul> <li>[ ] Plan breaking changes for v2.0.0</li> <li>[ ] Add <code>moved</code> blocks for renamed resources</li> <li>[ ] Update variable naming conventions</li> <li>[ ] Implement DNS standards</li> <li>[ ] Add HTTPS enforcement</li> <li>[ ] Update security group patterns</li> <li>[ ] Create migration documentation</li> <li>[ ] Test upgrade paths with real state files</li> </ul>"},{"location":"modules/CONTRIBUTING.html#security-excellence","title":"Security Excellence","text":"<ul> <li>[ ] No 0.0.0.0/0 ingress rules in module code</li> <li>[ ] Outbound 0.0.0.0/0 egress rules only for AWS APIs</li> <li>[ ] Use dedicated security group rule resources</li> <li>[ ] Implement certificate validation for internet-facing services</li> <li>[ ] Use user-provided security groups for external access</li> <li>[ ] Create internal security groups for service communication</li> </ul>"},{"location":"modules/CONTRIBUTING.html#testing-excellence","title":"Testing Excellence","text":"<ul> <li>[ ] All modules have Terraform Test Framework tests</li> <li>[ ] Tests use setup/ directory for SSM parameter retrieval</li> <li>[ ] Both unit tests (plan) and integration tests (apply)</li> <li>[ ] All tests pass before PR approval</li> <li>[ ] Tests reference examples, not module directly</li> </ul>"},{"location":"modules/CONTRIBUTING.html#architecture-guidance","title":"\ud83d\udcda Architecture Guidance","text":"<p>Note: These are our recommended patterns and preferences, not hard requirements for every module. Networking configurations should be defined at the example level to maintain module flexibility.</p>"},{"location":"modules/CONTRIBUTING.html#recommended-compute-strategy","title":"Recommended Compute Strategy","text":"<ol> <li>Serverless (Lambda, Fargate) - Preferred for simplicity and cost</li> <li>Managed Containers (ECS Fargate, EKS Fargate) - For scalable services</li> <li>Container Orchestration (ECS EC2, EKS EC2) - When Fargate limitations apply</li> <li>Dedicated EC2 - Only when technology requirements mandate it</li> </ol>"},{"location":"modules/CONTRIBUTING.html#recommended-networking-patterns","title":"Recommended Networking Patterns","text":"<p>Note: Networking decisions should be made at the example level, not enforced by modules. These are recommended patterns that examples can demonstrate.</p> <p>External Access Pattern (Example-Driven):</p> <pre><code>Internet Users \u2192 Public NLB \u2192 NLB Target (EKS, EC2, etc.)\n</code></pre> <p>External Access with ALB (When HTTP Routing Needed):</p> <pre><code>Internet Users \u2192 Public NLB \u2192 ALB \u2192 Service Targets (EKS Pods, EC2, etc.)\n</code></pre> <p>Internal Access Pattern (Example-Driven):</p> <pre><code>VPN/VDI Users \u2192 Private NLB \u2192 NLB Target (ALB, EKS, EC2, etc.)\n</code></pre> <p>VPC Endpoints: Currently not supported but under consideration. If you're interested in VPC Endpoints support, please submit a feature request to the toolkit.</p>"},{"location":"modules/CONTRIBUTING.html#excellence-pattern-multi-region-architecture","title":"Excellence Pattern: Multi-Region Architecture","text":"<p>Regional Isolation Pattern:</p> <ul> <li>Separate module instances per region</li> <li>Regional endpoints for user control</li> <li>Manual disaster recovery (users switch endpoints)</li> <li>Cross-region connectivity via VPC peering or Transit Gateway</li> </ul> <p>AWS Provider v6 Enhanced Region Support: We leverage Terraform's enhanced AWS provider region support for multi-region deployments. AWS Provider v6+ automatically handles region inheritance, reducing provider configuration complexity.</p> <pre><code># \u2705 EXCELLENT - Enhanced region support (AWS Provider v6+)\nmodule \"ddc_primary\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/unreal/unreal-cloud-ddc?ref=v1.2.0\"\n\n  # Enhanced region support - AWS provider auto-inherited\n  providers = {\n    kubernetes = kubernetes.primary\n    helm       = helm.primary\n    # AWS provider automatically inherited based on region\n  }\n\n  region = var.primary_region\n  existing_vpc_id = aws_vpc.primary.id\n\n  ddc_infra_config = {\n    region = var.primary_region\n    create_seed_node = true\n    # Primary region configuration...\n  }\n}\n\n# Secondary region\nmodule \"ddc_secondary\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/unreal/unreal-cloud-ddc?ref=v1.2.0\"\n\n  providers = {\n    kubernetes = kubernetes.secondary\n    helm       = helm.secondary\n    # AWS provider automatically inherited based on region\n  }\n\n  region = var.secondary_region\n  existing_vpc_id = aws_vpc.secondary.id\n\n  ddc_infra_config = {\n    region = var.secondary_region\n    create_seed_node = false\n    existing_scylla_seed = module.ddc_primary.ddc_infra.scylla_seed\n    # Secondary region configuration...\n  }\n\n  ddc_services_config = {\n    ddc_replication_region_url = module.ddc_primary.ddc_connection.endpoint_nlb\n    # Replication configuration...\n  }\n\n  depends_on = [module.ddc_primary]\n}\n</code></pre> <p>Benefits: AWS Provider v6 automatically handles region inheritance, reducing provider configuration complexity. Other providers (Kubernetes, Helm) still require explicit provider configuration until they adopt enhanced region support.</p>"},{"location":"modules/CONTRIBUTING.html#breaking-changes-prevention","title":"Breaking Changes Prevention","text":"<p>Excellence Rules:</p> <ul> <li>\u2705 ALWAYS use major version bumps for breaking changes</li> <li>\u2705 ALWAYS test migration paths with real state files</li> <li>\u2705 ALWAYS document breaking changes comprehensively</li> <li>\u274c NEVER change logical names without <code>moved</code> blocks</li> <li>\u274c NEVER change variable names in minor/patch versions</li> </ul> <p>Safe Enhancement Patterns:</p> <pre><code># \u2705 SAFE - Adding new resources\nresource \"aws_s3_bucket\" \"new_bucket\" {\n  bucket = \"${var.name_prefix}-new-bucket\"\n}\n\n# \u2705 SAFE - Adding optional variables with defaults\nvariable \"new_feature_enabled\" {\n  type        = bool\n  description = \"Enable new feature\"\n  default     = false  # REQUIRED default\n}\n\n# \u2705 SAFE - Adding new outputs\noutput \"new_resource_id\" {\n  value = aws_s3_bucket.new_bucket.id\n}\n</code></pre>"},{"location":"modules/CONTRIBUTING.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Review Gold Standards - Understand what excellence looks like</li> <li>Choose Module Structure - Simple parent module or justified submodules</li> <li>Implement Naming Standards - Variables, resources, and logical names</li> <li>Add Security Patterns - HTTPS-first, user-controlled access</li> <li>Create DNS Infrastructure - Private zones and regional endpoints</li> <li>Build Comprehensive Tests - Unit and integration tests with setup/</li> <li>Document with Examples - Show architecture decisions in examples</li> <li>Validate Excellence - Use checklists to ensure compliance</li> </ol> <p>For questions or clarifications on these standards, please engage with the team through GitHub discussions or issues.</p>"},{"location":"modules/DESIGN_STANDARDS.html","title":"CGD Toolkit Module Design Standards","text":""},{"location":"modules/DESIGN_STANDARDS.html#overview","title":"Overview","text":"<p>This document captures the design decisions and patterns we've collectively agreed upon for CGD Toolkit modules. These standards emerged from real-world usage, community feedback, and lessons learned from building production game development infrastructure.</p> <p>Why These Standards Matter: As CGD Toolkit grows, consistency becomes critical for maintainability, user experience, and contributor onboarding. These patterns represent our best practices for building reliable, secure, and user-friendly Terraform modules.</p> <p>Living Document: These standards evolve based on community needs and new AWS capabilities. When proposing changes, consider backward compatibility and migration paths for existing users.</p> <p>Module Evolution: You might find some modules that don't follow these patterns yet - they're likely on our refactoring roadmap. If you spot a recently updated module that diverges from these standards, let us know! We're always improving and appreciate the feedback.</p>"},{"location":"modules/DESIGN_STANDARDS.html#core-design-philosophy","title":"Core Design Philosophy","text":""},{"location":"modules/DESIGN_STANDARDS.html#1-readability-first","title":"1. Readability First","text":"<p>Why: Game development teams often include infrastructure newcomers. Clear, understandable code reduces onboarding time and prevents misconfigurations.</p> <ul> <li>Prefer explicit over implicit configurations</li> <li>Use descriptive variable names that explain purpose</li> <li>Self-documenting code over clever abstractions</li> <li>Comment complex logic with business context</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#2-flexibility-through-modularity","title":"2. Flexibility Through Modularity","text":"<p>Why: Game studios have diverse infrastructure needs. Rigid, opinionated modules force workarounds and reduce adoption.</p> <ul> <li>Modules provide building blocks, not complete solutions</li> <li>Configuration decisions happen in examples, not module internals</li> <li>Support multiple deployment patterns through simple variables</li> <li>Enable customization without requiring module forking</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#3-conservative-variable-exposure","title":"3. Conservative Variable Exposure","text":"<p>Why: Every exposed variable is a commitment to backward compatibility. We learned this from early modules that exposed too many options.</p> <ul> <li>Start with minimal variables based on known use cases</li> <li>Add variables when users request them (demand-driven)</li> <li>Easier to add than remove (breaking changes are painful)</li> <li>Default values should work for 80% of use cases</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#4-security-by-default","title":"4. Security by Default","text":"<p>Why: Game development infrastructure often handles sensitive assets and player data. Security mistakes are costly and hard to fix later.</p> <ul> <li>No 0.0.0.0/0 ingress rules in module code (unless you have a really good reason - we will ask) - more details</li> <li>Users explicitly define allowed access (security groups, CIDRs)</li> <li>Private-first architecture with controlled external access</li> <li>HTTPS enforcement for internet-facing services</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#module-architecture","title":"Module Architecture","text":""},{"location":"modules/DESIGN_STANDARDS.html#directory-structure","title":"Directory Structure","text":"<pre><code>modules/service-name/\n\u251c\u2500\u2500 main.tf              # Parent module orchestration\n\u251c\u2500\u2500 variables.tf         # Input variables with validation\n\u251c\u2500\u2500 outputs.tf           # Module outputs\n\u251c\u2500\u2500 versions.tf          # Terraform and provider version constraints\n\u251c\u2500\u2500 README.md            # Module documentation\n\u251c\u2500\u2500 modules/             # Submodules (when needed)\n\u2502   \u251c\u2500\u2500 infra/          # AWS resources only\n\u2502   \u2514\u2500\u2500 services/       # Kubernetes/Helm only\n\u251c\u2500\u2500 tests/              # Terraform tests\n\u2502   \u251c\u2500\u2500 setup/          # CI parameter retrieval\n\u2502   \u2514\u2500\u2500 *.tftest.hcl    # Test files\n\u2514\u2500\u2500 examples/           # Working examples\n    \u2514\u2500\u2500 */              # Example configurations\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#parent-module-pattern","title":"Parent Module Pattern","text":"<p>Why: When modules have submodules, the parent focuses on user experience while submodules handle implementation details.</p> <p>Responsibilities:</p> <ul> <li>Create some resources directly (DNS zones, security groups, etc.)</li> <li>Provide clean, user-friendly variable interface</li> <li>Validate inputs with helpful error messages</li> <li>Orchestrate submodules with proper dependencies (when present)</li> <li>Expose essential outputs for downstream usage</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#when-to-use-submodules","title":"When to Use Submodules","text":"<p>Why Split: Provider separation or complexity management.</p> <pre><code># \u2705 GOOD - Different providers\nmodule \"infra\" {\n  source = \"./modules/infra\"\n  providers = { aws = aws }\n}\n\nmodule \"services\" {\n  source = \"./modules/services\"\n  providers = { kubernetes = kubernetes, helm = helm }\n  depends_on = [module.infra]\n}\n\n# \u274c AVOID - Same provider, no clear benefit\nmodule \"s3_bucket\" {\n  source = \"./modules/s3\"\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#submodule-variable-alignment-pattern","title":"Submodule Variable Alignment Pattern","text":"<p>When using submodules, align parent variables directly to submodules for clarity:</p> <pre><code># \u2705 GOOD - Clear submodule alignment\nvariable \"infra_config\" {\n  type = object({\n    # All infrastructure settings grouped together\n    kubernetes_version = optional(string, \"1.33\")\n    database_config = object({...})\n    networking_config = object({...})\n  })\n}\n\nvariable \"services_config\" {\n  type = object({\n    # All service settings grouped together\n    app_version = optional(string, \"latest\")\n    credentials_arn = string\n  })\n}\n\n# Parent module orchestration\nmodule \"infra\" {\n  source = \"./modules/infra\"\n  config = var.infra_config  # Direct alignment\n}\n\nmodule \"services\" {\n  source = \"./modules/services\"\n  config = var.services_config  # Direct alignment\n}\n\n# \u274c AVOID - Scattered variables requiring manual mapping\nvariable \"kubernetes_version\" { }\nvariable \"database_instance_type\" { }\nvariable \"app_version\" { }\nvariable \"credentials_arn\" { }\n\nmodule \"infra\" {\n  kubernetes_version = var.kubernetes_version\n  database_instance_type = var.database_instance_type\n  # Manual mapping of many variables\n}\n</code></pre> <p>Benefits of Submodule Alignment:</p> <ul> <li>Clear responsibility - Users understand which settings affect which components</li> <li>Easy orchestration - Parent module passes entire objects to submodules</li> <li>Conditional creation - <code>config = null</code> skips entire submodules</li> <li>Reduced complexity - No manual variable mapping in parent module</li> <li>Logical grouping - Related settings stay together</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#networking-standards","title":"Networking Standards","text":""},{"location":"modules/DESIGN_STANDARDS.html#access-patterns","title":"Access Patterns","text":"<p>CGD Toolkit modules support three standardized access patterns:</p> <ul> <li>Internet-Accessible - Public services (DDC, Perforce, Jenkins) with controlled external access</li> <li>VPC-Only - Internal services (Databases, Monitoring) accessible only within VPC</li> <li>Mixed - Services with both public and private components</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#load-balancer-strategy","title":"Load Balancer Strategy","text":"<p>Consistent approach across all modules:</p> <ul> <li>Default to NLB for most services (connection-level health checks, static IPs, predictable routing)</li> <li>ALB when needed for HTTP/HTTPS routing, WAF integration, path-based routing</li> <li>User controls creation via boolean flags or configuration objects</li> <li>Automatic target group management - modules handle the complexity</li> <li>Cost justified - ~$16/month NLB vs Route53 health check complexity</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#dns-patterns","title":"DNS Patterns","text":"<p>Regional endpoints by default following AWS service patterns:</p> <ul> <li>Regional endpoints - <code>us-east-1.service.company.com</code> (performance, isolation, explicit control)</li> <li>Private zones - Always created for internal service discovery (<code>service.internal</code>)</li> <li>Global endpoints - Optional for advanced routing (latency-based, geolocation, failover)</li> <li>DNS hierarchy - <code>region.cluster.platform.service.domain</code> for complex services</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#variable-structure-philosophy","title":"Variable Structure Philosophy","text":"<p>Hybrid approach following popular module patterns:</p> <ul> <li>Flat variables for simple, common settings (following terraform-aws-modules pattern)</li> <li>Complex objects for logical grouping when they provide clear value (following AWS-IA pattern)</li> <li>Submodule alignment - Complex objects that map directly to submodules (<code>infra_config</code>, <code>services_config</code>)</li> <li>Component objects acceptable - <code>load_balancer_config</code>, <code>security_groups</code> for logical grouping</li> <li>Conditional creation - <code>config = null</code> skips entire components</li> <li>Intelligent defaults - Work for 80% of use cases, reduce cognitive load</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#security-group-integration","title":"Security Group Integration","text":"<p>Follow Terraform resource patterns for familiarity:</p> <ul> <li>User-controlled external access - Users provide security groups with their own rules</li> <li>Module-created internal groups - For service-to-service communication</li> <li>Component-specific grouping - General + NLB-specific + ALB-specific</li> <li>CIDR validation - No 0.0.0.0/0 ingress rules in module code</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#variable-design-patterns","title":"Variable Design Patterns","text":""},{"location":"modules/DESIGN_STANDARDS.html#general-naming-conventions","title":"General Naming Conventions","text":"<p>Why Descriptive Names: We avoid generic names like <code>this</code> because they don't scale and become confusing when you need multiple resources.</p> <pre><code># \u2705 GOOD - Descriptive, purpose-driven names\nresource \"aws_lb\" \"nlb\" { }                    # Network Load Balancer\nresource \"aws_lb\" \"alb\" { }                    # Application Load Balancer\nresource \"aws_eks_cluster\" \"main\" { }          # Primary EKS cluster\nresource \"aws_security_group\" \"nlb\" { }        # NLB security group\nresource \"aws_security_group\" \"internal\" { }   # Internal communication\nresource \"aws_s3_bucket\" \"artifacts\" { }       # Artifacts storage\nresource \"aws_s3_bucket\" \"logs\" { }            # Logs storage\nresource \"aws_route53_zone\" \"private\" { }      # Private DNS zone\n\n# \u274c BAD - Generic names that don't scale\nresource \"aws_lb\" \"this\" { }           # What kind of load balancer?\nresource \"aws_lb\" \"this2\" { }          # Now you need a second one...\nresource \"aws_s3_bucket\" \"bucket\" { }  # What's it for?\nresource \"aws_s3_bucket\" \"main\" { }    # Still not descriptive\n</code></pre> <p>The Problem with Generic Names:</p> <ul> <li>Not descriptive: <code>this</code> tells you nothing about purpose</li> <li>Doesn't scale: Need a second resource? Now it's <code>this2</code> or you rename everything</li> <li>Hard to reference: <code>aws_lb.this.dns_name</code> - which load balancer?</li> <li>Confusing in outputs: <code>nlb_dns_name = aws_lb.this.dns_name</code> - misleading</li> </ul> <p>Our Standard Logical Names:</p> <ul> <li><code>nlb</code>: Network Load Balancer</li> <li><code>alb</code>: Application Load Balancer</li> <li><code>main</code>: Primary resource of its type (EKS cluster, VPC)</li> <li><code>internal</code>: Internal communication security group</li> <li><code>artifacts</code>: Artifact storage bucket</li> <li><code>logs</code>: Logging storage bucket</li> <li><code>private</code>: Private DNS zone</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#3-tier-architecture","title":"3-Tier Architecture","text":"<p>Why: Users consistently need to separate applications, supporting services, and load balancers.</p> <pre><code>variable \"application_subnets\" {\n  type        = list(string)\n  description = \"Subnets for primary business applications\"\n}\n\nvariable \"service_subnets\" {\n  type        = list(string)\n  description = \"Subnets for supporting services (databases, caches)\"\n  default     = []  # Uses application_subnets if not specified\n}\n\nvariable \"load_balancer_config\" {\n  type = object({\n    nlb = object({\n      enabled         = optional(bool, true)\n      internet_facing = optional(bool, true)\n      subnets        = list(string)\n    })\n    alb = optional(object({\n      enabled         = optional(bool, false)\n      internet_facing = optional(bool, true)\n      subnets        = list(string)\n      enable_waf     = optional(bool, false)\n    }), null)\n  })\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#security-group-strategy","title":"Security Group Strategy","text":"<pre><code>variable \"security_groups\" {\n  type        = list(string)\n  description = \"Security group IDs for external access\"\n  default     = []\n}\n\nvariable \"additional_security_groups\" {\n  type = object({\n    load_balancer = optional(list(string), [])\n    eks_cluster   = optional(list(string), [])\n  })\n  description = \"Component-specific security groups\"\n  default     = {}\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#resource-patterns","title":"Resource Patterns","text":""},{"location":"modules/DESIGN_STANDARDS.html#remote-module-usage-philosophy","title":"Remote Module Usage Philosophy","text":"<p>CGD Toolkit modules prefer direct resources over remote module dependencies.</p>"},{"location":"modules/DESIGN_STANDARDS.html#default-approach-direct-resources","title":"Default Approach: Direct Resources","text":"<p>Start with direct AWS resources unless there's a compelling reason for a module:</p> <pre><code># \u2705 PREFERRED - Direct resource creation\nresource \"aws_eks_cluster\" \"main\" {\n  name     = local.cluster_name\n  role_arn = aws_iam_role.cluster.arn\n  version  = var.kubernetes_version\n\n  vpc_config {\n    subnet_ids              = var.existing_service_subnets\n    endpoint_private_access = var.eks_cluster_private_access\n    endpoint_public_access  = var.eks_cluster_public_access\n    public_access_cidrs     = var.eks_cluster_public_access_cidrs\n  }\n\n  # Direct configuration gives us full control\n}\n\n# \u274c AVOID - Remote module dependency\nmodule \"eks\" {\n  source = \"registry.terraform.io/example/eks/aws\"\n  # Adds complexity, version dependencies, limited customization\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#when-remote-modules-add-complexity","title":"When Remote Modules Add Complexity","text":"<p>Common issues we've encountered with remote modules:</p> <ul> <li>Version conflicts: Remote modules pin provider versions that conflict with our requirements</li> <li>Limited customization: Remote modules don't expose the exact configuration we need</li> <li>Bug dependencies: Waiting for upstream fixes when we could implement directly</li> <li>Breaking changes: Remote module updates can break our implementations</li> <li>Debugging difficulty: Issues span multiple codebases and maintainers</li> <li>Documentation gaps: Remote module docs may not cover our specific use cases</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#acceptable-remote-module-usage","title":"Acceptable Remote Module Usage","text":"<p>Use remote modules only when there's clear benefit:</p> <pre><code># \u2705 ACCEPTABLE - Well-established, stable modules with clear benefits\nmodule \"eks_addons\" {\n  source = \"registry.terraform.io/example/eks-addons/aws\"\n  version = \"~&gt; 2.0\"\n\n  # Only when:\n  # 1. Module is extremely stable and well-maintained\n  # 2. Provides significant complexity reduction\n  # 3. Benefits clearly outweigh the added complexity\n  # 4. Direct implementation would be overly complex\n}\n</code></pre> <p>Criteria for acceptable remote module usage:</p> <ul> <li>Stability: Module has long track record of stability</li> <li>Maintenance: Active maintenance and responsive maintainers</li> <li>Customization: Exposes all configuration we need</li> <li>Complexity reduction: Significantly reduces code complexity</li> <li>Clear benefit: Pros outweigh cons in terms of complexity it helps resolve</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#fork-first-strategy","title":"Fork-First Strategy","text":"<p>When you need a remote module, fork it first:</p> <pre><code># \u2705 RECOMMENDED - Fork and customize\nmodule \"custom_component\" {\n  source = \"./modules/forked-module\"  # Local fork\n\n  # Benefits:\n  # - Full control over changes\n  # - No waiting for upstream fixes\n  # - Can customize for our specific needs\n  # - No external version dependencies\n}\n\n# \u274c AVOID - Direct remote dependency\nmodule \"component\" {\n  source = \"github.com/example/external-module\"\n  # Creates external dependency and limits our control\n}\n</code></pre> <p>Fork-first benefits:</p> <ul> <li>Immediate fixes: Fix bugs without waiting for upstream</li> <li>Custom features: Add CGD Toolkit-specific functionality</li> <li>Version control: No external version dependency conflicts</li> <li>Stability: Changes only when we decide to update</li> <li>Documentation: We can document our specific usage patterns</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#implementation-guidelines","title":"Implementation Guidelines","text":""},{"location":"modules/DESIGN_STANDARDS.html#for-new-modules","title":"For New Modules","text":"<ol> <li>Start with direct resources: Always begin with AWS resources directly</li> <li>Evaluate complexity: Only consider modules if direct implementation is extremely complex</li> <li>Fork if needed: If you must use a remote module, fork it first</li> <li>Document decision: Explain why direct resources weren't sufficient</li> </ol>"},{"location":"modules/DESIGN_STANDARDS.html#for-existing-modules","title":"For Existing Modules","text":"<ol> <li>Audit dependencies: Review existing remote module usage</li> <li>Plan replacement: Create plan to replace with direct resources</li> <li>Gradual migration: Replace remote modules incrementally</li> <li>Test thoroughly: Ensure functionality remains identical</li> </ol>"},{"location":"modules/DESIGN_STANDARDS.html#code-review-checklist","title":"Code Review Checklist","text":"<p>When reviewing PRs that add remote modules:</p> <ul> <li>[ ] Justification provided: Clear explanation why direct resources aren't sufficient</li> <li>[ ] Alternatives explored: Evidence that direct implementation was considered</li> <li>[ ] Fork strategy: If remote module needed, is it forked locally?</li> <li>[ ] Stability assessment: Is the remote module well-maintained and stable?</li> <li>[ ] Customization needs: Does the module expose all needed configuration?</li> <li>[ ] Version pinning: Are versions properly pinned to avoid surprises?</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#examples-of-our-approach","title":"Examples of Our Approach","text":""},{"location":"modules/DESIGN_STANDARDS.html#eks-cluster-creation","title":"EKS Cluster Creation","text":"<pre><code># We use direct resources instead of remote EKS modules\nresource \"aws_eks_cluster\" \"main\" {\n  # Direct control over all EKS configuration\n}\n\nresource \"aws_eks_node_group\" \"main\" {\n  # Direct control over node group settings\n}\n\n# Why: Remote EKS modules often don't expose the exact configuration we need\n# for game development workloads\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#acceptable-remote-module-usage-in-core-modules","title":"Acceptable Remote Module Usage in Core Modules","text":"<pre><code># Example: EKS add-ons where complexity reduction justifies remote module\nmodule \"eks_addons\" {\n  source = \"registry.terraform.io/example/eks-addons/aws\"\n\n  # Why acceptable:\n  # - Handles complex EKS add-on lifecycle management\n  # - Significantly reduces implementation complexity\n  # - Well-maintained with responsive maintainers\n  # - Benefits clearly outweigh the dependency costs\n}\n\n# We still prefer direct resources for core EKS cluster\nresource \"aws_eks_cluster\" \"main\" {\n  # Direct control for primary resources\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#vpc-usage-in-examples","title":"VPC Usage in Examples","text":"<pre><code># In examples, we may use well-established modules for convenience\nmodule \"vpc\" {\n  source = \"registry.terraform.io/example/vpc/aws\"\n  # Acceptable in examples for user convenience\n  # Users can replace with their own VPC implementation\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#migration-strategy","title":"Migration Strategy","text":"<p>For modules currently using remote dependencies:</p> <ol> <li>Identify usage: Audit current remote module usage</li> <li>Assess impact: Determine complexity of direct implementation</li> <li>Create timeline: Plan gradual migration to direct resources</li> <li>Maintain compatibility: Ensure variable interfaces remain stable</li> <li>Document changes: Update examples and documentation</li> </ol> <p>This approach ensures:</p> <ul> <li>Full control: We control all aspects of resource creation</li> <li>Faster iteration: No waiting for upstream changes</li> <li>Reduced complexity: Fewer dependencies to manage</li> <li>Better debugging: All code is within our control</li> <li>Customization freedom: Can modify resources for game development needs</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#centralized-logging-design-patterns","title":"Centralized Logging Design Patterns","text":"<p>CGD Toolkit modules standardize on centralized logging for visibility and troubleshooting.</p>"},{"location":"modules/DESIGN_STANDARDS.html#logging-philosophy","title":"Logging Philosophy","text":"<p>All modules provide optional centralized logging with intelligent categorization:</p> <ul> <li>User controlled: Users can enable as much logging as desired for maximum visibility</li> <li>CloudWatch standardization: Native AWS logging service as the foundation</li> <li>Monitoring flexibility: Any monitoring solution that supports CloudWatch Logs can be used</li> <li>Intelligent categorization: Logs grouped by infrastructure, application, and service layers</li> <li>Cost conscious: Configurable retention periods with sensible defaults</li> <li>Security by default: Proper IAM permissions and encryption</li> </ul> <p>Why CloudWatch Logs: We standardize on CloudWatch Logs as the native AWS logging service. From there, customers can integrate with any monitoring solution they prefer - Grafana, Datadog, Splunk, New Relic, or custom solutions. This approach provides maximum flexibility while ensuring consistent log collection.</p>"},{"location":"modules/DESIGN_STANDARDS.html#three-tier-logging-structure","title":"Three-Tier Logging Structure","text":"<p>Logs are categorized into three distinct tiers:</p>"},{"location":"modules/DESIGN_STANDARDS.html#infrastructure-logs","title":"Infrastructure Logs","text":"<p>AWS managed services and infrastructure components:</p> <pre><code># Infrastructure category maps to AWS services\ninfrastructure = {\n  \"nlb\" = {}     # Network Load Balancer access logs\n  \"alb\" = {}     # Application Load Balancer access logs\n  \"eks\" = {}     # EKS control plane logs\n  \"rds\" = {}     # RDS database logs (when applicable)\n}\n</code></pre> <p>Examples by module:</p> <ul> <li>DDC Module: NLB access logs, EKS control plane logs</li> <li>Perforce Module: NLB/ALB access logs, EKS control plane logs, RDS logs</li> <li>Jenkins Module: ALB access logs, EKS control plane logs</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#application-logs","title":"Application Logs","text":"<p>Core business logic of the primary application:</p> <pre><code># Application category maps to primary service\napplication = {\n  \"ddc\" = {}       # DDC service logs (DDC module)\n  \"perforce\" = {}  # Perforce server logs (Perforce module)\n  \"jenkins\" = {}   # Jenkins controller logs (Jenkins module)\n}\n</code></pre> <p>Examples:</p> <ul> <li>DDC: Unreal Cloud DDC application pod logs</li> <li>Perforce: P4D server logs, Helix Core logs</li> <li>Jenkins: Jenkins controller and agent logs</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#service-logs","title":"Service Logs","text":"<p>Supporting services that enable the primary application:</p> <pre><code># Service category maps to supporting components\nservice = {\n  \"scylla\" = {}    # ScyllaDB database logs (DDC module)\n  \"p4-auth\" = {}   # Perforce authentication service (Perforce module)\n  \"p4-review\" = {} # Perforce code review service (Perforce module)\n}\n</code></pre> <p>Examples:</p> <ul> <li>DDC: ScyllaDB database logs</li> <li>Perforce: P4-auth service, P4-code-review service</li> <li>Jenkins: Supporting databases, caches, or queues</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#standard-logging-variable-pattern","title":"Standard Logging Variable Pattern","text":"<p>All modules implement consistent logging configuration:</p> <pre><code>variable \"centralized_logging\" {\n  type = object({\n    infrastructure = optional(map(object({\n      enabled        = optional(bool, true)\n      retention_days = optional(number, 90)\n    })), {})\n    application = optional(map(object({\n      enabled        = optional(bool, true)\n      retention_days = optional(number, 30)\n    })), {})\n    service = optional(map(object({\n      enabled        = optional(bool, true)\n      retention_days = optional(number, 60)\n    })), {})\n    log_group_prefix = optional(string, null)\n  })\n\n  description = \"Centralized logging configuration by category\"\n  default = null\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#log-group-naming-convention","title":"Log Group Naming Convention","text":"<p>Consistent naming across all modules:</p> <pre><code># Pattern: {log_group_prefix}/{category}/{component}\n# Default prefix: \"{project_prefix}-{service_name}-{region}\"\n\n# Examples:\n# cgd-unreal-cloud-ddc-us-east-1/infrastructure/nlb\n# cgd-unreal-cloud-ddc-us-east-1/application/ddc\n# cgd-unreal-cloud-ddc-us-east-1/service/scylla\n\n# cgd-perforce-us-west-2/infrastructure/alb\n# cgd-perforce-us-west-2/application/perforce\n# cgd-perforce-us-west-2/service/p4-auth\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#usage-examples","title":"Usage Examples","text":""},{"location":"modules/DESIGN_STANDARDS.html#enable-all-logging-with-defaults","title":"Enable All Logging with Defaults","text":"<pre><code>module \"ddc\" {\n  centralized_logging = {\n    infrastructure = { nlb = {}, eks = {} }\n    application    = { ddc = {} }\n    service        = { scylla = {} }\n  }\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#custom-retention-and-prefix","title":"Custom Retention and Prefix","text":"<pre><code>module \"perforce\" {\n  centralized_logging = {\n    infrastructure = {\n      nlb = { retention_days = 365 }\n      alb = { retention_days = 180 }\n      eks = { retention_days = 90 }\n    }\n    application = {\n      perforce = { retention_days = 60 }\n    }\n    service = {\n      \"p4-auth\" = { retention_days = 30 }\n      \"p4-review\" = { retention_days = 30 }\n    }\n    log_group_prefix = \"mycompany-perforce-prod\"\n  }\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#selective-logging","title":"Selective Logging","text":"<pre><code>module \"jenkins\" {\n  centralized_logging = {\n    infrastructure = {\n      alb = { enabled = false }  # Disable ALB logging\n      eks = {}                   # Enable EKS logging only\n    }\n    application = { jenkins = {} }\n    # No service logging needed for this deployment\n  }\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#default-retention-periods","title":"Default Retention Periods","text":"<p>Cost-optimized defaults based on log type:</p> <ul> <li>Infrastructure: 90 days (AWS service troubleshooting)</li> <li>Application: 30 days (balance between debugging and cost)</li> <li>Service: 60 days (database analysis and performance tuning)</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#module-specific-logging-patterns","title":"Module-Specific Logging Patterns","text":"<p>Not all modules fit the standard 3-tier structure. Modules should only create log groups for components they actually have.</p>"},{"location":"modules/DESIGN_STANDARDS.html#single-category-pattern-vdi-module-example","title":"Single Category Pattern (VDI Module Example)","text":"<p>When modules have simple architectures where everything happens in one place:</p> <pre><code># VDI Module - Single log group for all activities\nresource \"aws_cloudwatch_log_group\" \"vdi_logs\" {\n  name = \"/${var.project_prefix}/vdi/logs\"\n  # All VDI activity: SSM execution, user creation, DCV sessions, software installation\n}\n</code></pre> <p>Use single category when:</p> <ul> <li>All functionality runs on same compute (EC2 instances)</li> <li>SSM-based architecture where everything logs to same destination</li> <li>No separate infrastructure services to log</li> <li>Simpler structure matches module reality</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#standard-3-tier-pattern-ddcperforce-module-example","title":"Standard 3-Tier Pattern (DDC/Perforce Module Example)","text":"<p>When modules have distinct infrastructure, application, and service components:</p> <pre><code># Standard pattern for complex modules\ninfrastructure = { \"nlb\" = {}, \"eks\" = {} }\napplication    = { \"ddc\" = {} }\nservice        = { \"scylla\" = {} }\n</code></pre> <p>Principle: Match logging structure to module architecture, not arbitrary standards.</p>"},{"location":"modules/DESIGN_STANDARDS.html#implementation-requirements","title":"Implementation Requirements","text":"<p>All modules must implement:</p> <ul> <li>CloudWatch Log Groups: Created for each enabled component</li> <li>Proper IAM permissions: Services can write to their log groups</li> <li>S3 integration: Long-term storage with lifecycle policies</li> <li>Encryption: Log groups encrypted with appropriate KMS keys</li> <li>Validation: Only supported components allowed per module</li> <li>Documentation: Clear explanation of what each component logs</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#component-validation","title":"Component Validation","text":"<p>Modules validate only supported components:</p> <pre><code># Each module validates its specific supported components\nvalidation {\n  condition = alltrue([\n    # Infrastructure: only components this module actually creates\n    alltrue([\n      for component in keys(var.centralized_logging.infrastructure) :\n      contains([\"nlb\", \"eks\"], component)  # DDC module example\n    ]),\n    # Application: only the primary service\n    alltrue([\n      for component in keys(var.centralized_logging.application) :\n      contains([\"ddc\"], component)  # DDC module example\n    ]),\n    # Service: only supporting services this module deploys\n    alltrue([\n      for component in keys(var.centralized_logging.service) :\n      contains([\"scylla\"], component)  # DDC module example\n    ])\n  ])\n  error_message = \"Unsupported logging component specified for this module.\"\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#cost-considerations","title":"Cost Considerations","text":"<p>Logging configuration balances visibility with cost:</p> <ul> <li>Shorter retention = lower costs: Adjust based on compliance needs</li> <li>Selective enablement: Disable non-critical logging in development</li> <li>S3 lifecycle policies: Automatic transition to cheaper storage classes</li> <li>Log sampling: Consider sampling for high-volume logs</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#security-and-compliance","title":"Security and Compliance","text":"<p>All logging implementations include:</p> <ul> <li>Encryption at rest: CloudWatch logs encrypted with KMS</li> <li>IAM least privilege: Services only access their specific log groups</li> <li>VPC Flow Logs: Optional for network troubleshooting</li> <li>Audit trails: CloudTrail integration for API calls</li> <li>Data retention: Configurable retention for compliance requirements</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#future-cgd-toolkit-monitoring-module","title":"Future: CGD Toolkit Monitoring Module","text":"<p>We're actively developing a comprehensive monitoring module:</p> <ul> <li>Amazon Managed Grafana: Dashboard solution consuming CloudWatch Logs</li> <li>Game tooling infrastructure: Monitor VDI instances, Perforce, DDC, Jenkins, and more</li> <li>Multi-region support: Unified monitoring across all regional deployments</li> <li>Optional integration: Use with any CGD Toolkit modules that have logging enabled</li> <li>No ETA yet: Still in development, but will leverage our CloudWatch Logs foundation</li> </ul> <p>Design principle: Since all CGD Toolkit modules send logs to CloudWatch Logs when enabled, any monitoring solution that supports CloudWatch integration can be used - whether it's our future monitoring module, Amazon Managed Grafana, Datadog, Splunk, or custom solutions.</p> <p>This standardized approach provides:</p> <ul> <li>Maximum visibility: Users control how much logging they want enabled</li> <li>Consistent logging: Same patterns across all CGD Toolkit modules</li> <li>Monitoring flexibility: Works with any CloudWatch-compatible monitoring solution</li> <li>Operational visibility: Comprehensive logging for troubleshooting</li> <li>Cost control: Configurable retention and selective enablement</li> <li>Security compliance: Proper encryption and access controls</li> <li>Future-ready: Foundation for CGD Toolkit monitoring module and other solutions</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#naming-strategy","title":"Naming Strategy","text":"<p>Why: AWS services have different naming patterns when using prefixes. Our approach provides predictable, referenceable names.</p> <pre><code>resource \"random_id\" \"suffix\" {\n  byte_length = 4\n  keepers = {\n    project_prefix = var.project_prefix\n    name          = local.name\n  }\n}\n\nlocals {\n  name_prefix = \"${var.project_prefix}-${local.name}\"\n  name_suffix = random_id.suffix.hex\n\n  # Predictable names across all resources\n  nlb_name    = \"${local.name_prefix}-nlb-${local.name_suffix}\"\n  bucket_name = \"${local.name_prefix}-logs-${local.name_suffix}\"\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#load-balancer-philosophy","title":"Load Balancer Philosophy","text":"<p>Why: Game services often need Layer 4 (NLB) for performance. ALB adds value for HTTP/HTTPS routing scenarios.</p> <ul> <li>NLB: Always available, required for most modules</li> <li>ALB: Optional, module-specific validation prevents unsupported usage</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#dns-patterns_1","title":"DNS Patterns","text":"<p>Why Regional: Like AWS services, we default to regional endpoints for performance, isolation, and explicit control.</p> <pre><code># Regional endpoints (our default)\n# us-east-1.ddc.company.com\n# us-west-2.ddc.company.com\n\n# Users can add global endpoints for DR/geolocation\n# ddc.company.com -&gt; failover routing to regional endpoints\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#security-patterns","title":"Security Patterns","text":""},{"location":"modules/DESIGN_STANDARDS.html#the-00000-rule","title":"The 0.0.0.0/0 Rule","text":""},{"location":"modules/DESIGN_STANDARDS.html#ingress-incoming-avoid-00000","title":"Ingress (Incoming) - Avoid 0.0.0.0/0","text":"<p>Risk: \ud83d\udd34 HIGH - Direct attack surface</p> <pre><code># \u274c DANGEROUS\nresource \"aws_vpc_security_group_ingress_rule\" \"bad\" {\n  cidr_ipv4 = \"0.0.0.0/0\"  # Opens to entire internet\n}\n\n# \u2705 USER CONTROLLED\n# Users provide security groups with their own rules\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#egress-outgoing-often-necessary","title":"Egress (Outgoing) - Often Necessary","text":"<p>Risk: \ud83d\udfe1 MEDIUM - Controlled by application</p> <pre><code># \u2705 NECESSARY for AWS APIs, updates, container registries\nresource \"aws_vpc_security_group_egress_rule\" \"aws_apis\" {\n  cidr_ipv4 = \"0.0.0.0/0\"\n  description = \"AWS APIs, ECR, OS updates\"\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#implementation-pattern","title":"Implementation Pattern","text":"<pre><code># We create internal security groups\nresource \"aws_security_group\" \"internal\" {\n  name_prefix = \"${local.name_prefix}-internal-\"\n  vpc_id      = var.vpc_id\n}\n\n# Users control external access\nresource \"aws_lb\" \"nlb\" {\n  security_groups = concat(\n    var.security_groups,                           # User-controlled\n    var.additional_security_groups.load_balancer, # Component-specific\n    [aws_security_group.internal.id]              # Internal\n  )\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#provider-patterns","title":"Provider Patterns","text":""},{"location":"modules/DESIGN_STANDARDS.html#provider-strategy-root-vs-parent-vs-submodules","title":"Provider Strategy: Root vs Parent vs Submodules","text":"<p>Why This Matters: Provider configuration depends on where Terraform runs and how modules are consumed.</p> <p>Module Consumption: We assume users will reference CGD modules remotely via Git URLs, but they could also clone/fork the toolkit and deploy from examples directories directly.</p>"},{"location":"modules/DESIGN_STANDARDS.html#root-module-where-terraform-init-runs","title":"Root Module (Where <code>terraform init</code> Runs)","text":"<p>Scenario: Users run Terraform commands here - examples, user's own infrastructure</p> <pre><code># examples/single-region-basic/versions.tf\nterraform {\n  required_providers {\n    aws        = { source = \"hashicorp/aws\", version = \"&gt;= 6.0.0\" }\n    kubernetes = { source = \"hashicorp/kubernetes\", version = \"&gt;= 2.33.0\" }\n    helm       = { source = \"hashicorp/helm\", version = \"&gt;= 2.16.0, &lt; 3.0.0\" }\n  }\n}\n\n# examples/single-region-basic/providers.tf (when needed)\nprovider \"kubernetes\" {\n  host = module.ddc.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.ddc.cluster_ca_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command = \"aws\"\n    args = [\"eks\", \"get-token\", \"--cluster-name\", module.ddc.cluster_name]\n  }\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#parent-module-cgd-toolkit-modules","title":"Parent Module (CGD Toolkit Modules)","text":"<p>Scenario: CGD Toolkit modules that both create resources AND orchestrate submodules when needed</p> <p>Single Region (Simple):</p> <pre><code># modules/unreal-cloud-ddc/main.tf\n# Parent module creates some resources directly AND orchestrates submodules\n# Parent module receives providers from root module\n\n# Direct resource creation\nresource \"aws_route53_zone\" \"private\" {\n  name = \"${var.project_prefix}.internal\"\n  vpc {\n    vpc_id = var.vpc_id\n  }\n}\n\n# Submodule orchestration\n\nmodule \"infra\" {\n  source = \"./modules/infra\"\n  providers = { aws = aws }  # Pass from root (uses default or v6 region)\n}\n\nmodule \"services\" {\n  source = \"./modules/services\"\n  providers = { kubernetes = kubernetes, helm = helm }  # Pass from root\n  depends_on = [module.infra]\n}\n</code></pre> <p>Multi-Region (Complex):</p> <pre><code># Root module: examples/multi-region/main.tf\n# User must handle multi-region complexity at root level\n\n# AWS Provider v6 - No aliases needed!\nmodule \"ddc_us_east_1\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/unreal/unreal-cloud-ddc\"\n  region = \"us-east-1\"  # AWS Provider v6 handles this automatically\n\n  # Non-enhanced providers need explicit aliases\n  providers = {\n    kubernetes = kubernetes.us_east_1\n    helm       = helm.us_east_1\n  }\n}\n\nmodule \"ddc_us_west_2\" {\n  source = \"git::https://github.com/aws-games/cloud-game-development-toolkit.git//modules/unreal/unreal-cloud-ddc\"\n  region = \"us-west-2\"  # AWS Provider v6 handles this automatically\n\n  # Non-enhanced providers need explicit aliases\n  providers = {\n    kubernetes = kubernetes.us_west_2\n    helm       = helm.us_west_2\n  }\n}\n\n# Root module must define all provider aliases\nprovider \"kubernetes\" {\n  alias = \"us_east_1\"\n  host = module.ddc_us_east_1.cluster_endpoint\n  # ... configuration\n}\n\nprovider \"kubernetes\" {\n  alias = \"us_west_2\"\n  host = module.ddc_us_west_2.cluster_endpoint\n  # ... configuration\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#submodules","title":"Submodules","text":"<p>Scenario: Receive providers from parent, use specific provider family</p> <pre><code># modules/unreal-cloud-ddc/modules/infra/main.tf\n# Uses AWS provider passed from parent\nresource \"aws_eks_cluster\" \"main\" { }\n\n# modules/unreal-cloud-ddc/modules/services/main.tf\n# Uses Kubernetes/Helm providers passed from parent\nresource \"helm_release\" \"ddc\" { }\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#provider-value-sourcing-strategies","title":"Provider Value Sourcing Strategies","text":"<p>Why This Matters: Providers need configuration values, but the source depends on timing and dependencies.</p>"},{"location":"modules/DESIGN_STANDARDS.html#option-1-data-sources-independent-resources","title":"Option 1: Data Sources (Independent Resources)","text":"<p>When: Referencing existing, independent infrastructure</p> <pre><code># Root module: examples/existing-cluster/providers.tf\ndata \"aws_eks_cluster\" \"existing\" {\n  name = var.existing_cluster_name\n}\n\ndata \"aws_eks_cluster_auth\" \"existing\" {\n  name = var.existing_cluster_name\n}\n\nprovider \"kubernetes\" {\n  host = data.aws_eks_cluster.existing.endpoint\n  cluster_ca_certificate = base64decode(data.aws_eks_cluster.existing.certificate_authority[0].data)\n  token = data.aws_eks_cluster_auth.existing.token\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#option-2-module-outputs-dependent-resources","title":"Option 2: Module Outputs (Dependent Resources)","text":"<p>When: Module creates the infrastructure that providers need</p> <pre><code># Root module: examples/single-region-basic/providers.tf\nprovider \"kubernetes\" {\n  host = module.ddc.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.ddc.cluster_ca_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command = \"aws\"\n    args = [\"eks\", \"get-token\", \"--cluster-name\", module.ddc.cluster_name]\n  }\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#option-3-statichardcoded-values","title":"Option 3: Static/Hardcoded Values","text":"<p>When: Known, unchanging values (rare, mostly for testing)</p> <pre><code># Root module: tests/setup/providers.tf\nprovider \"kubernetes\" {\n  host = \"https://test-cluster.example.com\"\n  token = var.test_cluster_token  # From CI secrets\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#conditional-provider-configuration","title":"Conditional Provider Configuration","text":"<p>Why This Matters: Provider configurations are evaluated during every plan/apply. Understanding when to use <code>try()</code> vs explicit null checks is critical.</p>"},{"location":"modules/DESIGN_STANDARDS.html#use-try-for-data-sources","title":"Use <code>try()</code> for Data Sources","text":"<p>Why: Prevents plan failures when resources don't exist yet.</p> <pre><code># \u2705 RECOMMENDED - Graceful handling of missing resources\ndata \"aws_eks_cluster\" \"existing\" {\n  count = var.cluster_name != null ? 1 : 0\n  name  = var.cluster_name\n}\n\nprovider \"kubernetes\" {\n  # try() handles both missing data source AND missing attributes\n  host = try(data.aws_eks_cluster.existing[0].endpoint, null)\n  cluster_ca_certificate = try(\n    base64decode(data.aws_eks_cluster.existing[0].certificate_authority[0].data),\n    null\n  )\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#use-explicit-null-checks-for-module-outputs-cgd-toolkit-pattern","title":"Use Explicit Null Checks for Module Outputs (CGD Toolkit Pattern)","text":"<p>Why: Clearer dependency logic and better debugging.</p> <pre><code># \u2705 RECOMMENDED - Clear dependency logic\nprovider \"kubernetes\" {\n  host = module.infra.cluster_endpoint != null ?\n    module.infra.cluster_endpoint : null\n  cluster_ca_certificate = module.infra.cluster_ca_data != null ?\n    base64decode(module.infra.cluster_ca_data) : null\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command = \"aws\"\n    args = [\"eks\", \"get-token\", \"--cluster-name\", module.infra.cluster_name]\n  }\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#multi-region-global-replication-architecture","title":"Multi-Region: Global Replication Architecture","text":""},{"location":"modules/DESIGN_STANDARDS.html#standard-terraform-multi-region-patterns","title":"Standard Terraform Multi-Region Patterns","text":"<p>How most Terraform users handle multi-region deployments:</p>"},{"location":"modules/DESIGN_STANDARDS.html#pattern-a-monorepo-regional-structure-less-recommended","title":"Pattern A: Monorepo Regional Structure (Less Recommended)","text":"<p>One Git repository per AWS region containing ALL applications:</p> <pre><code>company-infrastructure-us-east-1/\n\u251c\u2500\u2500 networking/\n\u251c\u2500\u2500 databases/\n\u251c\u2500\u2500 applications/\n\u2502   \u251c\u2500\u2500 ddc/\n\u2502   \u251c\u2500\u2500 perforce/\n\u2502   \u2514\u2500\u2500 jenkins/\n\u2514\u2500\u2500 terraform.tfstate\n\ncompany-infrastructure-us-west-2/\n\u251c\u2500\u2500 networking/\n\u251c\u2500\u2500 databases/\n\u251c\u2500\u2500 applications/\n\u2502   \u251c\u2500\u2500 ddc/\n\u2502   \u251c\u2500\u2500 perforce/\n\u2502   \u2514\u2500\u2500 jenkins/\n\u2514\u2500\u2500 terraform.tfstate\n</code></pre> <p>Pros: Complete regional isolation Cons: Repository proliferation, monolithic state files, team conflicts</p>"},{"location":"modules/DESIGN_STANDARDS.html#pattern-b-application-specific-with-regional-folders-recommended","title":"Pattern B: Application-Specific with Regional Folders (Recommended)","text":"<p>Application-specific repositories with regional deployment folders:</p> <pre><code>company-ddc-infrastructure/\n\u251c\u2500\u2500 deployments/\n\u2502   \u251c\u2500\u2500 us-east-1/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfstate\n\u2502   \u2514\u2500\u2500 us-west-2/\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u251c\u2500\u2500 variables.tf\n\u2502       \u2514\u2500\u2500 terraform.tfstate\n\u2514\u2500\u2500 modules/\n    \u2514\u2500\u2500 shared-components/\n\ncompany-perforce-infrastructure/\n\u251c\u2500\u2500 deployments/\n\u2502   \u251c\u2500\u2500 us-east-1/\n\u2502   \u2514\u2500\u2500 us-west-2/\n\u2514\u2500\u2500 modules/\n</code></pre> <p>Pros: Application-focused ownership, separate state files, team independence Cons: Requires coordination for cross-region features</p> <p>Why Pattern B Works Better:</p> <ul> <li>Application ownership: Repository aligns with team responsibilities</li> <li>Separate state files: Each region has independent, manageable state</li> <li>Team independence: Teams can work on different regions simultaneously</li> <li>Focused scope: Smaller, application-specific state files</li> <li>Scalable: Multiple deployments per region possible</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#cgd-toolkit-multi-region-philosophy","title":"CGD Toolkit Multi-Region Philosophy","text":"<p>Multi-region in game development is about PERFORMANCE, not disaster recovery.</p> <p>Why We're Different: Game development applications (Perforce, DDC) work perfectly in single-region but are often deployed multi-region for geographically distributed teams.</p> <p>Primary Use Case: Global Development Teams</p> <ul> <li>DDC: Works great single-region, but multi-region provides low-latency cache access for global teams</li> <li>Perforce: Perfectly functional single-region, but multi-region enables synchronized repositories across continents</li> <li>Performance-driven: Multi-region reduces latency for geographically distributed developers</li> <li>Single-region viable: Both applications work perfectly fine in single-region deployments</li> <li>Multi-region benefit: Global teams get better performance with regional data locality</li> </ul> <p>Primary Purpose: Performance, NOT Disaster Recovery:</p> <ul> <li>Performance-driven: Multi-region DDC/Perforce is for active global usage and low-latency access</li> <li>DR as side benefit: Cross-region replication for performance means either region could serve as DR</li> <li>Nuanced DR considerations: While data replication enables DR capabilities, full DR requires application-specific planning</li> <li>Separate DR deployments: For dedicated DR (not performance), use completely separate Terraform deployments</li> </ul> <p>Why We Break the Anti-Pattern Rule:</p> <ul> <li>Performance benefit: Global teams get better performance with low-latency regional access</li> <li>Cross-region coordination: When deploying multi-region, replication setup requires shared resources</li> <li>Single system: Multi-region deployments create one global application, not separate deployments</li> <li>Optional optimization: Multi-region is a performance optimization, not a technical requirement</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#single-apply-requirement","title":"Single Apply Requirement","text":"<p>CRITICAL: All CGD Toolkit modules MUST support single-step multi-region deployment.</p> <pre><code># This MUST work - single command deploys all regions\nterraform apply\n# \u2705 Deploys us-east-1 DDC + us-west-2 DDC + cross-region replication\n</code></pre> <p>Why Single Apply Matters:</p> <ul> <li>Global replication setup: Cross-region configuration happens during initial deployment</li> <li>Dependency coordination: Primary region creates resources that secondary regions need</li> <li>User experience: Multi-region should be as easy as single-region</li> <li>Production readiness: No manual coordination steps between regions</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#terraform-multi-region-fundamentals","title":"Terraform Multi-Region Fundamentals","text":""},{"location":"modules/DESIGN_STANDARDS.html#each-module-instance-exactly-one-region","title":"Each Module Instance = Exactly One Region","text":"<pre><code># This is the fundamental pattern - each module does ONE region only\nmodule \"ddc_us_east_1\" {\n  source = \"../../modules/unreal-cloud-ddc\"\n  region = \"us-east-1\"  # This instance ONLY handles us-east-1\n}\n\nmodule \"ddc_us_west_2\" {\n  source = \"../../modules/unreal-cloud-ddc\"\n  region = \"us-west-2\"  # This instance ONLY handles us-west-2\n}\n</code></pre> <p>Key Principle: Users instantiate the module once per region they want.</p> <p>AWS Provider v6 Revolution: Enhanced region support eliminates AWS provider aliases.</p>"},{"location":"modules/DESIGN_STANDARDS.html#before-aws-provider-v6-traditional","title":"Before AWS Provider v6 (Traditional)","text":"<p>Problem: Every region needed explicit AWS provider aliases</p> <pre><code># Root module - OLD WAY (still needed for non-AWS providers)\nprovider \"aws\" {\n  alias  = \"us_east_1\"\n  region = \"us-east-1\"\n}\n\nprovider \"aws\" {\n  alias  = \"us_west_2\"\n  region = \"us-west-2\"\n}\n\nmodule \"ddc_us_east_1\" {\n  source = \"./modules/unreal-cloud-ddc\"\n  providers = {\n    aws        = aws.us_east_1     # Explicit AWS alias required\n    kubernetes = kubernetes.us_east_1\n  }\n}\n\nmodule \"ddc_us_west_2\" {\n  source = \"./modules/unreal-cloud-ddc\"\n  providers = {\n    aws        = aws.us_west_2     # Explicit AWS alias required\n    kubernetes = kubernetes.us_west_2\n  }\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#with-aws-provider-v6-enhanced-region-support","title":"With AWS Provider v6 (Enhanced Region Support)","text":"<p>Magic: AWS provider automatically inherits region from module configuration</p> <pre><code># Root module - NEW WAY\n# NO AWS provider aliases needed!\n\nmodule \"ddc_us_east_1\" {\n  source = \"./modules/unreal-cloud-ddc\"\n  region = \"us-east-1\"  # AWS Provider v6 magic - auto-inherits region\n\n  # Only non-enhanced providers need aliases\n  providers = {\n    kubernetes = kubernetes.us_east_1\n    helm       = helm.us_east_1\n  }\n}\n\nmodule \"ddc_us_west_2\" {\n  source = \"./modules/unreal-cloud-ddc\"\n  region = \"us-west-2\"  # AWS Provider v6 magic - auto-inherits region\n\n  # Only non-enhanced providers need aliases\n  providers = {\n    kubernetes = kubernetes.us_west_2\n    helm       = helm.us_west_2\n  }\n}\n\n# Still need aliases for non-enhanced providers\nprovider \"kubernetes\" {\n  alias = \"us_east_1\"\n  host = module.ddc_us_east_1.cluster_endpoint\n}\n\nprovider \"kubernetes\" {\n  alias = \"us_west_2\"\n  host = module.ddc_us_west_2.cluster_endpoint\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#how-aws-provider-v6-works","title":"How AWS Provider v6 Works","text":"<ol> <li>Module declares region: <code>region = \"us-east-1\"</code></li> <li>AWS provider auto-configures: Uses that region automatically</li> <li>No aliases needed: AWS resources deploy to correct region</li> <li>Simple scaling: Add regions by adding module blocks (max 2 recommended)</li> </ol>"},{"location":"modules/DESIGN_STANDARDS.html#what-still-needs-aliases","title":"What Still Needs Aliases","text":"<ul> <li>Kubernetes provider: Not enhanced, needs manual aliases</li> <li>Helm provider: Not enhanced, needs manual aliases</li> <li>kubectl provider: Not enhanced, needs manual aliases</li> <li>Any other provider: Only AWS has enhanced region support</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#inside-cgd-toolkit-modules","title":"Inside CGD Toolkit Modules","text":"<p>How modules handle the region variable:</p> <pre><code># modules/unreal-cloud-ddc/variables.tf\nvariable \"region\" {\n  type        = string\n  description = \"AWS region for deployment\"\n}\n\n# modules/unreal-cloud-ddc/main.tf\n# AWS resources automatically use the region from variable\nresource \"aws_eks_cluster\" \"main\" {\n  name = \"${local.name_prefix}-cluster-${var.region}\"\n  # AWS Provider v6 automatically uses var.region\n}\n\n# Pass region to submodules\nmodule \"infra\" {\n  source = \"./modules/infra\"\n  region = var.region  # Propagate region down\n  providers = { aws = aws }  # AWS provider inherits region automatically\n}\n</code></pre> <p>Benefits:</p> <ul> <li>AWS Provider v6: Simplified region handling, no aliases needed</li> <li>Other providers: Still require manual aliases per region</li> <li>Clean code: Each module block identical except for region</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#multi-region-implementation-pattern","title":"Multi-Region Implementation Pattern","text":""},{"location":"modules/DESIGN_STANDARDS.html#explicit-module-blocks-only-recommended-pattern","title":"Explicit Module Blocks (Only Recommended Pattern)","text":"<p>Best for: Multi-region deployments (max 2 regions)</p> <pre><code># examples/multi-region-basic/main.tf\n# Clear, explicit, easy to understand\n\n# Primary Region - Creates shared resources\nmodule \"ddc_primary\" {\n  source = \"../../modules/unreal-cloud-ddc\"\n  region = \"us-east-1\"\n\n  providers = {\n    kubernetes = kubernetes.primary\n    helm       = helm.primary\n  }\n\n  scylla_config = {\n    current_region = {\n      datacenter_name = \"us_east\"\n      replication_factor = 3\n      node_count = 3\n    }\n    enable_cross_region_replication = true\n  }\n\n  # Primary creates bearer token for replication\n  bearer_token_replica_regions = [\"us-west-2\"]\n}\n\n# Secondary Region - Uses shared resources\nmodule \"ddc_secondary\" {\n  source = \"../../modules/unreal-cloud-ddc\"\n  region = \"us-west-2\"\n\n  providers = {\n    kubernetes = kubernetes.secondary\n    helm       = helm.secondary\n  }\n\n  scylla_config = {\n    current_region = {\n      datacenter_name = \"us_west\"\n      replication_factor = 2\n      node_count = 2\n    }\n    enable_cross_region_replication = true\n  }\n\n  # Secondary uses primary's bearer token\n  create_bearer_token = false\n  bearer_token_secret_arn = module.ddc_primary.bearer_token_secret_arn\n\n  depends_on = [module.ddc_primary]  # Ensures proper ordering\n}\n</code></pre> <p>Benefits:</p> <ul> <li>\u2705 Clear and explicit - obvious what's deployed where</li> <li>\u2705 Different configurations - each region can have unique settings</li> <li>\u2705 Easy debugging - clear dependency chain</li> <li>\u2705 Single apply - all regions deployed together</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#dns-and-regional-endpoint-patterns","title":"DNS and Regional Endpoint Patterns","text":""},{"location":"modules/DESIGN_STANDARDS.html#private-dns-zones-always-created","title":"Private DNS Zones (Always Created)","text":"<p>All CGD Toolkit modules automatically create private DNS zones for internal service discovery:</p> <pre><code># Always create private zone for internal routing\nresource \"aws_route53_zone\" \"private\" {\n  name = var.existing_route53_public_hosted_zone_name != null ?\n    \"${var.project_prefix}.${var.existing_route53_public_hosted_zone_name}\" :\n    \"${var.project_prefix}.internal\"\n\n  vpc {\n    vpc_id = var.existing_vpc_id\n  }\n}\n\n# Internal service discovery records\nresource \"aws_route53_record\" \"service_internal\" {\n  zone_id = aws_route53_zone.private.zone_id\n  name    = \"service\"\n  type    = \"A\"\n  ttl     = 300\n  records = [aws_lb.nlb.dns_name]\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#regional-endpoint-strategy","title":"Regional Endpoint Strategy","text":"<p>Following AWS service patterns, we default to regional endpoints:</p> <pre><code># Regional DNS pattern (our default)\nlocals {\n  regional_dns_name = var.existing_route53_public_hosted_zone_name != null ?\n    \"${var.region}.${local.service_name}.${var.existing_route53_public_hosted_zone_name}\" :\n    null\n\n  service_name = \"ddc\"  # or \"perforce\", \"jenkins\", etc.\n}\n\n# Examples of regional endpoints:\n# us-east-1.ddc.company.com\n# us-west-2.ddc.company.com\n# eu-west-1.perforce.company.com\n</code></pre> <p>Why Regional Endpoints:</p> <ul> <li>Performance: Direct routing to nearest region</li> <li>Isolation: Regional failures don't affect DNS routing</li> <li>Explicit control: Users know exactly which region they're accessing</li> <li>AWS consistency: Follows AWS service endpoint patterns</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#global-endpoint-flexibility","title":"Global Endpoint Flexibility","text":"<p>Users can optionally create global endpoints with routing policies:</p> <pre><code># Optional: Global endpoint with latency-based routing\nresource \"aws_route53_record\" \"global_latency\" {\n  zone_id = var.existing_route53_public_hosted_zone_id\n  name    = \"ddc\"  # Global endpoint: ddc.company.com\n  type    = \"A\"\n\n  set_identifier = \"us-east-1\"\n  latency_routing_policy {\n    region = \"us-east-1\"\n  }\n\n  alias {\n    name    = module.ddc_primary.nlb_dns_name\n    zone_id = module.ddc_primary.nlb_zone_id\n  }\n}\n\nresource \"aws_route53_record\" \"global_latency_secondary\" {\n  zone_id = var.existing_route53_public_hosted_zone_id\n  name    = \"ddc\"  # Same global endpoint\n  type    = \"A\"\n\n  set_identifier = \"us-west-2\"\n  latency_routing_policy {\n    region = \"us-west-2\"\n  }\n\n  alias {\n    name    = module.ddc_secondary.nlb_dns_name\n    zone_id = module.ddc_secondary.nlb_zone_id\n  }\n}\n</code></pre> <p>Global Routing Options:</p> <ul> <li>Latency-based: Route to lowest latency region</li> <li>Geolocation: Route based on user's geographic location</li> <li>Failover: Primary/secondary with health checks</li> <li>Weighted: Distribute traffic by percentage</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#dns-output-strategy","title":"DNS Output Strategy","text":"<p>Modules provide both regional and global DNS flexibility:</p> <pre><code># Module outputs for DNS flexibility\noutput \"dns_endpoints\" {\n  description = \"DNS endpoints for service access\"\n  value = {\n    # Regional endpoints (always available)\n    regional = {\n      public_dns  = local.regional_dns_name\n      private_dns = \"${local.service_name}.${aws_route53_zone.private.name}\"\n    }\n\n    # Load balancer details for global routing\n    load_balancer = {\n      nlb_dns_name = aws_lb.nlb.dns_name\n      nlb_zone_id  = aws_lb.nlb.zone_id\n    }\n  }\n}\n</code></pre> <p>This approach provides:</p> <ul> <li>Regional by default: Each region gets its own endpoint</li> <li>Global flexibility: Users can create global endpoints if needed</li> <li>Internal routing: Private DNS for service-to-service communication</li> <li>Load balancer access: Direct NLB access for advanced routing scenarios</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#networking-and-security-boundaries","title":"Networking and Security Boundaries","text":""},{"location":"modules/DESIGN_STANDARDS.html#clear-demarcation-what-modules-dont-create","title":"Clear Demarcation: What Modules DON'T Create","text":"<p>CGD Toolkit modules have clear boundaries - we don't create foundational infrastructure:</p> <p>\ud83d\udeab Modules DO NOT Create:</p> <ul> <li>VPCs and Subnets: Users provide existing VPC and subnet IDs</li> <li>SSL/TLS Certificates: Users provide existing ACM certificate ARNs</li> <li>Public Hosted Zones: Users provide existing Route53 hosted zone names</li> <li>VPC-to-VPC Connectivity: Peering connections, Transit Gateway, etc.</li> <li>Network ACLs: Users manage network-level security</li> <li>Internet/NAT Gateways: Users provide connectivity infrastructure</li> </ul> <p>\u2705 Modules DO Create:</p> <ul> <li>Private DNS zones: For internal service discovery</li> <li>Security groups: For service-specific access control</li> <li>Load balancers: NLB/ALB for service access</li> <li>DNS records: In both private and public zones (when provided)</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#ssltls-certificate-integration","title":"SSL/TLS Certificate Integration","text":"<p>Modules integrate with existing certificates, don't create them:</p> <pre><code># User creates certificate outside module\nresource \"aws_acm_certificate\" \"service_cert\" {\n  domain_name       = \"*.ddc.company.com\"\n  validation_method = \"DNS\"\n\n  subject_alternative_names = [\n    \"ddc.company.com\",\n    \"*.us-east-1.ddc.company.com\",\n    \"*.us-west-2.ddc.company.com\"\n  ]\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\n# Module accepts certificate reference\nmodule \"ddc\" {\n  source = \"../../modules/unreal-cloud-ddc\"\n\n  # Pass existing certificate ARN\n  existing_certificate_arn = aws_acm_certificate.service_cert.arn\n\n  # Module configures HTTPS listeners\n  # Module handles certificate attachment to load balancers\n}\n</code></pre> <p>Why This Approach:</p> <ul> <li>Certificate lifecycle: Users control certificate renewal and management</li> <li>Domain ownership: Users own and validate their domains</li> <li>Security control: Certificate management stays with domain owners</li> <li>Flexibility: Users can use existing certificate management processes</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#vpc-and-subnet-integration","title":"VPC and Subnet Integration","text":"<p>Modules work within existing network infrastructure:</p> <pre><code># User provides existing network infrastructure\nmodule \"ddc\" {\n  source = \"../../modules/unreal-cloud-ddc\"\n\n  # Existing VPC (user-created)\n  existing_vpc_id = \"vpc-12345678\"\n\n  # Existing subnets (user-created)\n  existing_load_balancer_subnets = [\n    \"subnet-12345678\",  # Public subnet for internet-facing LB\n    \"subnet-87654321\"   # Public subnet for HA\n  ]\n\n  existing_service_subnets = [\n    \"subnet-abcdef12\",  # Private subnet for EKS/services\n    \"subnet-21fedcba\"   # Private subnet for HA\n  ]\n\n  # Module creates resources within provided network\n}\n</code></pre> <p>Network Architecture Assumptions:</p> <ul> <li>Public subnets: For internet-facing load balancers</li> <li>Private subnets: For EKS clusters, databases, internal services</li> <li>NAT Gateway: Users provide internet access for private subnets</li> <li>Route tables: Users configure routing for subnets</li> <li>VPC endpoints: Users create for AWS service access (optional)</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#security-group-strategy_1","title":"Security Group Strategy","text":"<p>Modules create service-specific security groups, users control external access:</p> <pre><code># Users create external access security groups\nresource \"aws_security_group\" \"office_access\" {\n  name_prefix = \"office-access-\"\n  vpc_id      = var.vpc_id\n}\n\nresource \"aws_vpc_security_group_ingress_rule\" \"office_https\" {\n  security_group_id = aws_security_group.office_access.id\n  description       = \"HTTPS from office network\"\n  ip_protocol       = \"tcp\"\n  from_port         = 443\n  to_port           = 443\n  cidr_ipv4         = \"203.0.113.0/24\"  # Office CIDR\n}\n\n# Module accepts user-controlled security groups\nmodule \"ddc\" {\n  source = \"../../modules/unreal-cloud-ddc\"\n\n  # User-controlled external access\n  existing_security_groups = [\n    aws_security_group.office_access.id\n  ]\n\n  # Module creates internal security groups for service communication\n}\n</code></pre> <p>Security Responsibilities:</p> <ul> <li>Users control: External access rules, CIDR blocks, source security groups</li> <li>Modules create: Internal service communication rules, AWS API access</li> <li>Principle: Users define \"who can access\", modules define \"how services communicate\"</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#public-hosted-zone-integration","title":"Public Hosted Zone Integration","text":"<p>Modules use existing public zones, don't create them:</p> <pre><code># User owns and manages public hosted zone\ndata \"aws_route53_zone\" \"company\" {\n  name = \"company.com\"\n}\n\n# Module uses existing zone for public DNS records\nmodule \"ddc\" {\n  source = \"../../modules/unreal-cloud-ddc\"\n\n  # Reference existing public zone\n  existing_route53_public_hosted_zone_name = \"company.com\"\n\n  # Module creates records like: us-east-1.ddc.company.com\n  # Module does NOT create the company.com zone\n}\n</code></pre> <p>DNS Responsibilities:</p> <ul> <li>Users own: Domain registration, public hosted zone management</li> <li>Modules create: Service-specific DNS records in provided zones</li> <li>Private zones: Modules always create for internal service discovery</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#multi-region-network-considerations","title":"Multi-Region Network Considerations","text":"<p>For multi-region deployments, users handle cross-region connectivity:</p> <pre><code># Users create VPC peering or Transit Gateway (outside modules)\nresource \"aws_vpc_peering_connection\" \"cross_region\" {\n  vpc_id      = var.primary_vpc_id    # us-east-1\n  peer_vpc_id = var.secondary_vpc_id  # us-west-2\n  peer_region = \"us-west-2\"\n\n  # Users manage cross-region network connectivity\n}\n\n# Modules work within each region's VPC independently\nmodule \"ddc_primary\" {\n  existing_vpc_id = var.primary_vpc_id    # us-east-1 VPC\n}\n\nmodule \"ddc_secondary\" {\n  existing_vpc_id = var.secondary_vpc_id  # us-west-2 VPC\n}\n</code></pre> <p>Cross-Region Network Responsibilities:</p> <ul> <li>Users handle: VPC peering, Transit Gateway, cross-region routing</li> <li>Modules handle: Application-level cross-region communication (database replication, etc.)</li> <li>Clear separation: Network connectivity vs. application connectivity</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#example-integration-pattern","title":"Example Integration Pattern","text":"<p>Complete example showing user vs. module responsibilities:</p> <pre><code># USER RESPONSIBILITIES (outside module)\n# 1. VPC and networking\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n}\n\n# 2. SSL certificate\nresource \"aws_acm_certificate\" \"ddc\" {\n  domain_name = \"*.ddc.company.com\"\n}\n\n# 3. External access security group\nresource \"aws_security_group\" \"external_access\" {\n  vpc_id = aws_vpc.main.id\n}\n\n# MODULE RESPONSIBILITIES (inside module)\nmodule \"ddc\" {\n  source = \"../../modules/unreal-cloud-ddc\"\n\n  # Use existing infrastructure\n  existing_vpc_id                          = aws_vpc.main.id\n  existing_certificate_arn                 = aws_acm_certificate.ddc.arn\n  existing_security_groups                 = [aws_security_group.external_access.id]\n  existing_route53_public_hosted_zone_name = \"company.com\"\n\n  # Module creates: EKS, NLB, private DNS, internal security groups\n}\n</code></pre> <p>This pattern ensures:</p> <ul> <li>Clear ownership: Users own foundational infrastructure</li> <li>Module focus: Modules focus on service-specific resources</li> <li>Flexibility: Users can integrate with existing infrastructure</li> <li>Security: Users control access boundaries, modules handle service communication</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#multi-region-design-requirements-for-all-modules","title":"Multi-Region Design Requirements for All Modules","text":""},{"location":"modules/DESIGN_STANDARDS.html#must-support-single-apply-for-inherently-multi-region-apps","title":"MUST Support Single Apply (For Inherently Multi-Region Apps)","text":"<p>CGD Toolkit modules that are inherently multi-region MUST enable single-step deployment:</p> <pre><code># This MUST work for Perforce, DDC, and similar cross-region apps\ncd examples/multi-region-basic/\nterraform init\nterraform apply  # Deploys PRIMARY + SECONDARY regions (MAX 2)\n</code></pre> <p>\u26a0\ufe0f IMPORTANT: This is ONLY for applications that require cross-region replication by design.</p>"},{"location":"modules/DESIGN_STANDARDS.html#cross-region-coordination-patterns","title":"Cross-Region Coordination Patterns","text":"<p>Primary/Secondary Pattern (Recommended):</p> <ul> <li>Primary region: Creates shared resources (bearer tokens, seed nodes)</li> <li>Secondary regions: Reference primary's outputs</li> <li>Dependencies: <code>depends_on = [module.primary]</code> ensures proper ordering</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#module-implementation-standards","title":"Module Implementation Standards","text":"<p>For inherently multi-region modules (Perforce, DDC):</p> <ul> <li>Single apply: PRIMARY + SECONDARY regions (MAX 2) deploy with one <code>terraform apply</code></li> <li>Cross-region variables: Support peer region configuration</li> <li>Dependency management: Use <code>depends_on</code> for proper ordering</li> <li>Regional DNS: Support regional endpoints (us-east-1.service.domain.com)</li> <li>Shared resources: Primary creates, secondary references</li> <li>Provider compatibility: Work with AWS Provider v6 enhanced regions</li> <li>Example provided: Working multi-region example in <code>examples/</code></li> <li>Documentation: Clear guidance on when to use separate deployments instead</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#critical-when-not-to-use-multi-region-single-state","title":"\u26a0\ufe0f CRITICAL: When NOT to Use Multi-Region Single State","text":"<p>\ud83d\udeab ABSOLUTE ANTI-PATTERNS:</p> <ul> <li>General applications: Most apps should be single-region</li> <li>Dedicated disaster recovery: Use completely separate Terraform deployments for DR-only scenarios</li> <li>Environment separation: dev/staging/prod should be separate states</li> <li>\"Just in case\" deployments: Don't deploy to regions you don't actively use</li> <li>More than 2 regions in one state: Creates unmanageable complexity</li> </ul> <p>\u2705 ONLY Valid Use Cases:</p> <ul> <li>Applications that benefit from multi-region: Perforce, DDC where cross-region replication improves performance</li> <li>Active global usage: All regions actively used by distributed teams for better performance</li> <li>Performance optimization: Low-latency access across continents for same application data</li> <li>Maximum 1-2 regions: Keep state files manageable</li> <li>Single-region alternative: Remember these applications work perfectly fine single-region too</li> </ul> <p>\ud83c\udfaf The Rule: If your application doesn't REQUIRE cross-region data replication for performance/functionality, use separate Terraform deployments per region.</p> <p>DR Considerations:</p> <ul> <li>Side benefit: Performance-driven replication means either region could serve as DR</li> <li>Application-specific: Each application (DDC, Perforce) has different DR capabilities and requirements</li> <li>Not primary purpose: DR should not be the main reason for choosing multi-region single-state pattern</li> <li>Dedicated DR: For DR-only scenarios, use separate Terraform deployments in different regions</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#summary-multi-region-best-practices","title":"Summary: Multi-Region Best Practices","text":"<p>Recommended Approach: Use explicit module blocks - MAX 2 regions</p> <pre><code>module \"service_primary\" { region = \"us-east-1\" }\nmodule \"service_secondary\" { region = \"us-west-2\" }\n# \u274c DON'T ADD MORE - Use separate Terraform deployments instead\n</code></pre> <p>\ud83c\udfaf For Most Applications: Deploy each region as separate Terraform root modules</p> <pre><code># Recommended pattern for most applications (follows Pattern B above)\ncd deployments/us-east-1/\nterraform apply  # Separate state file\n\ncd ../us-west-2/\nterraform apply  # Separate state file\n</code></pre> <p>This follows Pattern B (Application-Specific with Regional Folders) and provides:</p> <ul> <li>Independent state files: Each region manageable separately</li> <li>Team parallelism: Multiple teams can work simultaneously</li> <li>Reduced blast radius: Regional isolation prevents cascading failures</li> <li>Application focus: Repository ownership aligns with team responsibilities</li> <li>Standard tooling: Works with existing Terraform workflows</li> </ul> <p>Key Principles:</p> <ul> <li>\u2705 Each module instance = exactly one region</li> <li>\u26a0\ufe0f Single apply ONLY for inherently multi-region apps (Perforce, DDC)</li> <li>\u2705 Multi-region is for performance, not DR</li> <li>\u26a0\ufe0f Maximum 1-2 regions per state file</li> <li>\u2705 Most applications should use separate Terraform deployments per region</li> <li>\u2705 Use explicit module blocks, not dynamic generation</li> <li>\u2705 AWS Provider v6 eliminates AWS provider aliases</li> </ul> <p>Benefits:</p> <ul> <li>Performance: Low latency for global teams</li> <li>Single deployment: All regions with one <code>terraform apply</code></li> <li>Global replication: Cross-region data sharing</li> <li>AWS Provider v6: Simplified region handling</li> </ul> <p>\u26a0\ufe0f CRITICAL Considerations:</p> <ul> <li>\ud83d\udd25 State file explosion: More regions = exponentially larger, slower state</li> <li>\ud83d\udd25 Massive blast radius: One mistake destroys all regions</li> <li>\ud83d\udd25 Performance degradation: <code>terraform plan</code> becomes painfully slow</li> <li>\ud83d\udd25 Team paralysis: Multiple teams can't work independently</li> <li>\ud83d\udd25 Debugging nightmare: Finding issues across regions becomes impossible</li> <li>Network costs: Cross-region data transfer charges</li> <li>Complexity: Exponentially more moving parts to troubleshoot</li> </ul> <p>\ud83c\udfaf Solution: Use separate Terraform deployments for most applications</p>"},{"location":"modules/DESIGN_STANDARDS.html#version-conflicts-and-resolution","title":"Version Conflicts and Resolution","text":"<p>Common Problem: Different modules require different provider versions</p>"},{"location":"modules/DESIGN_STANDARDS.html#conflict-scenario","title":"Conflict Scenario","text":"<pre><code># Module A requires\nkubernetes = { version = \"&gt;= 2.30.0\" }\n\n# Module B requires\nkubernetes = { version = \"&gt;= 2.33.0, &lt; 2.35.0\" }\n\n# Root module must satisfy BOTH\nkubernetes = { version = \"&gt;= 2.33.0, &lt; 2.35.0\" }  # Intersection\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#resolution-strategy","title":"Resolution Strategy","text":"<ol> <li>Use intersection of all constraints: Find version range that satisfies all modules</li> <li>Update modules: Align version requirements across CGD Toolkit</li> <li>Test compatibility: Ensure chosen version works with all modules</li> <li>Document decisions: Explain version choices in examples</li> </ol>"},{"location":"modules/DESIGN_STANDARDS.html#multi-region-version-management","title":"Multi-Region Version Management","text":"<pre><code># Root module must declare ALL provider versions for ALL regions\nterraform {\n  required_providers {\n    aws = { version = \"&gt;= 6.0.0\" }  # Enhanced region support\n    kubernetes = { version = \"&gt;= 2.33.0\" }  # All regions use same version\n    helm = { version = \"&gt;= 2.16.0, &lt; 3.0.0\" }\n  }\n}\n\n# Each region gets same provider versions\nprovider \"kubernetes\" {\n  alias = \"us_east_1\"\n  # Same version as declared above\n}\n\nprovider \"kubernetes\" {\n  alias = \"us_west_2\"\n  # Same version as declared above\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#provider-configuration-timing","title":"Provider Configuration Timing","text":"<p>Critical: Understanding WHEN provider configurations are evaluated</p> <pre><code># Terraform command lifecycle:\n# 1. terraform init - Downloads providers, NO configuration evaluation\n# 2. terraform plan - Provider configurations evaluated HERE\n# 3. terraform apply - Configurations re-evaluated if dependencies changed\n</code></pre> <p>Implications:</p> <ul> <li>First run: Infrastructure doesn't exist, providers get null values</li> <li>Second run: Infrastructure exists, providers get real values</li> <li>Dependencies: Provider configs must handle missing dependencies gracefully</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#aws-provider-best-practices","title":"AWS Provider Best Practices","text":""},{"location":"modules/DESIGN_STANDARDS.html#security-group-rules","title":"Security Group Rules","text":"<pre><code># \u2705 Use dedicated rule resources (AWS Provider v6 requirement)\nresource \"aws_vpc_security_group_ingress_rule\" \"example\" {\n  security_group_id = aws_security_group.example.id\n  description       = \"HTTP access from office network\"\n  ip_protocol       = \"tcp\"\n  from_port         = 80\n  to_port           = 80\n  cidr_ipv4         = \"203.0.113.0/24\"\n}\n\n# \u274c Don't use inline rules or aws_security_group_rule\nresource \"aws_security_group\" \"bad\" {\n  ingress { /* ... */ }  # Deprecated pattern\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#iam-policies","title":"IAM Policies","text":"<pre><code># \u2705 Use policy documents\ndata \"aws_iam_policy_document\" \"example\" {\n  statement {\n    effect = \"Allow\"\n    actions = [\"s3:GetObject\", \"s3:PutObject\"]\n    resources = [\"${aws_s3_bucket.example.arn}/*\"]\n  }\n}\n\nresource \"aws_iam_policy\" \"example\" {\n  name   = \"example-policy\"\n  policy = data.aws_iam_policy_document.example.json\n}\n\n# \u274c Avoid jsonencode unless absolutely necessary\nresource \"aws_iam_policy\" \"bad\" {\n  policy = jsonencode({ /* ... */ })\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#iam-role-attachments","title":"IAM Role Attachments","text":"<pre><code># \u2705 Use attachment resources\nresource \"aws_iam_role\" \"example\" {\n  name = \"example-role\"\n  assume_role_policy = data.aws_iam_policy_document.assume_role.json\n}\n\nresource \"aws_iam_role_policy_attachment\" \"example\" {\n  role       = aws_iam_role.example.name\n  policy_arn = \"arn:aws:iam::aws:policy/ReadOnlyAccess\"\n}\n\n# \u274c Don't use deprecated arguments\nresource \"aws_iam_role\" \"bad\" {\n  managed_policy_arns = [\"...\"]  # Deprecated\n  inline_policy { /* ... */ }    # Deprecated\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#output-strategy","title":"Output Strategy","text":""},{"location":"modules/DESIGN_STANDARDS.html#what-to-include","title":"What to Include","text":"<p>Philosophy: Expose what users commonly need, expand based on requests.</p> <pre><code># Connection information\noutput \"service_endpoints\" {\n  description = \"Service connection endpoints\"\n  value = {\n    nlb_dns   = aws_lb.nlb.dns_name\n    https_url = local.public_dns_name != null ? \"https://${local.public_dns_name}\" : null\n  }\n}\n\n# Integration points\noutput \"cluster_info\" {\n  description = \"EKS cluster information for kubectl\"\n  value = {\n    cluster_name     = aws_eks_cluster.main.name\n    cluster_endpoint = aws_eks_cluster.main.endpoint\n    cluster_ca_data  = aws_eks_cluster.main.certificate_authority[0].data\n  }\n}\n\n# Automation helpers\noutput \"security_group_ids\" {\n  description = \"Security group IDs for additional rules\"\n  value = {\n    nlb      = aws_security_group.nlb.id\n    internal = aws_security_group.internal.id\n  }\n}\n</code></pre> <p>Include: Connection info, integration points, automation helpers Exclude: Internal implementation details, rarely used attributes Request Pattern: Users can request additional outputs via PR</p>"},{"location":"modules/DESIGN_STANDARDS.html#breaking-changes-prevention","title":"Breaking Changes Prevention","text":""},{"location":"modules/DESIGN_STANDARDS.html#critical-rules","title":"Critical Rules","text":"<ul> <li>NEVER change logical names without <code>moved</code> blocks</li> <li>NEVER change variable names in minor/patch versions</li> <li>ALWAYS use major version bumps for breaking changes</li> <li>ALWAYS test migration paths with real state files</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#safe-patterns","title":"Safe Patterns","text":"<pre><code># \u2705 SAFE - Adding resources, optional variables with defaults, new outputs\nresource \"aws_s3_bucket\" \"new_feature\" { }\n\nvariable \"new_option\" {\n  type    = bool\n  default = false  # Required default\n}\n\noutput \"new_info\" {\n  value = aws_s3_bucket.new_feature.id\n}\n</code></pre>"},{"location":"modules/DESIGN_STANDARDS.html#implementation-checklist","title":"Implementation Checklist","text":""},{"location":"modules/DESIGN_STANDARDS.html#for-new-modules_1","title":"For New Modules","text":"<ul> <li>[ ] Use 3-tier architecture variables</li> <li>[ ] Implement standardized logical names</li> <li>[ ] Use random IDs for predictable naming</li> <li>[ ] Create private DNS zones automatically</li> <li>[ ] Implement security group strategy (no 0.0.0.0/0 ingress)</li> <li>[ ] Add comprehensive examples with versions.tf</li> <li>[ ] Create tests with setup/ directory</li> <li>[ ] Document architecture and usage patterns</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#for-existing-modules_1","title":"For Existing Modules","text":"<ul> <li>[ ] Plan breaking changes for major versions only</li> <li>[ ] Add <code>moved</code> blocks for renamed resources</li> <li>[ ] Update variable naming to match standards</li> <li>[ ] Test upgrade paths with real state files</li> <li>[ ] Create migration documentation</li> </ul>"},{"location":"modules/DESIGN_STANDARDS.html#building-great-modules-together","title":"Building Great Modules Together","text":"<p>These standards represent our collective wisdom from building production game development infrastructure. By following these patterns, you're contributing to a toolkit that:</p> <ul> <li>Empowers game developers to focus on creating amazing games instead of wrestling with infrastructure</li> <li>Reduces cognitive load through consistent, predictable interfaces</li> <li>Scales with teams from indie studios to AAA publishers</li> <li>Evolves safely with backward compatibility and clear migration paths</li> </ul> <p>Every module you build following these standards makes the entire ecosystem stronger. Thank you for being part of this journey!</p> <p>Questions or Ideas? Open an issue or discussion - we love hearing from the community and these standards improve through your feedback.</p>"},{"location":"modules/jenkins/index.html","title":"Jenkins","text":""},{"location":"modules/jenkins/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws ~&gt; 6.6 random ~&gt; 3.7"},{"location":"modules/jenkins/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 random ~&gt; 3.7"},{"location":"modules/jenkins/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/jenkins/index.html#resources","title":"Resources","text":"Name Type aws_autoscaling_group.jenkins_build_farm_asg resource aws_cloudwatch_log_group.jenkins_service_log_group resource aws_ecs_cluster.jenkins_cluster resource aws_ecs_cluster_capacity_providers.jenkins_cluster_fargate_rpvodiers resource aws_ecs_service.jenkins_service resource aws_ecs_task_definition.jenkins_task_definition resource aws_efs_access_point.jenkins_efs_access_point resource aws_efs_backup_policy.policy resource aws_efs_file_system.jenkins_efs_file_system resource aws_efs_mount_target.jenkins_efs_mount_target resource aws_fsx_openzfs_file_system.jenkins_build_farm_fsxz_file_system resource aws_fsx_openzfs_volume.jenkins_build_farm_fsxz_volume resource aws_iam_instance_profile.build_farm_instance_profile resource aws_iam_policy.build_farm_fsxz_policy resource aws_iam_policy.build_farm_s3_policy resource aws_iam_policy.ec2_fleet_plugin_policy resource aws_iam_policy.jenkins_default_policy resource aws_iam_role.build_farm_role resource aws_iam_role.jenkins_default_role resource aws_iam_role.jenkins_task_execution_role resource aws_iam_role_policy_attachment.build_farm_role_fsxz_attachment resource aws_iam_role_policy_attachment.build_farm_role_s3_attachment resource aws_iam_role_policy_attachment.default_role resource aws_iam_role_policy_attachment.ec2_fleet_plugin_policy_attachment resource aws_iam_role_policy_attachment.task_execution resource aws_launch_template.jenkins_build_farm_launch_template resource aws_lb.jenkins_alb resource aws_lb_listener.jenkins_alb_https_listener resource aws_lb_target_group.jenkins_alb_target_group resource aws_s3_bucket.artifact_buckets resource aws_s3_bucket.jenkins_alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_s3_bucket_public_access_block.artifacts_bucket_public_block resource aws_s3_bucket_versioning.artifact_bucket_versioning resource aws_security_group.jenkins_alb_sg resource aws_security_group.jenkins_build_farm_sg resource aws_security_group.jenkins_build_storage_sg resource aws_security_group.jenkins_efs_security_group resource aws_security_group.jenkins_service_sg resource aws_vpc_security_group_egress_rule.jenkins_alb_outbound_service resource aws_vpc_security_group_egress_rule.jenkins_build_farm_outbound_ipv4 resource aws_vpc_security_group_egress_rule.jenkins_build_farm_outbound_ipv6 resource aws_vpc_security_group_egress_rule.jenkins_service_outbound_ipv4 resource aws_vpc_security_group_egress_rule.jenkins_service_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.jenkins_build_farm_inbound_ssh_service resource aws_vpc_security_group_ingress_rule.jenkins_build_vpc_all_traffic resource aws_vpc_security_group_ingress_rule.jenkins_efs_inbound_service resource aws_vpc_security_group_ingress_rule.jenkins_service_inbound_alb resource random_string.artifact_buckets resource random_string.build_farm resource random_string.fsxz resource random_string.jenkins resource random_string.jenkins_alb_access_logs_bucket_suffix resource aws_caller_identity.current data source aws_ecs_cluster.jenkins_cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.build_farm_fsxz_policy data source aws_iam_policy_document.build_farm_s3_policy data source aws_iam_policy_document.ec2_fleet_plugin_policy data source aws_iam_policy_document.ec2_trust_relationship data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.jenkins_default_policy data source aws_region.current data source aws_vpc.build_farm_vpc data source"},{"location":"modules/jenkins/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required build_farm_subnets The subnets to deploy the build farms into. <code>list(string)</code> n/a yes jenkins_alb_subnets A list of subnet ids to deploy the Jenkins load balancer into. Public subnets are recommended. <code>list(string)</code> n/a yes jenkins_service_subnets A list of subnets to deploy the Jenkins service into. Private subnets are recommended. <code>list(string)</code> n/a yes vpc_id The ID of the existing VPC you would like to deploy the Jenkins service and build farms into. <code>string</code> n/a yes artifact_buckets List of Amazon S3 buckets you wish to create to store build farm artifacts. <pre>map(    object({      name                 = string      enable_force_destroy = optional(bool, true)      enable_versioning    = optional(bool, true)      tags                 = optional(map(string), {})    })  )</pre> <code>null</code> no build_farm_compute Each object in this map corresponds to an ASG used by Jenkins as build agents. <pre>map(object(    {      ami = string      #TODO: Support mixed instances / spot with custom policies      instance_type     = string      ebs_optimized     = optional(bool, true)      enable_monitoring = optional(bool, true)    }  ))</pre> <code>{}</code> no build_farm_fsx_openzfs_storage Each object in this map corresponds to an FSx OpenZFS file system used by the Jenkins build agents. <pre>map(object(    {      storage_capacity    = number      throughput_capacity = number      storage_type        = optional(string, \"SSD\") # \"SSD\", \"HDD\"      deployment_type     = optional(string, \"SINGLE_AZ_1\")      route_table_ids     = optional(list(string), null)      tags                = optional(map(string), null)    }  ))</pre> <code>{}</code> no certificate_arn The TLS certificate ARN for the Jenkins service load balancer. <code>string</code> <code>null</code> no cluster_name The ARN of the cluster to deploy the Jenkins service into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no container_cpu The CPU allotment for the Jenkins container. <code>number</code> <code>1024</code> no container_memory The memory allotment for the Jenkins container. <code>number</code> <code>4096</code> no container_name The name of the Jenkins service container. <code>string</code> <code>\"jenkins-container\"</code> no container_port The container port used by the Jenkins service container. <code>number</code> <code>8080</code> no create_application_load_balancer Controls creation of an application load balancer within the module. Defaults to true. <code>bool</code> <code>true</code> no create_ec2_fleet_plugin_policy Optional creation of IAM Policy required for Jenkins EC2 Fleet plugin. Default is set to false. <code>bool</code> <code>false</code> no create_jenkins_default_policy Optional creation of Jenkins Default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_jenkins_default_role Optional creation of Jenkins Default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_jenkins_role ARN of the custom IAM Role you wish to use with Jenkins. <code>string</code> <code>null</code> no debug This value disables certain protections to accelerate testing (note that by enabling this variable, data will not be saved between destroys) <code>bool</code> <code>false</code> no enable_default_efs_backup_plan This flag controls EFS backups for the Jenkins module. Default is set to true. <code>bool</code> <code>true</code> no enable_jenkins_alb_access_logs Enables access logging for the Jenkins ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_jenkins_alb_deletion_protection Enables deletion protection for the Jenkins ALB. Defaults to true. <code>bool</code> <code>true</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_artifact_buckets List of ARNs of the S3 buckets used to store artifacts created by the build farm. <code>list(string)</code> <code>[]</code> no existing_security_groups A list of existing security group IDs to attach to the Jenkins service load balancer. <code>list(string)</code> <code>null</code> no internal Set this flag to true if you do not want the Jenkins service load balancer to have a public IP. <code>bool</code> <code>false</code> no jenkins_agent_secret_arns A list of secretmanager ARNs (wildcards allowed) that contain any secrets which need to be accessed by the Jenkins service. <code>list(string)</code> <code>null</code> no jenkins_alb_access_logs_bucket ID of the S3 bucket for Jenkins ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no jenkins_alb_access_logs_prefix Log prefix for Jenkins ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no jenkins_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Jenkins. <code>string</code> <code>365</code> no jenkins_efs_performance_mode The performance mode of the EFS file system used by the Jenkins service. Defaults to general purpose. <code>string</code> <code>\"generalPurpose\"</code> no jenkins_efs_throughput_mode The throughput mode of the EFS file system used by the Jenkins service. Defaults to bursting. <code>string</code> <code>\"bursting\"</code> no jenkins_service_desired_container_count The desired number of containers running the Jenkins service. <code>number</code> <code>1</code> no name The name attached to Jenkins module resources. <code>string</code> <code>\"jenkins\"</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"iac-management\": \"CGD-Toolkit\",  \"iac-module\": \"Jenkins\",  \"iac-provider\": \"Terraform\"}</pre> no"},{"location":"modules/jenkins/index.html#outputs","title":"Outputs","text":"Name Description alb_security_group_id Security group associated with the Jenkins load balancer build_farm_security_group_id Security group associated with the build farm autoscaling groups jenkins_alb_dns_name The DNS name of the Jenkins application load balancer. jenkins_alb_zone_id The zone ID of the Jenkins ALB. service_security_group_id Security group associated with the ECS service hosting jenkins service_target_group_arn The ARN of the Jenkins service target group"},{"location":"modules/perforce/index.html","title":"Perforce on AWS Terraform Module","text":"<p>For a video walkthrough demonstrating how to use this module, see this YouTube Video:</p> <p></p>"},{"location":"modules/perforce/index.html#features","title":"Features","text":"<ul> <li>Dynamic creation and configuration of P4 Server (formerly Helix Core)</li> <li>Dynamic creation and configuration   of P4 Code Review (formerly Helix Swarm)</li> <li>Dynamic creation and configuration   of P4Auth (formerly Helix Authentication Service)</li> </ul>"},{"location":"modules/perforce/index.html#architecture","title":"Architecture","text":""},{"location":"modules/perforce/index.html#full-example-using-aws-route53-public-hosted-zone","title":"Full example using AWS Route53 Public Hosted Zone","text":""},{"location":"modules/perforce/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Existing DNS Configured</li> <li>To use this module, you must have an existing domain and related DNS configuration. The example at       <code>/examples/create-resources-complete</code> demonstrates how to provision resources while using Amazon Route53 (       recommended) as the DNS provider. This will make deployment and management easier.</li> <li>You may optionally use a 3rd party DNS provider, however you must create records in your DNS provider to route to       the endpoints that you will create for each component when using the module (e.g. <code>perforce.example.com</code>,       <code>review.perforce.example.com</code>, <code>auth.perforce.example.com</code>). The module has variables that you can use to       customize the subdomains for the services (P4 Server, P4 Code Review, P4Auth), however if not set, the defaults       mentioned above will be used. Ensure you create these records to allow users to connect to the services once       provisioned in AWS.</li> <li>Note: When using either of the two options mentioned above, by default the module will create a Route53       Private Hosted Zone. This is used for internal communication and routing of traffic between P4 Server, P4 Code       Review, and P4Auth.</li> <li>SSL TLS Certificate</li> <li>You must have an existing SSL/TLS certificate, or create one during deployment alongside the other resources the       module will create. This is used to provide secure connectivity to the Perforce resources that will be running in       AWS. The certificate will be used by the Application Load Balancer (ALB) that the module will deploy for you. If       using Amazon Route53, see the example at <code>/examples/create-resources-complete</code> to see how to create the related       certificate in Amazon Certificate Manager (ACM). Using a Route53 as the DNS provider makes this process a bit       easier, as ACM can automatically create the required CNAME records needed for DNS validation (a process required       to verify DNS ownership) if you are also using Amazon Route53.</li> <li> <p>If using an 3rd party DNS provider, you must add these CNAME records manually (in addition to the other records       mentioned above for general DNS purposes). If you would prefer to use a 3rd party to create the SSL/TLS       certificate, the module allows you to import this into ACM to be used for the other components that will be       deployed (such as the internal ALB). You may also use Email validation to validate DNS ownership.</p> </li> <li> <p>Existing Perforce Amazon Machine Images (AMIs)</p> </li> <li>P4 Server AMI: As mentioned in the architecture, an Amazon EC2 instance is used for the P4 Server, and this       instance must be provisioned using an AMI that is configured for Perforce. To expedite this process, we have       sample HashiCorp Packer templates provided in       the AWS Cloud Game Development Toolkit repository       that you can use to create a Perforce AMI in your AWS Account. Note: You must also reference the       <code>p4_configure.sh</code> and <code>p4_setup.sh</code> files that are in this directory, as these are used to configure the P4 Commit       Server. These are already referenced in the <code>perforce_arm64.pkr.hcl</code> and <code>perforce_x86.pkr.hcl</code> packer templates       that are available for use.</li> <li>P4 Code Review AMI: If deploying P4 Code Review (Helix Swarm), you must also build an AMI using the Packer       template at assets/packer/perforce/p4-code-review.       See the P4 Code Review Packer README       for detailed build instructions.</li> </ul>"},{"location":"modules/perforce/index.html#examples","title":"Examples","text":"<p>For example configurations, please see the examples.</p>"},{"location":"modules/perforce/index.html#deployment-instructions","title":"Deployment Instructions","text":"<ol> <li>Create the required AMIs in your AWS account using the supplied Packer templates:</li> <li>P4 Server AMI (required): Use the templates in <code>assets/packer/perforce/p4-server/</code>. Choose the template that      aligns with the architecture type (e.g. arm64) of the EC2 instance you wish to create.</li> <li>P4 Code Review AMI (if deploying P4 Code Review): Use the template in <code>assets/packer/perforce/p4-code-review/</code>.</li> </ol> <p>On the Terraform side, you may set the architecture using the <code>instance_architecture</code> variable. Ensure your    <code>instance_type</code> is supported for your desired <code>instance_architecture</code>. For a full list of this mapping, see    the AWS Docs for EC2 Naming Conventions.    You can also use the interactive chart on Instances by Vantage.</p> <p>IMPORTANT: By default, the module will create compute resources with <code>x86_64</code> architecture. Ensure you use this corresponding Packer template unless you set the <code>instance_architecture</code> variable to <code>arm64</code> or the deployment will fail. Also, unless explicitly set, the Packer templates are configured to build the AMI in whichever AWS region your current credentials are set to (e.g. <code>us-east-1</code>) which will also be the same AWS region your Terraform resources are deployed to unless you explicitly set this. Ensure the AMI is available in the AWS Region you will use the module to deploy resources into.</p> <p>To deploy the template (<code>x86_64</code>) with Packer, do the following (while in the <code>/assets/perforce/p4-server directory</code>)</p> <pre><code>packer init perforce_x86.pkr.hcl\n</code></pre> <pre><code>packer validate perforce_x86.pkr.hcl\n</code></pre> <pre><code>packer build perforce_x86.pkr.hcl\n</code></pre> <ol> <li> <p>Reference your existing fully qualified domain name within each related Perforce service you would like to    provision (e.g. <code>p4_server_config</code>, <code>p4_auth_config</code>, <code>p4_code_review_config</code>) using the    <code>fully_qualified_domain_name</code> variable. We recommend abstracting this t a local value such as    <code>local.fully_qualified_domain_name</code> to ensure this value is consistent across the modules. The module will    automatically configure Perforce using default subdomains of <code>perforce.&lt;your-domain-name&gt;</code> for P4 Server,    <code>auth.perforce.&lt;your-domain-name</code> for P4 Auth, and <code>review.perforce.&lt;your-domain-name</code> for P4 Code Review. You will    also need to create DNS records that will route traffic destined for these domains in the following manner:</p> <ul> <li>Traffic destined for P4 Server will need to route to the Elastic IP (EIP) that is associated with the P4   Server EC2 Instance. By default, this will be using a subdomain named <code>perforce</code>. In your DNS provider, create an   A record named <code>perforce.&lt;your-domain-name&gt;</code> and have it route traffic to the EIP. This value is available as a   Terraform output for your convenience.</li> <li>Traffic destined for <code>*.perforce.&lt;your-domain-name&gt;</code> will need to route to the DNS name of the Network Load   Balancer (NLB) that the module creates. In your DNS provider, create a CNAME record that routes traffic to NLB.   This value is available as a Terraform output for your convenience.</li> <li>Note: If using Amazon Route53 as your DNS provider, the example at  <code>/examples/create-resources-complete</code>   shows you have to leverage Terraform to automatically create these records in an existing Route53 Public Hosted   Zone, as well as how to create the certificate in Amazon Certificate Manager (ACM).</li> </ul> </li> <li> <p>Make any other modifications as desired (such as referencing existing VPC resources) and run <code>terraform init</code> to    initialize Terraform in the current working directory, <code>terraform plan</code> to create and validate the execution plan of    the resources that will be created, and finally <code>terraform apply</code> to create the resources in your AWS Account.</p> </li> <li>Once the resources have finished provisioning successfully, you will need to modify your inbound Security Group Rules    on the P4 Commit Server Instance to allow TCP traffic from your public IP on port 1666 (the perforce default port).    This is necessary to allow your local machine(s) to connect to the P4 Commit Server. Optionally, you can pass in an entire security group to also add to the resource. The complete example demonstrates how to use the <code>existing_security_groups</code> variable to accomplish this.<ul> <li>Note: You may use other means to allow traffic to reach this EC2 Instance (Customer-managed prefix list, VPN   to the VPC that the instance is running in, etc.) but regardless, it is essential that you have the security group   rules set configured correctly to allow access.</li> </ul> </li> <li>Next, modify your inbound Security Group rules for the Perforce Network Load Balancer (NLB) to allow traffic from    HTTPS (port 443) from your public IP address/ This is to provide access to the P4 Code Review and P4Auth services    that are running behind the Application Load Balancer (ALB). Optionally, you can pass in an entire security group to also add to the resource. The complete example demonstrates how to use the <code>existing_security_groups</code> variable to accomplish this.<ul> <li>Note: You may use other means to allow traffic to reach this the Network Load Balancer (Customer-managed   prefix list, VPN to the VPC that the instance is running in, etc.) but regardless, it is essential that you have   the security group rules set configured correctly to allow access.</li> <li>IMPORTANT: Ensure your networking configuration is correct, especially in terms of any public or private   subnets that you reference. This is very important for the internal routing between the P4 resources, as well as   the related Security Groups. Failure to set these correctly may cause a variety of connectivity issues such as web   pages not loading, NLB health checks failing, etc.</li> </ul> </li> <li>Use the provided Terraform outputs to quickly find the URL for P4Auth, P4 Code Review. If you haven't modified the    default values, relevant values for the P4 Server default username/password, and the P4 Code Review default    username/password were created for you and are stored in AWS Secrets Manager.</li> <li>In P4V, use the url of <code>ssl:&lt;your-supplied-root-domain&gt;:1666</code> and the username and password stored in AWS Secrets    Manager to gain access to the commit server.</li> <li>At this point, you should be able to access your P4 Commit Server (P4), and visit the URLs for P4 Code Review (P4    Code Review) and P4Auth (P4Auth).</li> </ol>"},{"location":"modules/perforce/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws ~&gt; 6.6 awscc ~&gt; 1.51 local ~&gt; 2.5 null ~&gt; 3.2 random ~&gt; 3.7"},{"location":"modules/perforce/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 null ~&gt; 3.2 random ~&gt; 3.7"},{"location":"modules/perforce/index.html#modules","title":"Modules","text":"Name Source Version p4_auth ./modules/p4-auth n/a p4_code_review ./modules/p4-code-review n/a p4_server ./modules/p4-server n/a"},{"location":"modules/perforce/index.html#resources","title":"Resources","text":"Name Type aws_ecs_cluster.perforce_web_services_cluster resource aws_ecs_cluster_capacity_providers.providers resource aws_lb.perforce resource aws_lb.perforce_web_services resource aws_lb_listener.perforce resource aws_lb_listener.perforce_web_services resource aws_lb_listener.perforce_web_services_http_listener resource aws_lb_listener_rule.p4_code_review resource aws_lb_listener_rule.p4_code_review_http resource aws_lb_listener_rule.perforce_p4_auth resource aws_lb_listener_rule.perforce_p4_auth_http resource aws_lb_target_group.perforce resource aws_lb_target_group_attachment.perforce resource aws_route53_record.internal_p4_server resource aws_route53_record.internal_perforce_web_services resource aws_route53_zone.perforce_private_hosted_zone resource aws_s3_bucket.shared_lb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.shared_access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.shared_lb_access_logs_bucket_policy resource aws_security_group.perforce_network_load_balancer resource aws_security_group.perforce_web_services_alb resource aws_vpc_security_group_egress_rule.p4_code_review_outbound_to_p4_server resource aws_vpc_security_group_egress_rule.p4_server_outbound_to_perforce_web_services_alb_https resource aws_vpc_security_group_egress_rule.perforce_alb_outbound_to_p4_auth resource aws_vpc_security_group_egress_rule.perforce_alb_outbound_to_p4_code_review resource aws_vpc_security_group_egress_rule.perforce_nlb_outbound_to_perforce_web_services_alb resource aws_vpc_security_group_ingress_rule.p4_auth_inbound_from_perforce_web_services_alb resource aws_vpc_security_group_ingress_rule.p4_code_review_inbound_from_perforce_web_services_alb resource aws_vpc_security_group_ingress_rule.p4_server_inbound_from_p4_code_review resource aws_vpc_security_group_ingress_rule.perforce_web_services_inbound_from_p4_server resource aws_vpc_security_group_ingress_rule.perforce_web_services_inbound_from_perforce_nlb resource null_resource.parent_module_certificate resource random_string.shared_lb_access_logs_bucket resource aws_elb_service_account.main data source aws_iam_policy_document.shared_lb_access_logs_bucket_lb_write data source"},{"location":"modules/perforce/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required vpc_id The VPC ID where the Perforce resources will be deployed. <code>string</code> n/a yes certificate_arn The ARN of the ACM certificate to be used with the HTTPS listener for the NLB. <code>string</code> <code>null</code> no create_default_sgs Whether to create default security groups for the Perforce resources. <code>bool</code> <code>true</code> no create_route53_private_hosted_zone Whether to create a private Route53 Hosted Zone for the Perforce resources. This private hosted zone is used for internal communication between the P4 Server, P4 Auth Service, and P4 Code Review Service. <code>bool</code> <code>true</code> no create_shared_application_load_balancer Whether to create a shared Application Load Balancer for the Perforce resources. <code>bool</code> <code>true</code> no create_shared_network_load_balancer Whether to create a shared Network Load Balancer for the Perforce resources. <code>bool</code> <code>true</code> no enable_shared_alb_deletion_protection Enables deletion protection for the shared Application Load Balancer for the Perforce resources. <code>bool</code> <code>false</code> no enable_shared_lb_access_logs Enables access logging for both the shared NLB and shared ALB. Defaults to false. <code>bool</code> <code>false</code> no existing_ecs_cluster_name The name of an existing ECS cluster to use for the Perforce server. If omitted a new cluster will be created. <code>string</code> <code>null</code> no existing_security_groups A list of existing security group IDs to attach to the shared network load balancer. <code>list(string)</code> <code>[]</code> no p4_auth_config # General    name: \"The string including in the naming of resources related to P4Auth. Default is 'p4-auth'.\"    project_prefix : \"The project prefix for the P4Auth service. Default is 'cgd'.\"    environment : \"The environment where the P4Auth service will be deployed. Default is 'dev'.\"    enable_web_based_administration: \"Whether to de enable web based administration. Default is 'true'.\"    debug : \"Whether to enable debug mode for the P4Auth service. Default is 'false'.\"    fully_qualified_domain_name : \"The FQDN for the P4Auth Service. This is used for the P4Auth's Perforce configuration.\"    # Compute    cluster_name : \"The name of the ECS cluster where the P4Auth service will be deployed. Cluster is not created if this variable is null.\"    container_name : \"The name of the P4Auth service container. Default is 'p4-auth-container'.\"    container_port : \"The port on which the P4Auth service will be listening. Default is '3000'.\"    container_cpu : \"The number of CPU units to reserve for the P4Auth service container. Default is '1024'.\"    container_memory : \"The number of CPU units to reserve for the P4Auth service container. Default is '4096'.\"    pd4_port : \"The full URL you will use to access the P4 Depot in clients such P4V and P4Admin. Note, this typically starts with 'ssl:' and ends with the default port of ':1666'.\"    # Storage &amp; Logging    cloudwatch_log_retention_in_days : \"The number of days to retain the P4Auth service logs in CloudWatch. Default is 365 days.\"    # Networking    create_defaults_sgs : \"Whether to create default security groups for the P4Auth service.\"    internal : \"Set this flag to true if you do not want the P4Auth service to have a public IP.\"    create_default_role : \"Whether to create the P4Auth default IAM Role. Default is set to true.\"    custom_role : \"ARN of a custom IAM Role you wish to use with P4Auth.\"    admin_username_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4Auth Administrator username.\"    admin_password_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4Auth Administrator password.\"    # - SCIM -    p4d_super_user_arn : \"If you would like to use SCIM to provision users and groups, you need to set this variable to the ARN of an AWS Secrets Manager secret containing the super user username for p4d.\"    p4d_super_user_password_arn : \"If you would like to use SCIM to provision users and groups, you need to set this variable to the ARN of an AWS Secrets Manager secret containing the super user password for p4d.\"    scim_bearer_token_arn : \"If you would like to use SCIM to provision users and groups, you need to set this variable to the ARN of an AWS Secrets Manager secret containing the bearer token.\"    extra_env : \"Extra configuration environment variables to set on the p4 auth svc container.\" <pre>object({    # - General -    name                            = optional(string, \"p4-auth\")    project_prefix                  = optional(string, \"cgd\")    environment                     = optional(string, \"dev\")    enable_web_based_administration = optional(bool, true)    debug                           = optional(bool, false)    fully_qualified_domain_name     = string    # - Compute -    container_name   = optional(string, \"p4-auth-container\")    container_port   = optional(number, 3000)    container_cpu    = optional(number, 1024)    container_memory = optional(number, 4096)    p4d_port         = optional(string, null)    # - Storage &amp; Logging -    cloudwatch_log_retention_in_days = optional(number, 365)    # - Networking &amp; Security -    service_subnets          = optional(list(string), null)    create_default_sgs       = optional(bool, true)    existing_security_groups = optional(list(string), [])    internal                 = optional(bool, false)    certificate_arn           = optional(string, null)    create_default_role       = optional(bool, true)    custom_role               = optional(string, null)    admin_username_secret_arn = optional(string, null)    admin_password_secret_arn = optional(string, null)    # SCIM    p4d_super_user_arn          = optional(string, null)    p4d_super_user_password_arn = optional(string, null)    scim_bearer_token_arn       = optional(string, null)    extra_env                   = optional(map(string), null)  })</pre> <code>null</code> no p4_code_review_config # General    name: \"The string including in the naming of resources related to P4 Code Review. Default is 'p4-code-review'.\"    project_prefix : \"The project prefix for the P4 Code Review service. Default is 'cgd'.\"    environment : \"The environment where the P4 Code Review service will be deployed. Default is 'dev'.\"    fully_qualified_domain_name : \"The FQDN for the P4 Code Review Service. This is used for the P4 Code Review's Perforce configuration.\"    # Compute    application_port : \"The port on which the P4 Code Review service will be listening. Default is '80'.\"    instance_type : \"EC2 instance type for running P4 Code Review. Default is 'm5.large'.\"    ami_id : \"Optional AMI ID for P4 Code Review. If not provided, will use the latest Packer-built AMI.\"    p4d_port : \"The full URL you will use to access the P4 Depot in clients such P4V and P4Admin. Note, this typically starts with 'ssl:' and ends with the default port of ':1666'.\"    p4charset : \"The P4CHARSET environment variable to set for the P4 Code Review instance.\"    existing_redis_connection : \"The existing Redis connection for the P4 Code Review service.\"    # Storage &amp; Logging    cloudwatch_log_retention_in_days : \"The number of days to retain the P4 Code Review service logs in CloudWatch. Default is 365 days.\"    ebs_volume_size : \"Size in GB for the EBS volume that stores P4 Code Review data. Default is '20'.\"    ebs_volume_type : \"EBS volume type for P4 Code Review data storage. Default is 'gp3'.\"    ebs_volume_encrypted : \"Enable encryption for the EBS volume storing P4 Code Review data. Default is 'true'.\"    ebs_availability_zone : \"Availability zone for the EBS volume. Must match the EC2 instance AZ.\"    # Networking &amp; Security    create_default_sgs : \"Whether to create default security groups for the P4 Code Review service.\"    internal : \"Set this flag to true if you do not want the P4 Code Review service to have a public IP.\"    instance_subnet_id : \"The subnet ID where the EC2 instance will be launched. Should be a private subnet for security.\"    super_user_password_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4 Server super user password.\"    p4_code_review_user_password_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4 Code Review user's password.\"    p4_code_review_user_username_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4 Code Review user's username.\"    custom_config : \"JSON string with additional Swarm configuration to merge with the generated config.php. Use this for SSO/SAML setup, notifications, Jira integration, etc.\"    # Caching    elasticache_node_count : \"The number of Elasticache nodes to create for the P4 Code Review service. Default is '1'.\"    elasticache_node_type : \"The type of Elasticache node to create for the P4 Code Review service. Default is 'cache.t4g.micro'.\" <pre>object({    # General    name                        = optional(string, \"p4-code-review\")    project_prefix              = optional(string, \"cgd\")    environment                 = optional(string, \"dev\")    fully_qualified_domain_name = string    # Compute    application_port = optional(number, 80)    instance_type    = optional(string, \"m5.large\")    ami_id           = optional(string, null)    p4d_port         = optional(string, null)    p4charset        = optional(string, null)    existing_redis_connection = optional(object({      host = string      port = number    }), null)    # Storage &amp; Logging    cloudwatch_log_retention_in_days = optional(number, 365)    ebs_volume_size                  = optional(number, 20)    ebs_volume_type                  = optional(string, \"gp3\")    ebs_volume_encrypted             = optional(bool, true)    ebs_availability_zone            = optional(string, null)    # Networking &amp; Security    create_default_sgs       = optional(bool, true)    existing_security_groups = optional(list(string), [])    internal                 = optional(bool, false)    service_subnets          = optional(list(string), null)    instance_subnet_id       = string    super_user_password_secret_arn          = optional(string, null)    p4_code_review_user_password_secret_arn = optional(string, null)    p4_code_review_user_username_secret_arn = optional(string, null)    custom_config                           = optional(string, null)    # Caching    elasticache_node_count = optional(number, 1)    elasticache_node_type  = optional(string, \"cache.t4g.micro\")  })</pre> <code>null</code> no p4_server_config # - General -    name: \"The string including in the naming of resources related to P4 Server. Default is 'p4-server'\"    project_prefix: \"The project prefix for this workload. This is appended to the beginning of most resource names.\"    environment: \"The current environment (e.g. dev, prod, etc.)\"    auth_service_url: \"The URL for the P4Auth Service.\"    fully_qualified_domain_name = \"The FQDN for the P4 Server. This is used for the P4 Server's Perforce configuration.\"    # - Compute -    lookup_existing_ami : \"Whether to lookup the existing Perforce P4 Server AMI.\"    ami_prefix: \"The AMI prefix to use for the AMI that will be created for P4 Server.\"    instance_type: \"The instance type for Perforce P4 Server. Defaults to c6g.large.\"    instance_architecture: \"The architecture of the P4 Server instance. Allowed values are 'arm64' or 'x86_64'.\"    IMPORTANT: \"Ensure the instance family of the instance type you select supports the instance_architecture you select. For example, 'c6in' instance family only works for 'x86_64' architecture, not 'arm64'. For a full list of this mapping, see the AWS Docs for EC2 Naming Conventions: https://docs.aws.amazon.com/ec2/latest/instancetypes/instance-type-names.html\"    p4_server_type: \"The Perforce P4 Server server type. Valid values are 'p4d_commit' or 'p4d_replica'.\"    unicode: \"Whether to enable Unicode configuration for P4 Server the -xi flag for p4d. Set to true to enable Unicode support.\"    selinux: \"Whether to apply SELinux label updates for P4 Server. Don't enable this if SELinux is disabled on your target operating system.\"    case_sensitive: \"Whether or not the server should be case insensitive (Server will run '-C1' mode), or if the server will run with case sensitivity default of the underlying platform. False enables '-C1' mode. Default is set to true.\"    plaintext: \"Whether to enable plaintext authentication for P4 Server. This is not recommended for production environments unless you are using a load balancer for TLS termination. Default is set to false.\"    # - Storage -    storage_type: \"The type of backing store. Valid values are either 'EBS' or 'FSxN'\"    depot_volume_size: \"The size of the depot volume in GiB. Defaults to 128 GiB.\"    metadata_volume_size: \"The size of the metadata volume in GiB. Defaults to 32 GiB.\"    logs_volume_size: \"The size of the logs volume in GiB. Defaults to 32 GiB.\"    # - Networking &amp; Security -    instance_subnet_id: \"The subnet where the P4 Server instance will be deployed.\"    instance_private_ip: \"The private IP address to assign to the P4 Server.\"    create_default_sg : \"Whether to create a default security group for the P4 Server instance.\"    existing_security_groups: \"A list of existing security group IDs to attach to the P4 Server load balancer.\"    internal: \"Set this flag to true if you do not want the P4 Server instance to have a public IP.\"    admin_username: \"Username for the Perforce admin account. The 'super' service account is always created automatically for internal tooling. Default is 'perforce'.\"    admin_password_secret_arn: \"Optional ARN of existing Secrets Manager secret for admin password. If not provided, a password will be auto-generated.\"    create_default_role: \"Optional creation of P4 Server default IAM Role with SSM managed instance core policy attached. Default is set to true.\"    custom_role: \"ARN of a custom IAM Role you wish to use with P4 Server.\" <pre>object({    # General    name                        = optional(string, \"p4-server\")    project_prefix              = optional(string, \"cgd\")    environment                 = optional(string, \"dev\")    auth_service_url            = optional(string, null)    fully_qualified_domain_name = string    # Compute    lookup_existing_ami = optional(bool, true)    ami_prefix          = optional(string, \"p4_al2023\")    instance_type         = optional(string, \"c6i.large\")    instance_architecture = optional(string, \"x86_64\")    p4_server_type        = optional(string, null)    unicode        = optional(bool, false)    selinux        = optional(bool, false)    case_sensitive = optional(bool, true)    plaintext      = optional(bool, false)    # Storage    storage_type         = optional(string, \"EBS\")    depot_volume_size    = optional(number, 128)    metadata_volume_size = optional(number, 32)    logs_volume_size     = optional(number, 32)    # Networking &amp; Security    instance_subnet_id       = optional(string, null)    instance_private_ip      = optional(string, null)    create_default_sg        = optional(bool, true)    existing_security_groups = optional(list(string), [])    internal                 = optional(bool, false)    admin_username            = optional(string, \"perforce\")    admin_password_secret_arn = optional(string, null)    create_default_role = optional(bool, true)    custom_role         = optional(string, null)    # FSxN    fsxn_password                     = optional(string, null)    fsxn_filesystem_security_group_id = optional(string, null)    protocol                          = optional(string, null)    fsxn_region                       = optional(string, null)    fsxn_management_ip                = optional(string, null)    fsxn_svm_name                     = optional(string, null)    amazon_fsxn_svm_id                = optional(string, null)    fsxn_aws_profile                  = optional(string, null)  })</pre> <code>null</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no route53_private_hosted_zone_name The name of the private Route53 Hosted Zone for the Perforce resources. <code>string</code> <code>null</code> no s3_enable_force_destroy Enables force destroy for the S3 bucket for both the shared NLB and shared ALB access log storage. Defaults to true. <code>bool</code> <code>true</code> no shared_alb_access_logs_prefix Log prefix for shared ALB access logs. <code>string</code> <code>\"perforce-alb-\"</code> no shared_alb_subnets A list of subnets to attach to the shared application load balancer. <code>list(string)</code> <code>null</code> no shared_application_load_balancer_name The name of the shared Application Load Balancer for the Perforce resources. <code>string</code> <code>\"p4alb\"</code> no shared_ecs_cluster_name The name of the ECS cluster to use for the shared ECS Cluster. <code>string</code> <code>\"perforce-cluster\"</code> no shared_lb_access_logs_bucket ID of the S3 bucket for both the shared NLB and shared ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no shared_network_load_balancer_name The name of the shared Network Load Balancer for the Perforce resources. <code>string</code> <code>\"p4nlb\"</code> no shared_nlb_access_logs_prefix Log prefix for shared NLB access logs. <code>string</code> <code>\"perforce-nlb-\"</code> no shared_nlb_subnets A list of subnets to attach to the shared network load balancer. <code>list(string)</code> <code>null</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"terraform-aws-perforce\",  \"ModuleSource\": \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/perforce\",  \"RootModuleName\": \"-\"}</pre> no"},{"location":"modules/perforce/index.html#outputs","title":"Outputs","text":"Name Description p4_auth_alb_dns_name The DNS name of the P4Auth ALB. p4_auth_alb_security_group_id Security group associated with the P4Auth load balancer. p4_auth_alb_zone_id The hosted zone ID of the P4Auth ALB. p4_auth_perforce_cluster_name Name of the ECS cluster hosting P4Auth. p4_auth_service_security_group_id Security group associated with the ECS service running P4Auth. p4_auth_target_group_arn The service target group for the P4Auth. p4_code_review_alb_dns_name The DNS name of the P4 Code Review ALB. p4_code_review_alb_security_group_id Security group associated with the P4 Code Review load balancer. p4_code_review_alb_zone_id The hosted zone ID of the P4 Code Review ALB. p4_code_review_service_security_group_id Security group associated with P4 Code Review application. p4_code_review_target_group_arn The service target group for the P4 Code Review. p4_server_admin_password_secret_arn The ARN of the AWS Secrets Manager secret holding the admin account password. p4_server_admin_username_secret_arn The ARN of the AWS Secrets Manager secret holding the admin account username. p4_server_eip_id The ID of the Elastic IP associated with your P4 Server instance. p4_server_eip_public_ip The public IP of your P4 Server instance. p4_server_instance_id Instance ID for the P4 Server instance p4_server_lambda_link_name The name of the Lambda link for the P4 Server instance to use with FSxN. p4_server_private_ip Private IP for the P4 Server instance p4_server_security_group_id The default security group of your P4 Server instance. p4_server_super_password_secret_arn The ARN of the AWS Secrets Manager secret holding the service account (super) password. shared_application_load_balancer_arn The ARN of the shared application load balancer. shared_network_load_balancer_arn The ARN of the shared network load balancer."},{"location":"modules/perforce/examples/create-resources-complete/index.html","title":"Create Resources Complete (with Route53)","text":"<p>This example deploys P4 Server (formerly Helix Core), P4 Code Review (formerly Helix Swarm), and the P4 Auth Service (formerly the Helix Auth Service) using Amazon Route53 as the DNS provider.</p>"},{"location":"modules/perforce/examples/create-resources-complete/index.html#architecture","title":"Architecture","text":""},{"location":"modules/perforce/examples/create-resources-complete/index.html#important","title":"Important","text":"<p>This example creates DNS records in an existing Rout53 Public Hosted Zone and as well as an ACM Certificate. This certificate needs to be validated, which is not a fixed amount of time. During a standard deployment this is a non-issue since multiple dependent resources take longer to deploy than the certificate takes to validate. However, if you change the name of your domain where referenced in the ACM certificate after the initial apply, you may encounter the following error:</p> <pre><code>\u2502 Error : modifying ELBv2 Listener (arn : aws :elasticloadbalancing : us-east-1 : xx : listener/app/cgd-perforce-shared-alb/xx) : operation error Elastic Load Balancing v2 : ModifyListener, https response error StatusCode : 400, RequestID : xx-xx-xx-xx-xx, api error UnsupportedCertificate : The certificate 'arn:aws:acm:us-east-1:x:certificate/xx-xx-xx-xx-xx' must have a fully-qualified domain name, a supported signature, and a supported key size.\n\u2502\n\u2502   with module.terraform-aws-perforce.aws_lb_listener.perforce_web_services[0\n],\n\u2502   on../../lb.tf line 161, in resource \"aws_lb_listener\" \"perforce_web_services\" :\n\u2502  161 : resource \"aws_lb_listener\" \"perforce_web_services\" {\n</code></pre> <p>If this occurs, it is because Terraform is attempting to attach the certificate to the ALB listener before it has finished validation. Wait a few minutes and retry <code>terraform apply</code>.</p>"},{"location":"modules/perforce/examples/create-resources-complete/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws ~&gt; 6.6 awscc ~&gt; 1.51 http ~&gt; 3.5 netapp-ontap ~&gt; 2.3 random ~&gt; 3.7"},{"location":"modules/perforce/examples/create-resources-complete/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 http ~&gt; 3.5"},{"location":"modules/perforce/examples/create-resources-complete/index.html#modules","title":"Modules","text":"Name Source Version terraform-aws-perforce ../../ n/a"},{"location":"modules/perforce/examples/create-resources-complete/index.html#resources","title":"Resources","text":"Name Type aws_acm_certificate.perforce resource aws_acm_certificate_validation.perforce resource aws_default_security_group.default resource aws_eip.nat_gateway_eip resource aws_internet_gateway.igw resource aws_nat_gateway.nat_gateway resource aws_route.private_rt_nat_gateway resource aws_route53_record.external_perforce_p4_server resource aws_route53_record.external_perforce_web_services resource aws_route53_record.perforce_cert resource aws_route_table.private_rt resource aws_route_table.public_rt resource aws_route_table_association.private_rt_asso resource aws_route_table_association.public_rt_asso resource aws_security_group.allow_my_ip resource aws_subnet.private_subnets resource aws_subnet.public_subnets resource aws_vpc.perforce_vpc resource aws_vpc_security_group_ingress_rule.allow_http resource aws_vpc_security_group_ingress_rule.allow_https resource aws_vpc_security_group_ingress_rule.allow_icmp resource aws_vpc_security_group_ingress_rule.allow_perforce resource aws_availability_zones.available data source aws_lb.shared_services_nlb data source aws_route53_zone.root data source http_http.my_ip data source"},{"location":"modules/perforce/examples/create-resources-complete/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required route53_public_hosted_zone_name The name of your existing Route53 Public Hosted Zone. This is required to create the ACM certificate and Route53 records. <code>string</code> n/a yes"},{"location":"modules/perforce/examples/create-resources-complete/index.html#outputs","title":"Outputs","text":"Name Description p4_auth_admin_url The URL for the P4Auth service admin page. p4_code_review_url The URL for the P4 Code Review service. p4_server_connection_string The connection string for the P4 Server. Set your P4PORT environment variable to this value."},{"location":"modules/perforce/modules/p4-auth/index.html","title":"P4Auth Submodule","text":"<p>P4Auth enables you to integrate certain Perforce products with your organization's Identity Provider (IdP).</p> <p>This module creates the following resources:</p> <ul> <li>An Elastic Container Service (ECS) cluster backed by AWS Fargate. This can also be created externally and passed in via the <code>cluster_name</code> variable.</li> <li>An ECS service running the latest P4Auth container (perforce/helix-auth-svc) available.</li> <li>AWS Secrets Manager secrets for an administrative user that has access to the Helix Authentication Service's web UI. These credentials are needed to configure external identity providers through the UI.</li> <li>Supporting resources such as Cloudwatch log groups, IAM roles, and security groups.</li> </ul>"},{"location":"modules/perforce/modules/p4-auth/index.html#architecture","title":"Architecture","text":""},{"location":"modules/perforce/modules/p4-auth/index.html#prerequisites","title":"Prerequisites","text":"<p>P4Admin can be configured at deployment time or through the web UI following deployment. If you opt to configure P4Admin through the web-based UI you will need to create an administrative user for initial login. You can either create and upload these credentials to AWS Secrets Manager yourself, or you opt to have the module create these credentials for you. The parent module does this by default.</p> <p>Should you choose to create this administrative user yourself you will need to specify the ARN for the username and password as module variables. You can create the secret using the AWS CLI:</p> <pre><code>aws secretsmanager create-secret \\\n    --name P4AuthAdmin \\\n    --description \"P4Auth Admin\" \\\n    --secret-string \"{\\\"username\\\":\\\"admin\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\"\n</code></pre> <p>And then provide the relevant ARNs as variables when you define the Helix Authentication module in your Terraform configurations:</p> <pre><code>module \"p4_auth\" {\n    source = \"modules/perforce/modules/p4-auth\"\n    ...\n    admin_username_secret_arn = \"arn:aws:secretsmanager:&lt;your-aws-region&gt;:&lt;your-aws-account-id&gt;:secret:P4AuthAdmin-a1b2c3:username::\"\n    admin_password_secret_arn = \"arn:aws:secretsmanager:&lt;your-aws-region&gt;:&lt;your-aws-account-id&gt;:secret:P4AuthAdmin-a1b2c3:password::\"\n}\n</code></pre> <p>If you do not provide these the module will create a random Super User and create the secret for you. The ARN of this secret is then available as an output to be referenced elsewhere, and can be accessed from the AWS Secrets Manager console.</p>"},{"location":"modules/perforce/modules/p4-auth/index.html#enabling-system-for-cross-domain-identity-management-scim","title":"Enabling System for Cross-domain Identity Management (SCIM)","text":"<p>P4Auth supports System for Cross-domain Identity Management (SCIM) for provisioning users and groups from an identity management system.</p> <p>To enable SCIM in the Terraform module, you need to:</p> <ol> <li>Set up a secret containing your SCIM Bearer Token in AWS Secrets Manager.</li> <li>Provide the appropriate <code>scim_bearer_token_arn</code>, <code>p4d_super_user_arn</code>, <code>p4d_super_user_password_arn</code> and <code>p4d_port</code> variables to the module.</li> <li>Set up connectivity between P4 Server and P4Auth. The parent module does this for you.</li> </ol> <p>Once this is set up, you can verify that SCIM works by making the following call to create a user:</p> <pre><code>curl -X POST -H 'Authorization: Bearer &lt;base64-encoded bearer token&gt;' \\\n  -H \"Content-Type: application/scim+json\" \\\n  -d '{\n    \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"],\n    \"userName\": \"example1\",\n    \"externalId\": \"example1\",\n    \"name\": {\n      \"formatted\": \"Example 1\",\n      \"familyName\": \"Example\",\n      \"givenName\": \"One\"\n    }\n  }' \\ -v -v -v https://&lt;p4auth domain name&gt;/scim/v2/Users\n</code></pre>"},{"location":"modules/perforce/modules/p4-auth/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws ~&gt; 6.6 awscc ~&gt; 1.51 random ~&gt; 3.7"},{"location":"modules/perforce/modules/p4-auth/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 awscc ~&gt; 1.51 random ~&gt; 3.7"},{"location":"modules/perforce/modules/p4-auth/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/modules/p4-auth/index.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.log_group resource aws_ecs_cluster.cluster resource aws_ecs_cluster_capacity_providers.cluster_fargate_providers resource aws_ecs_service.service resource aws_ecs_task_definition.task_definition resource aws_iam_policy.default_policy resource aws_iam_policy.scim_secrets_manager_policy resource aws_iam_policy.secrets_manager_policy resource aws_iam_role.default_role resource aws_iam_role.task_execution_role resource aws_iam_role_policy_attachment.default_role resource aws_iam_role_policy_attachment.task_execution_role_ecs resource aws_iam_role_policy_attachment.task_execution_role_scim_secrets_manager resource aws_iam_role_policy_attachment.task_execution_role_secrets_manager resource aws_lb.alb resource aws_lb_listener.alb_https_listener resource aws_lb_target_group.alb_target_group resource aws_s3_bucket.alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_security_group.alb resource aws_security_group.ecs_service resource aws_vpc_security_group_egress_rule.alb_outbound_to_ecs_service resource aws_vpc_security_group_egress_rule.ecs_service_outbound_to_internet_ipv4 resource aws_vpc_security_group_egress_rule.ecs_service_outbound_to_internet_ipv6 resource aws_vpc_security_group_ingress_rule.ecs_service_inbound_from_alb resource awscc_secretsmanager_secret.admin_password resource awscc_secretsmanager_secret.admin_username resource random_string.alb_access_logs_bucket_suffix resource random_string.p4_auth resource aws_ecs_cluster.cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.default_policy data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.helix_authentication_service_scim_secrets_manager_policy data source aws_iam_policy_document.secrets_manager_policy data source aws_region.current data source"},{"location":"modules/perforce/modules/p4-auth/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required subnets A list of subnets to deploy the P4Auth ECS Service into. Private subnets are recommended. <code>list(string)</code> n/a yes vpc_id The ID of the existing VPC you would like to deploy P4Auth into. <code>string</code> n/a yes admin_password_secret_arn Optionally provide the ARN of an AWS Secret for the P4Auth Administrator password. <code>string</code> <code>null</code> no admin_username_secret_arn Optionally provide the ARN of an AWS Secret for the P4Auth Administrator username. <code>string</code> <code>null</code> no alb_access_logs_bucket ID of the S3 bucket for P4Auth ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no alb_access_logs_prefix Log prefix for P4Auth ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no alb_subnets A list of subnets to deploy the load balancer into. Public subnets are recommended. <code>list(string)</code> <code>[]</code> no application_load_balancer_name The name of the P4Auth ALB. Defaults to the project prefix and module name. <code>string</code> <code>null</code> no certificate_arn The TLS certificate ARN for the P4Auth load balancer. <code>string</code> <code>null</code> no cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for P4Auth. <code>string</code> <code>365</code> no cluster_name The name of the ECS cluster to deploy the P4Auth into. Cluster is not created if this variable is null. <code>string</code> <code>null</code> no container_cpu The CPU allotment for the P4Auth container. <code>number</code> <code>1024</code> no container_memory The memory allotment for the P4Auth container. <code>number</code> <code>4096</code> no container_name The name of the P4Auth container. <code>string</code> <code>\"p4-auth-container\"</code> no container_port The container port that P4Auth runs on. <code>number</code> <code>3000</code> no create_application_load_balancer This flag controls the creation of an application load balancer as part of the module. <code>bool</code> <code>true</code> no create_default_role Optional creation of P4Auth default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_role ARN of the custom IAM Role you wish to use with P4Auth. <code>string</code> <code>null</code> no debug Set this flag to enable execute command on service containers and force redeploys. <code>bool</code> <code>false</code> no deregistration_delay The amount of time to wait for in-flight requests to complete while deregistering a target. The range is 0-3600 seconds. <code>number</code> <code>30</code> no enable_alb_access_logs Enables access logging for the P4Auth ALB. Defaults to false. <code>bool</code> <code>false</code> no enable_alb_deletion_protection Enables deletion protection for the P4Auth ALB. Defaults to true. <code>bool</code> <code>false</code> no enable_web_based_administration Flag for enabling web based administration of P4Auth. <code>bool</code> <code>false</code> no existing_security_groups A list of existing security group IDs to attach to the P4Auth load balancer. <code>list(string)</code> <code>[]</code> no extra_env Extra configuration environment variables to set on the p4 auth svc container. <code>map(string)</code> <code>null</code> no fully_qualified_domain_name The fully qualified domain name where P4Auth will be available. <code>string</code> <code>null</code> no internal Set this flag to true if you do not want the P4Auth load balancer to have a public IP. <code>bool</code> <code>false</code> no name The name attached to P4Auth module resources. <code>string</code> <code>\"p4-auth\"</code> no p4d_port The P4D_PORT environment variable where Helix Authentication Service should look for Helix Core. Required if you want to use SCIM to provision users and groups. Defaults to 'ssl:perforce:1666' <code>string</code> <code>\"ssl:perforce:1666\"</code> no p4d_super_user_arn If you would like to use SCIM to provision users and groups, you need to set this variable to the ARN of an AWS Secrets Manager secret containing the super user username for p4d. <code>string</code> <code>null</code> no p4d_super_user_password_arn If you would like to use SCIM to provision users and groups, you need to set this variable to the ARN of an AWS Secrets Manager secret containing the super user password for p4d. <code>string</code> <code>null</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no s3_enable_force_destroy Enables force destroy for the S3 bucket for P4Auth access log storage. Defaults to true. <code>bool</code> <code>true</code> no scim_bearer_token_arn If you would like to use SCIM to provision users and groups, you need to set this variable to the ARN of an AWS Secrets Manager secret containing the bearer token. <code>string</code> <code>null</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"p4-auth\",  \"ModuleSource\": \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/perforce\",  \"RootModuleName\": \"terraform-aws-perforce\"}</pre> no"},{"location":"modules/perforce/modules/p4-auth/index.html#outputs","title":"Outputs","text":"Name Description alb_dns_name The DNS name of the P4Auth ALB alb_security_group_id Security group associated with the P4Auth load balancer alb_zone_id The hosted zone ID of the P4Auth ALB cluster_name Name of the ECS cluster hosting P4Auth service_security_group_id Security group associated with the ECS service running P4Auth target_group_arn The service target group for P4Auth"},{"location":"modules/perforce/modules/p4-code-review/index.html","title":"P4 Code Review Submodule","text":"<p>P4 Code Review is a free code review tool for projects hosted in P4 Server. This module deploys P4 Code Review on an EC2 Auto Scaling Group using a custom AMI built with Packer.</p> <p>P4 Code Review also relies on a Redis cache. The module provisions a single node AWS Elasticache Redis OSS cluster and configures connectivity for the P4 Code Review service.</p> <p>This module deploys the following resources:</p> <ul> <li>An EC2 Auto Scaling Group running the P4 Code Review AMI (built using the Packer template).</li> <li>A persistent EBS volume for P4 Code Review data that survives instance replacement.</li> <li>An Application Load Balancer for TLS termination of the P4 Code Review service.</li> <li>A single node AWS Elasticache Redis OSS cluster.</li> <li>Supporting resources such as CloudWatch log groups, IAM roles, and security groups.</li> </ul>"},{"location":"modules/perforce/modules/p4-code-review/index.html#architecture","title":"Architecture","text":""},{"location":"modules/perforce/modules/p4-code-review/index.html#prerequisites","title":"Prerequisites","text":"<p>P4 Code Review needs to be able to connect to a P4 Server. P4 Code Review leverages the same authentication mechanism as P4 Server, and needs to install required plugins on the upstream P4 Server instance during setup. This happens automatically, but P4 Code Review requires an administrative user's credentials to be able to initially connect. These credentials are provided to the module through variables specifying AWS Secrets Manager secrets, and then pulled into the P4 Code Review instance during startup. See the <code>p4d_super_user_arn</code>, <code>p4d_super_user_password_arn</code>, <code>p4d_swarm_user_arn</code>, and <code>p4d_swarm_password_arn</code> variables below for more details.</p> <p>The P4 Server submodule creates an administrative user on initial deployment, and stores the credentials in AWS Secrets manager. The ARN of the credentials secret is then made available as a Terraform output from the module, and can be referenced elsewhere. The is done by default by the parent Perforce module.</p> <p>Should you need to manually create the administrative user secret the following AWS CLI command may prove useful:</p> <pre><code>aws secretsmanager create-secret \\\n    --name P4CodeReviewSuperUser \\\n    --description \"P4 Code Review Super User\" \\\n    --secret-string \"{\\\"username\\\":\\\"swarm\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\"\n</code></pre> <p>You can then provide these credentials as variables when you define the P4 Code Review module in your Terraform configurations (the parent Perforce module does this for you):</p> <pre><code>module \"p4_code_review\" {\n    source = \"modules/perforce/modules/p4-code-review\"\n    ...\n    p4d_super_user_arn = \"arn:aws:secretsmanager:&lt;your-aws-region&gt;:&lt;your-aws-account-id&gt;:secret:P4CodeReviewSuperUser-a1b2c3:username::\"\n    p4d_super_user_password_arn = \"arn:aws:secretsmanager:&lt;your-aws-region&gt;:&lt;your-aws-account-id&gt;:secret:P4CodeReviewSuperUser-a1b2c3:password::\"\n}\n</code></pre>"},{"location":"modules/perforce/modules/p4-code-review/index.html#debugging","title":"Debugging","text":"<p>If you're running into issues with P4 Code Review, here are some common log files to investigate:</p> <ul> <li><code>/var/log/apache2/swarm.error_log</code>: any PHP / configuration related errors</li> <li><code>/opt/perforce/swarm/data/configure-swarm.log</code>: errors coming from p4cr configuration</li> <li><code>/opt/perforce/swarm/data/log</code>: errors from the p4cr runtime</li> </ul>"},{"location":"modules/perforce/modules/p4-code-review/index.html#custom-configuration","title":"Custom Configuration","text":"<p>The <code>custom_config</code> variable allows you to pass additional configuration to P4 Code Review as a JSON string. This configuration is merged with the generated <code>config.php</code> using PHP's <code>array_replace_recursive</code> function at instance startup.</p> <p>This can be used to configure:</p> <ul> <li>SSO/SAML authentication</li> <li>Email notifications</li> <li>Jira integration</li> <li>Project settings</li> <li>And any other Swarm configuration option</li> </ul>"},{"location":"modules/perforce/modules/p4-code-review/index.html#example-ssosaml-with-auth0","title":"Example: SSO/SAML with Auth0","text":"<p>SSO/SAML configuration requires two parts:</p> <ol> <li><code>p4.sso</code> - Enables the SSO login option. Values:</li> <li><code>\"disabled\"</code> - No SSO, only password login (default)</li> <li><code>\"optional\"</code> - Both SSO and password login available</li> <li> <p><code>\"enabled\"</code> - SSO only, no password login</p> </li> <li> <p><code>saml</code> - The SAML technical configuration (IdP/SP settings, certificates)</p> </li> </ol> <pre><code>module \"p4_code_review\" {\n  source = \"modules/perforce/modules/p4-code-review\"\n  # ... other required variables ...\n\n  custom_config = jsonencode({\n    # Enable SSO login option\n    p4 = {\n      sso = \"optional\"\n    }\n    # SAML configuration\n    saml = {\n      header = \"Log in with SSO\"\n      sp = {\n        entityId                  = \"https://swarm.example.com\"\n        assertionConsumerService = {\n          url = \"https://swarm.example.com/saml/acs\"\n        }\n        singleLogoutService = {\n          url = \"https://swarm.example.com/saml/sls\"\n        }\n        NameIDFormat = \"urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress\"\n      }\n      idp = {\n        entityId                 = \"urn:your-auth0-domain\"\n        singleSignOnService = {\n          url = \"https://your-auth0-domain/samlp/YOUR_CLIENT_ID\"\n        }\n        singleLogoutService = {\n          url = \"https://your-auth0-domain/samlp/YOUR_CLIENT_ID/logout\"\n        }\n        x509cert = \"YOUR_IDP_CERTIFICATE_HERE\"\n      }\n    }\n  })\n}\n</code></pre>"},{"location":"modules/perforce/modules/p4-code-review/index.html#example-email-notifications","title":"Example: Email Notifications","text":"<pre><code>module \"p4_code_review\" {\n  source = \"modules/perforce/modules/p4-code-review\"\n  # ... other required variables ...\n\n  custom_config = jsonencode({\n    mail = {\n      transport = {\n        host = \"smtp.example.com\"\n        port = 587\n        security = \"tls\"\n      }\n      sender = \"swarm@example.com\"\n    }\n  })\n}\n</code></pre>"},{"location":"modules/perforce/modules/p4-code-review/index.html#example-jira-integration","title":"Example: Jira Integration","text":"<pre><code>module \"p4_code_review\" {\n  source = \"modules/perforce/modules/p4-code-review\"\n  # ... other required variables ...\n\n  custom_config = jsonencode({\n    jira = {\n      host     = \"https://your-company.atlassian.net\"\n      user     = \"jira-user@example.com\"\n      password = \"your-api-token\"\n      job_field = \"customfield_10001\"\n    }\n  })\n}\n</code></pre>"},{"location":"modules/perforce/modules/p4-code-review/index.html#combining-multiple-configurations","title":"Combining Multiple Configurations","text":"<p>You can combine multiple configuration sections in a single <code>custom_config</code>:</p> <pre><code>custom_config = jsonencode({\n  saml = {\n    # SSO configuration...\n  }\n  mail = {\n    # Email configuration...\n  }\n  jira = {\n    # Jira configuration...\n  }\n})\n</code></pre>"},{"location":"modules/perforce/modules/p4-code-review/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws ~&gt; 6.6 random ~&gt; 3.7"},{"location":"modules/perforce/modules/p4-code-review/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 random ~&gt; 3.7"},{"location":"modules/perforce/modules/p4-code-review/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/modules/p4-code-review/index.html#resources","title":"Resources","text":"Name Type aws_autoscaling_group.swarm_asg resource aws_cloudwatch_log_group.application_log_group resource aws_cloudwatch_log_group.redis_service_log_group resource aws_ebs_volume.swarm_data resource aws_elasticache_cluster.cluster resource aws_elasticache_subnet_group.subnet_group resource aws_iam_instance_profile.ec2_instance_profile resource aws_iam_policy.ebs_attachment_policy resource aws_iam_policy.secrets_manager_policy resource aws_iam_role.ec2_instance_role resource aws_iam_role_policy_attachment.ec2_instance_role_ebs resource aws_iam_role_policy_attachment.ec2_instance_role_secrets_manager resource aws_iam_role_policy_attachment.ec2_instance_role_ssm resource aws_launch_template.swarm_instance resource aws_lb.alb resource aws_lb_listener.alb_https_listener resource aws_lb_target_group.alb_target_group resource aws_s3_bucket.alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_security_group.alb resource aws_security_group.application resource aws_security_group.ec2_instance resource aws_security_group.elasticache resource aws_vpc_security_group_egress_rule.alb_outbound_to_application resource aws_vpc_security_group_egress_rule.application_outbound_to_internet_ipv4 resource aws_vpc_security_group_egress_rule.application_outbound_to_internet_ipv6 resource aws_vpc_security_group_egress_rule.ec2_instance_outbound_to_internet_ipv4 resource aws_vpc_security_group_egress_rule.ec2_instance_outbound_to_internet_ipv6 resource aws_vpc_security_group_ingress_rule.alb_inbound_from_application resource aws_vpc_security_group_ingress_rule.application_inbound_alb resource aws_vpc_security_group_ingress_rule.elasticache_inbound_from_application resource random_string.alb_access_logs_bucket_suffix resource random_string.p4_code_review resource aws_ami.p4_code_review data source aws_caller_identity.current data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.ebs_attachment_policy data source aws_iam_policy_document.ec2_instance_trust_relationship data source aws_iam_policy_document.secrets_manager_policy data source aws_region.current data source aws_subnet.instance_subnet data source"},{"location":"modules/perforce/modules/p4-code-review/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required instance_subnet_id The subnet ID where the EC2 instance will be launched. Should be a private subnet for security. <code>string</code> n/a yes p4_code_review_user_password_secret_arn Optionally provide the ARN of an AWS Secret for the p4d P4 Code Review password. <code>string</code> n/a yes p4_code_review_user_username_secret_arn Optionally provide the ARN of an AWS Secret for the p4d P4 Code Review username. <code>string</code> n/a yes subnets A list of subnets for ElastiCache Redis deployment. Private subnets are recommended. <code>list(string)</code> n/a yes super_user_password_secret_arn Optionally provide the ARN of an AWS Secret for the p4d super user password. <code>string</code> n/a yes vpc_id The ID of the existing VPC you would like to deploy P4 Code Review into. <code>string</code> n/a yes alb_access_logs_bucket ID of the S3 bucket for P4 Code Review ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no alb_access_logs_prefix Log prefix for P4 Code Review ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no alb_subnets A list of subnets to deploy the load balancer into. Public subnets are recommended. <code>list(string)</code> <code>[]</code> no ami_id Optional AMI ID for P4 Code Review. If not provided, will use the latest Packer-built AMI with name pattern 'p4_code_review_ubuntu-*'. <code>string</code> <code>null</code> no application_load_balancer_name The name of the P4 Code Review ALB. Defaults to the project prefix and module name. <code>string</code> <code>null</code> no application_port The port that P4 Code Review listens on. Used for ALB target group configuration. <code>number</code> <code>80</code> no certificate_arn The TLS certificate ARN for the P4 Code Review service load balancer. <code>string</code> <code>null</code> no cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for P4 Code Review. <code>string</code> <code>365</code> no create_application_load_balancer This flag controls the creation of an application load balancer as part of the module. <code>bool</code> <code>true</code> no custom_config JSON string with additional Swarm configuration to merge with the generated config.php. Use this for SSO/SAML setup, notifications, Jira integration, etc. See README for examples. <code>string</code> <code>null</code> no deregistration_delay The amount of time to wait for in-flight requests to complete while deregistering a target. The range is 0-3600 seconds. <code>number</code> <code>30</code> no ebs_availability_zone Availability zone for the EBS volume. Must match the EC2 instance AZ. If not provided, will use the AZ of the instance_subnet_id. <code>string</code> <code>null</code> no ebs_volume_encrypted Enable encryption for the EBS volume storing P4 Code Review data. <code>bool</code> <code>true</code> no ebs_volume_size Size in GB for the EBS volume that stores P4 Code Review data (/opt/perforce/swarm/data). This volume persists across instance replacement. <code>number</code> <code>20</code> no ebs_volume_type EBS volume type for P4 Code Review data storage. <code>string</code> <code>\"gp3\"</code> no elasticache_node_count Number of cache nodes to provision in the Elasticache cluster. <code>number</code> <code>1</code> no elasticache_node_type The type of nodes provisioned in the Elasticache cluster. <code>string</code> <code>\"cache.t4g.micro\"</code> no enable_alb_access_logs Enables access logging for the P4 Code Review ALB. Defaults to false. <code>bool</code> <code>false</code> no enable_alb_deletion_protection Enables deletion protection for the P4 Code Review ALB. Defaults to true. <code>bool</code> <code>false</code> no existing_redis_connection The connection specifications to use for an existing Redis deployment. <pre>object({    host = string    port = number  })</pre> <code>null</code> no existing_security_groups A list of existing security group IDs to attach to the P4 Code Review load balancer. <code>list(string)</code> <code>[]</code> no fully_qualified_domain_name The fully qualified domain name that P4 Code Review should use for internal URLs. <code>string</code> <code>null</code> no instance_type EC2 instance type for running P4 Code Review. Swarm requires persistent storage and runs natively on EC2. <code>string</code> <code>\"m5.large\"</code> no internal Set this flag to true if you do not want the P4 Code Review service load balancer to have a public IP. <code>bool</code> <code>false</code> no name The name attached to P4 Code Review module resources. <code>string</code> <code>\"p4-code-review\"</code> no p4charset The P4CHARSET environment variable to set for the P4 Code Review instance. <code>string</code> <code>\"none\"</code> no p4d_port The P4D_PORT environment variable where P4 Code Review should look for P4 Server. Defaults to 'ssl:perforce:1666' <code>string</code> <code>\"ssl:perforce:1666\"</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no s3_enable_force_destroy Enables force destroy for the S3 bucket for P4 Code Review access log storage. Defaults to true. <code>bool</code> <code>true</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"p4-code-review\",  \"ModuleSource\": \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/perforce\",  \"RootModuleName\": \"terraform-aws-perforce\"}</pre> no"},{"location":"modules/perforce/modules/p4-code-review/index.html#outputs","title":"Outputs","text":"Name Description alb_dns_name The DNS name of the P4 Code Review ALB alb_security_group_id Security group associated with the P4 Code Review load balancer alb_zone_id The hosted zone ID of the P4 Code Review ALB application_security_group_id Security group associated with the P4 Code Review application autoscaling_group_name The name of the Auto Scaling Group for P4 Code Review ebs_volume_id The ID of the EBS volume storing P4 Code Review persistent data instance_profile_arn The ARN of the IAM instance profile for P4 Code Review EC2 instances launch_template_id The ID of the launch template for P4 Code Review instances target_group_arn The target group ARN for P4 Code Review"},{"location":"modules/perforce/modules/p4-server/index.html","title":"P4 Server Submodule","text":"<p>P4 Server is a scalable version control system that helps teams manage code alongside large, digital assets and collaborate more effectively in one central, secure location. With AWS, teams can quickly deploy Helix Core and accelerate innovation.</p> <p>This module provisions P4 Server on an EC2 Instance with three dedicated EBS volumes for depots, metadata, and logs. It can also be configured to automatically install the required plugins to integrate with P4Auth. This is the default option if using the parent module. This allows end users to quickly set up single-sign-on for their Perforce Helix Core server.</p>"},{"location":"modules/perforce/modules/p4-server/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/perforce/modules/p4-server/index.html#prerequisites","title":"Prerequisites","text":"<p>This module deploys P4 Server on AWS using an Amazon Machine Image (AMI) that is included in the Cloud Game Development Toolkit. You must provision this AMI using Hashicorp Packer prior to deploying this module. To get started consult the documentation for the P4 Server AMI.</p>"},{"location":"modules/perforce/modules/p4-server/index.html#user-management","title":"User Management","text":"<p>This module creates two users with super privileges:</p> <ol> <li> <p>Service Account (<code>super</code>): An internal service account used by P4 Code Review (Helix Swarm) and other Perforce tooling. This user is always created automatically with a randomly generated password stored in AWS Secrets Manager. The service account uses password-based authentication (non-SSO).</p> </li> <li> <p>Admin Account: A human administrator account for managing the Perforce server. The username defaults to <code>perforce</code> but can be customized via the <code>admin_username</code> variable. The password is auto-generated and stored in AWS Secrets Manager, or you can provide your own secret ARN.</p> </li> </ol>"},{"location":"modules/perforce/modules/p4-server/index.html#configuring-the-admin-account","title":"Configuring the Admin Account","text":"<p>By default, an admin user named <code>perforce</code> is created:</p> <pre><code>module \"p4_server\" {\n    source = \"modules/perforce/modules/p4-server\"\n    ...\n    # Uses default admin_username = \"perforce\"\n    # Password auto-generated and stored in Secrets Manager\n}\n</code></pre> <p>To customize the admin username:</p> <pre><code>module \"p4_server\" {\n    source = \"modules/perforce/modules/p4-server\"\n    ...\n    admin_username = \"myadmin\"\n}\n</code></pre> <p>To use an existing password secret:</p> <pre><code>module \"p4_server\" {\n    source = \"modules/perforce/modules/p4-server\"\n    ...\n    admin_username            = \"myadmin\"\n    admin_password_secret_arn = \"arn:aws:secretsmanager:us-west-2:123456789012:secret:MyAdminPassword-a1b2c3\"\n}\n</code></pre>"},{"location":"modules/perforce/modules/p4-server/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws ~&gt; 6.6 awscc ~&gt; 1.51 local ~&gt; 2.5 netapp-ontap ~&gt; 2.3 null ~&gt; 3.2 random ~&gt; 3.7"},{"location":"modules/perforce/modules/p4-server/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 awscc ~&gt; 1.51 netapp-ontap ~&gt; 2.3 random ~&gt; 3.7"},{"location":"modules/perforce/modules/p4-server/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/modules/p4-server/index.html#resources","title":"Resources","text":"Name Type aws_ebs_volume.depot resource aws_ebs_volume.logs resource aws_ebs_volume.metadata resource aws_eip.server_eip resource aws_fsx_ontap_volume.depot resource aws_fsx_ontap_volume.logs resource aws_fsx_ontap_volume.metadata resource aws_iam_instance_profile.instance_profile resource aws_iam_policy.default_policy resource aws_iam_role.default_role resource aws_iam_role.lambda_role resource aws_iam_role_policy_attachment.default_role_default_policy resource aws_iam_role_policy_attachment.default_role_ssm_managed_instance_core resource aws_iam_role_policy_attachment.lambda_service_basic_execution_role resource aws_iam_role_policy_attachment.lambda_service_role resource aws_iam_role_policy_attachment.lambda_vpc_access_role resource aws_instance.server_instance resource aws_lambda_function.lambda_function resource aws_security_group.default_security_group resource aws_security_group.fsxn_lambda_link_security_group resource aws_volume_attachment.depot_attachment resource aws_volume_attachment.logs_attachment resource aws_volume_attachment.metadata_attachment resource aws_vpc_security_group_egress_rule.link_outbound_fsxn resource aws_vpc_security_group_egress_rule.server_internet resource aws_vpc_security_group_ingress_rule.fsxn_inbound_link resource awscc_secretsmanager_secret.admin_password resource awscc_secretsmanager_secret.admin_username resource awscc_secretsmanager_secret.super_password resource netapp-ontap_lun.depots_volume_lun resource netapp-ontap_lun.logs_volume_lun resource netapp-ontap_lun.metadata_volume_lun resource netapp-ontap_san_igroup.perforce_igroup resource netapp-ontap_san_lun-map.depots_lun_map resource netapp-ontap_san_lun-map.logs_lun_map resource netapp-ontap_san_lun-map.metadata_lun_map resource random_string.p4_server resource aws_ami.existing_server_ami data source aws_iam_policy_document.default_policy data source aws_iam_policy_document.ec2_trust_relationship data source aws_subnet.instance_subnet data source"},{"location":"modules/perforce/modules/p4-server/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required instance_subnet_id The subnet where the P4 Server instance will be deployed. <code>string</code> n/a yes p4_server_type The Perforce P4 Server type. <code>string</code> n/a yes storage_type The type of backing store [EBS, FSxN] <code>string</code> n/a yes vpc_id The VPC where P4 Server should be deployed <code>string</code> n/a yes admin_password_secret_arn Optional ARN of existing Secrets Manager secret for admin password. If not provided, a password will be auto-generated. <code>string</code> <code>null</code> no admin_username Username for the Perforce admin account (human user). The 'super' service account is always created automatically for internal tooling. <code>string</code> <code>\"perforce\"</code> no amazon_fsxn_filesystem_id The ID of the existing FSx ONTAP file system to use if storage type is FSxN. <code>string</code> <code>null</code> no amazon_fsxn_svm_id The ID of the Storage Virtual Machine (SVM) for the FSx ONTAP filesystem. <code>string</code> <code>null</code> no auth_service_url The URL for the P4Auth Service. <code>string</code> <code>null</code> no case_sensitive Whether or not the server should be case insensitive (Server will run '-C1' mode), or if the server will run with case sensitivity default of the underlying platform. False enables '-C1' mode <code>bool</code> <code>true</code> no create_default_role Optional creation of P4 Server default IAM Role with SSM managed instance core policy attached. Default is set to true. <code>bool</code> <code>true</code> no create_default_sg Whether to create a default security group for the P4 Server instance. <code>bool</code> <code>true</code> no custom_role ARN of the custom IAM Role you wish to use with P4 Server. <code>string</code> <code>null</code> no depot_volume_size The size of the depot volume in GiB. Defaults to 128 GiB. <code>number</code> <code>128</code> no environment The environment attached to P4 Server module resources. <code>string</code> <code>\"dev\"</code> no existing_security_groups A list of existing security group IDs to attach to the P4 Server load balancer. <code>list(string)</code> <code>[]</code> no fsxn_filesystem_security_group_id The ID of the security group for the FSx ONTAP filesystem. <code>string</code> <code>null</code> no fsxn_management_ip FSxN management ip address <code>string</code> <code>null</code> no fsxn_password FSxN admin user password AWS secret manager arn <code>string</code> <code>null</code> no fsxn_region The ID of the Storage Virtual Machine (SVM) for the FSx ONTAP filesystem. <code>string</code> <code>null</code> no fsxn_svm_name FSxN storage virtual machine name <code>string</code> <code>null</code> no fully_qualified_domain_name The fully qualified domain name where P4 Server will be available. This is used to generate self-signed certificates on the P4 Server. <code>string</code> <code>null</code> no instance_architecture The architecture of the P4 Server instance. Allowed values are 'arm64' or 'x86_64'. <code>string</code> <code>\"x86_64\"</code> no instance_private_ip The private IP address to assign to the P4 Server. <code>string</code> <code>null</code> no instance_type The instance type for Perforce P4 Server. Defaults to c6g.large. <code>string</code> <code>\"c6i.large\"</code> no internal Set this flag to true if you do not want the P4 Server instance to have a public IP. <code>bool</code> <code>false</code> no logs_volume_size The size of the logs volume in GiB. Defaults to 32 GiB. <code>number</code> <code>32</code> no metadata_volume_size The size of the metadata volume in GiB. Defaults to 32 GiB. <code>number</code> <code>32</code> no name The name attached to P4 Server module resources. <code>string</code> <code>\"p4-server\"</code> no plaintext Whether to enable plaintext authentication for P4 Server. This is not recommended for production environments unless you are using a load balancer for TLS termination. <code>bool</code> <code>false</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no protocol Specify the protocol (NFS or ISCSI) <code>string</code> <code>null</code> no selinux Whether to apply SELinux label updates for P4 Server. Don't enable this if SELinux is disabled on your target operating system. <code>bool</code> <code>false</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"p4-server\",  \"ModuleSource\": \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/perforce\",  \"RootModuleName\": \"terraform-aws-perforce\"}</pre> no unicode Whether to enable Unicode configuration for P4 Server the -xi flag for p4d. Set to true to enable Unicode support. <code>bool</code> <code>false</code> no"},{"location":"modules/perforce/modules/p4-server/index.html#outputs","title":"Outputs","text":"Name Description admin_password_secret_arn The ARN of the AWS Secrets Manager secret holding the admin account password. admin_username_secret_arn The ARN of the AWS Secrets Manager secret holding the admin account username. eip_id The ID of the Elastic IP associated with your P4 Server instance. eip_public_ip The public IP of your P4 Server instance. instance_id Instance ID for the P4 Server instance lambda_link_name Lambda function name for the FSxN Link private_ip Private IP for the P4 Server instance security_group_id The default security group of your P4 Server instance. super_password_secret_arn The ARN of the AWS Secrets Manager secret holding the service account (super) password."},{"location":"modules/perforce/tests/index.html","title":"Perforce Module Tests","text":"<p>This directory contains comprehensive tests for the Perforce wrapper module, organized into unit tests and integration tests.</p>"},{"location":"modules/perforce/tests/index.html#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                                    # Mock-based unit tests\n\u2502   \u251c\u2500\u2500 01_conditional_creation.tftest.hcl   # Tests submodule conditional creation\n\u2502   \u251c\u2500\u2500 02_shared_resources.tftest.hcl       # Tests shared resource logic\n\u2502   \u2514\u2500\u2500 README.md                            # Unit test documentation\n\u2502\n\u251c\u2500\u2500 integration/                             # Integration tests with real AWS\n\u2502   \u251c\u2500\u2500 setup/                               # Setup module for SSM parameters\n\u2502   \u2502   \u251c\u2500\u2500 ssm.tf                          # SSM parameter data sources\n\u2502   \u2502   \u2514\u2500\u2500 versions.tf                     # Provider requirements\n\u2502   \u251c\u2500\u2500 01_create_resources_complete.tftest.hcl\n\u2502   \u251c\u2500\u2500 02_p4_server_fsxn.tftest.hcl\n\u2502   \u2514\u2500\u2500 README.md                            # Integration test documentation\n\u2502\n\u2514\u2500\u2500 README.md                                # This file\n</code></pre>"},{"location":"modules/perforce/tests/index.html#quick-start","title":"Quick Start","text":""},{"location":"modules/perforce/tests/index.html#run-all-tests","title":"Run All Tests","text":"<pre><code>cd /path/to/modules/perforce\nterraform test\n</code></pre>"},{"location":"modules/perforce/tests/index.html#run-only-unit-tests-no-aws-required","title":"Run Only Unit Tests (No AWS Required)","text":"<pre><code>terraform test -filter=tests/unit/\n</code></pre>"},{"location":"modules/perforce/tests/index.html#run-only-integration-tests-aws-required","title":"Run Only Integration Tests (AWS Required)","text":"<pre><code>export AWS_PROFILE=your-profile\nterraform test -filter=tests/integration/\n</code></pre>"},{"location":"modules/perforce/tests/index.html#test-types","title":"Test Types","text":""},{"location":"modules/perforce/tests/index.html#unit-tests-unit","title":"Unit Tests (<code>unit/</code>)","text":"<p>Purpose: Validate conditional logic and resource creation without deploying infrastructure</p> <p>Characteristics:</p> <ul> <li>\u2705 Uses mock providers (no AWS credentials needed)</li> <li>\u2705 Fast execution (seconds)</li> <li>\u2705 Safe to run anywhere</li> <li>\u2705 Tests all conditional logic paths</li> <li>\u2705 No AWS costs</li> </ul> <p>When to Run: On every code change, in CI/CD pipelines, during development</p> <p>Test Coverage:</p> <ul> <li>Conditional creation of P4 Server, P4 Auth, and P4 Code Review submodules</li> <li>Shared ECS cluster creation logic</li> <li>Load balancer and Route53 resource creation</li> <li>Security group configurations</li> </ul> <p>\ud83d\udcd6 Unit Tests Documentation</p>"},{"location":"modules/perforce/tests/index.html#integration-tests-integration","title":"Integration Tests (<code>integration/</code>)","text":"<p>Purpose: Validate that example deployments work with real AWS resources</p> <p>Characteristics:</p> <ul> <li>\u26a0\ufe0f Requires AWS credentials</li> <li>\u26a0\ufe0f Slower execution (minutes)</li> <li>\u26a0\ufe0f Plans against real AWS (no apply by default)</li> <li>\u2705 Tests real-world scenarios</li> <li>\u2705 Validates examples work correctly</li> </ul> <p>When to Run: Before releases, when testing infrastructure changes, in CI/CD with AWS access</p> <p>Test Coverage:</p> <ul> <li>Complete Perforce deployment example</li> <li>P4 Server with FSxN storage example</li> <li>Example configurations with real parameters</li> </ul> <p>\ud83d\udcd6 Integration Tests Documentation</p>"},{"location":"modules/perforce/tests/index.html#cicd-integration","title":"CI/CD Integration","text":""},{"location":"modules/perforce/tests/index.html#validation-workflow","title":"Validation Workflow","text":"<p>The <code>terraform-validation.yml</code> workflow validates Terraform configurations:</p> <p>What it validates:</p> <ul> <li>All directories containing <code>.tf</code> files (modules, submodules, examples, test setup)</li> <li>Runs <code>terraform init</code> and <code>terraform validate</code></li> <li>Skips directories with only <code>.tftest.hcl</code> files</li> </ul> <p>What triggers it:</p> <ul> <li>Changes to <code>modules/**/*.tf</code> or <code>samples/**/*.tf</code></li> <li>Push to <code>main</code> branch</li> <li>Manual workflow dispatch</li> </ul>"},{"location":"modules/perforce/tests/index.html#test-workflow","title":"Test Workflow","text":"<p>The <code>terraform-tests.yml</code> workflow runs Terraform tests:</p> <p>What it runs:</p> <ul> <li>All <code>.tftest.hcl</code> files in modules with a <code>tests/</code> directory</li> <li>Automatically runs when module files change</li> <li>Requires AWS credentials for integration tests</li> </ul> <p>Workflow behavior:</p> <ul> <li>Detects changed modules</li> <li>Runs <code>terraform test</code> from module root</li> <li>Reports failures to pull requests</li> </ul>"},{"location":"modules/perforce/tests/index.html#development-workflow","title":"Development Workflow","text":""},{"location":"modules/perforce/tests/index.html#adding-new-features","title":"Adding New Features","text":"<ol> <li>Write unit tests first - Add test scenarios to <code>unit/</code> for new conditional logic</li> <li>Implement the feature - Modify module code</li> <li>Run unit tests - Verify conditional logic works: <code>terraform test -filter=tests/unit/</code></li> <li>Add integration tests - If needed, add scenarios to <code>integration/</code></li> <li>Run all tests - Verify everything works: <code>terraform test</code></li> </ol>"},{"location":"modules/perforce/tests/index.html#debugging-test-failures","title":"Debugging Test Failures","text":"<p>Unit test failures:</p> <pre><code># Run with verbose output\nterraform test -filter=tests/unit/01_conditional_creation.tftest.hcl -verbose\n\n# Check specific assertion\n# Look for \"error_message\" in the output to see which assertion failed\n</code></pre> <p>Integration test failures:</p> <pre><code># Verify AWS credentials\naws sts get-caller-identity\n\n# Check SSM parameters exist\naws ssm get-parameter --name \"/cloud-game-development-toolkit/modules/perforce/route53-public-hosted-zone-name\"\n\n# Run with verbose output\nterraform test -filter=tests/integration/ -verbose\n</code></pre>"},{"location":"modules/perforce/tests/index.html#test-maintenance","title":"Test Maintenance","text":""},{"location":"modules/perforce/tests/index.html#when-to-update-tests","title":"When to Update Tests","text":"<p>Update unit tests when:</p> <ul> <li>Adding new conditional logic</li> <li>Adding new submodules or shared resources</li> <li>Changing variable validation rules</li> <li>Modifying resource creation conditions</li> </ul> <p>Update integration tests when:</p> <ul> <li>Adding new examples</li> <li>Changing example configurations</li> <li>Modifying required variables</li> <li>Adding new deployment patterns</li> </ul>"},{"location":"modules/perforce/tests/index.html#adding-new-test-files","title":"Adding New Test Files","text":"<p>Unit tests:</p> <ol> <li>Create new <code>.tftest.hcl</code> file in <code>unit/</code></li> <li>Copy mock provider blocks from existing test</li> <li>Add test scenarios with clear names and assertions</li> <li>Update <code>unit/README.md</code> with test documentation</li> </ol> <p>Integration tests:</p> <ol> <li>Create new <code>.tftest.hcl</code> file in <code>integration/</code></li> <li>Add required SSM parameters to <code>integration/setup/ssm.tf</code></li> <li>Reference appropriate example deployment</li> <li>Update <code>integration/README.md</code> with test documentation</li> </ol>"},{"location":"modules/perforce/tests/index.html#best-practices","title":"Best Practices","text":""},{"location":"modules/perforce/tests/index.html#writing-good-tests","title":"Writing Good Tests","text":"<p>\u2705 DO:</p> <ul> <li>Use descriptive test names (<code>p4_server_only</code> not <code>test1</code>)</li> <li>Write clear assertion error messages</li> <li>Test both success and failure scenarios</li> <li>Document complex test logic with comments</li> <li>Keep tests focused on one aspect</li> </ul> <p>\u274c DON'T:</p> <ul> <li>Hardcode sensitive values (use SSM for integration tests)</li> <li>Create tests that depend on execution order</li> <li>Test implementation details (test behavior, not code)</li> <li>Ignore test failures (fix or document expected failures)</li> </ul>"},{"location":"modules/perforce/tests/index.html#mock-provider-patterns","title":"Mock Provider Patterns","text":"<p>When creating unit tests:</p> <ol> <li>Always include all mock providers (even if unused)</li> <li>Use realistic mock data (valid ARNs, IDs, etc.)</li> <li>Copy mock blocks from existing tests for consistency</li> <li>Document any custom mock configurations</li> </ol>"},{"location":"modules/perforce/tests/index.html#performance-considerations","title":"Performance Considerations","text":""},{"location":"modules/perforce/tests/index.html#test-execution-time","title":"Test Execution Time","text":"Test Type Typical Duration Parallelization Unit (single file) 2-5 seconds Yes Unit (all) 10-15 seconds Yes Integration (single) 30-60 seconds Yes Integration (all) 2-5 minutes Yes"},{"location":"modules/perforce/tests/index.html#optimizing-test-speed","title":"Optimizing Test Speed","text":"<ul> <li>Run unit tests during development (fast feedback)</li> <li>Run integration tests before commits (thorough validation)</li> <li>Use <code>-filter</code> to run specific tests during debugging</li> <li>Leverage Terraform's parallel test execution</li> </ul>"},{"location":"modules/perforce/tests/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"modules/perforce/tests/index.html#common-issues","title":"Common Issues","text":""},{"location":"modules/perforce/tests/index.html#no-tests-found","title":"\"No tests found\"","text":"<ul> <li>Ensure you're running from the module root directory</li> <li>Verify <code>.tftest.hcl</code> files exist in <code>tests/</code> subdirectories</li> </ul>"},{"location":"modules/perforce/tests/index.html#module-not-found","title":"\"Module not found\"","text":"<ul> <li>Check that module paths are relative to the test file location</li> <li>Integration tests should use <code>../../examples/</code> for example paths</li> <li>Unit tests should reference the module root</li> </ul>"},{"location":"modules/perforce/tests/index.html#provider-configuration-not-found","title":"\"Provider configuration not found\"","text":"<ul> <li>Verify all required mock providers are declared</li> <li>Check that provider versions match <code>versions.tf</code></li> </ul>"},{"location":"modules/perforce/tests/index.html#variable-not-set","title":"\"Variable not set\"","text":"<ul> <li>Ensure all required variables are provided in test scenarios</li> <li>Check that variable types match module expectations</li> </ul>"},{"location":"modules/perforce/tests/index.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Terraform Testing Documentation</li> <li>Module README</li> <li>Example Deployments</li> <li>Horde Module Tests - Reference implementation</li> </ul>"},{"location":"modules/perforce/tests/index.html#contributing","title":"Contributing","text":"<p>When contributing tests:</p> <ol> <li>Follow existing test patterns and naming conventions</li> <li>Update documentation when adding new tests</li> <li>Ensure tests pass locally before submitting PR</li> <li>Add test coverage for new features</li> <li>Keep tests maintainable and well-documented</li> </ol>"},{"location":"modules/perforce/tests/index.html#questions","title":"Questions?","text":"<p>For questions about testing:</p> <ul> <li>Review the unit test README for mock-based testing</li> <li>Review the integration test README for AWS-based testing</li> <li>Check the main module documentation for module usage</li> <li>Open an issue in the repository for specific problems</li> </ul>"},{"location":"modules/perforce/tests/integration/index.html","title":"Perforce Module Integration Tests","text":"<p>This directory contains integration tests that validate the Perforce module against real AWS infrastructure by invoking the example deployments.</p>"},{"location":"modules/perforce/tests/integration/index.html#overview","title":"Overview","text":"<p>Integration tests differ from unit tests in that they:</p> <ul> <li>Use real AWS resources - Actual infrastructure is deployed (in plan mode for safety)</li> <li>Test example deployments - Validates that examples work correctly</li> <li>Require AWS credentials - Must have valid AWS authentication configured</li> <li>Use SSM parameters - Fetch configuration values from AWS Systems Manager Parameter Store</li> </ul>"},{"location":"modules/perforce/tests/integration/index.html#test-files","title":"Test Files","text":""},{"location":"modules/perforce/tests/integration/index.html#01_create_resources_completetftesthcl","title":"<code>01_create_resources_complete.tftest.hcl</code>","text":"<p>Purpose: Validates the complete Perforce deployment example</p> <p>Example Invoked: <code>examples/create-resources-complete</code></p> <p>Required SSM Parameters:</p> <ul> <li><code>/cloud-game-development-toolkit/modules/perforce/route53-public-hosted-zone-name</code></li> </ul> <p>Test Flow:</p> <ol> <li><code>setup</code> run - Fetches Route53 zone name from SSM Parameter Store</li> <li><code>unit_test</code> run - Plans the complete example deployment using fetched parameters</li> </ol>"},{"location":"modules/perforce/tests/integration/index.html#02_p4_server_fsxntftesthcl","title":"<code>02_p4_server_fsxn.tftest.hcl</code>","text":"<p>Purpose: Validates P4 Server deployment with FSxN (NetApp ONTAP) storage</p> <p>Example Invoked: <code>examples/p4-server-fsxn</code></p> <p>Required SSM Parameters:</p> <ul> <li><code>/cloud-game-development-toolkit/modules/perforce/route53-public-hosted-zone-name</code></li> <li><code>/cloud-game-development-toolkit/modules/perforce/fsxn-password</code></li> <li><code>/cloud-game-development-toolkit/modules/perforce/fsxn-aws-profile</code></li> </ul> <p>Test Flow:</p> <ol> <li><code>setup</code> run - Fetches FSxN configuration from SSM Parameter Store</li> <li><code>unit_test</code> run - Plans the FSxN example deployment using fetched parameters</li> </ol>"},{"location":"modules/perforce/tests/integration/index.html#setup-module","title":"Setup Module","text":"<p>The <code>setup/</code> directory contains a Terraform module that fetches test configuration from AWS Systems Manager Parameter Store.</p> <p>Files:</p> <ul> <li><code>setup/ssm.tf</code> - Data sources for SSM parameters and outputs</li> <li><code>setup/versions.tf</code> - Terraform and provider version constraints</li> </ul> <p>Purpose: Centralizes test configuration management and avoids hardcoding sensitive values in test files.</p>"},{"location":"modules/perforce/tests/integration/index.html#running-integration-tests","title":"Running Integration Tests","text":""},{"location":"modules/perforce/tests/integration/index.html#prerequisites","title":"Prerequisites","text":"<ol> <li>AWS Credentials - Configure AWS authentication:</li> </ol> <pre><code>export AWS_PROFILE=your-profile\n# OR\nexport AWS_ACCESS_KEY_ID=xxx\nexport AWS_SECRET_ACCESS_KEY=xxx\n</code></pre> <ol> <li>SSM Parameters - Create required parameters in your AWS account:</li> </ol> <pre><code>aws ssm put-parameter \\\n  --name \"/cloud-game-development-toolkit/modules/perforce/route53-public-hosted-zone-name\" \\\n  --value \"your-domain.com\" \\\n  --type String\n</code></pre>"},{"location":"modules/perforce/tests/integration/index.html#run-all-integration-tests","title":"Run All Integration Tests","text":"<p>From the module root directory:</p> <pre><code>cd /path/to/modules/perforce\nterraform test -filter=tests/integration/\n</code></pre>"},{"location":"modules/perforce/tests/integration/index.html#run-specific-integration-test","title":"Run Specific Integration Test","text":"<pre><code>terraform test -filter=tests/integration/01_create_resources_complete.tftest.hcl\n</code></pre>"},{"location":"modules/perforce/tests/integration/index.html#e2e-tests-disabled","title":"E2E Tests (Disabled)","text":"<p>The integration test files contain commented-out <code>e2e_test</code> run blocks that would deploy actual infrastructure using <code>command = apply</code>. These are currently disabled due to Terraform test error handling limitations:</p> <ul> <li>Issue: hashicorp/terraform#36846</li> <li>When to enable: Once Terraform improves error handling and retry logic for test commands</li> </ul>"},{"location":"modules/perforce/tests/integration/index.html#test-workflow","title":"Test Workflow","text":"<p>Integration tests are automatically run by the <code>terraform-tests.yml</code> GitHub Actions workflow:</p> <p>Trigger Conditions:</p> <ul> <li>Pull requests that modify files in <code>modules/**</code></li> <li>Manual workflow dispatch</li> </ul> <p>Test Discovery:</p> <ul> <li>Finds all modules with a <code>tests/</code> directory</li> <li>Runs <code>terraform test</code> from the module root</li> </ul> <p>Note: Integration tests require AWS credentials configured in the CI environment via GitHub Secrets or OIDC authentication.</p>"},{"location":"modules/perforce/tests/integration/index.html#comparison-with-unit-tests","title":"Comparison with Unit Tests","text":"Aspect Unit Tests Integration Tests Speed Fast (seconds) Slow (minutes) AWS Access Not required Required Infrastructure Mock providers Real AWS resources Purpose Test conditional logic Test example deployments When to Run Every commit Before releases Cost Free AWS costs (minimal with plan-only)"},{"location":"modules/perforce/tests/integration/index.html#maintenance","title":"Maintenance","text":""},{"location":"modules/perforce/tests/integration/index.html#adding-new-integration-tests","title":"Adding New Integration Tests","text":"<ol> <li>Create a new <code>.tftest.hcl</code> file in this directory</li> <li>Add required SSM parameters to <code>setup/ssm.tf</code></li> <li>Reference the appropriate example deployment</li> <li>Update this README with test details</li> </ol>"},{"location":"modules/perforce/tests/integration/index.html#updating-ssm-parameters","title":"Updating SSM Parameters","text":"<p>When test configuration changes:</p> <ol> <li>Update SSM parameter values in your AWS account</li> <li>Update <code>setup/ssm.tf</code> if new parameters are needed</li> <li>Update integration test files to use new parameters</li> </ol>"},{"location":"modules/perforce/tests/integration/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"modules/perforce/tests/integration/index.html#parameter-not-found-errors","title":"\"Parameter not found\" errors","text":"<ul> <li>Verify SSM parameters exist in your AWS account</li> <li>Check parameter names match exactly (case-sensitive)</li> <li>Ensure AWS credentials have permission to read SSM parameters</li> </ul>"},{"location":"modules/perforce/tests/integration/index.html#access-denied-errors","title":"\"Access denied\" errors","text":"<ul> <li>Verify your IAM role/user has required permissions</li> <li>Check that the AWS region is correct</li> <li>Ensure AWS credentials are properly configured</li> </ul>"},{"location":"modules/perforce/tests/integration/index.html#example-not-found-errors","title":"Example not found errors","text":"<ul> <li>Verify the example path is correct relative to module root</li> <li>Check that the example directory contains valid Terraform files</li> <li>Ensure you're running tests from the module root directory</li> </ul>"},{"location":"modules/perforce/tests/integration/index.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Unit Tests - Mock-based tests for conditional logic</li> <li>Module Examples - Example deployments referenced by tests</li> <li>Main Module README - Module usage and configuration</li> </ul>"},{"location":"modules/perforce/tests/unit/index.html","title":"Perforce Module Unit Tests","text":"<p>This directory contains mock-based unit tests for the Perforce wrapper module. These tests validate the conditional logic and resource creation without requiring AWS credentials or deploying actual infrastructure.</p>"},{"location":"modules/perforce/tests/unit/index.html#overview","title":"Overview","text":"<p>The Perforce module is a wrapper module that orchestrates the deployment of three submodules:</p> <ul> <li>P4 Server (<code>modules/p4-server/</code>) - Perforce Helix Core version control server</li> <li>P4 Auth (<code>modules/p4-auth/</code>) - Perforce authentication service</li> <li>P4 Code Review (<code>modules/p4-code-review/</code>) - Perforce Swarm code review platform</li> </ul> <p>Unit tests ensure that:</p> <ul> <li>Submodules are created only when their configuration is provided</li> <li>Shared resources (ECS cluster, load balancers, Route53) are created correctly</li> <li>Various combinations of submodules work without conflicts</li> </ul>"},{"location":"modules/perforce/tests/unit/index.html#test-files","title":"Test Files","text":""},{"location":"modules/perforce/tests/unit/index.html#01_conditional_creationtftesthcl","title":"<code>01_conditional_creation.tftest.hcl</code>","text":"<p>Purpose: Validates that submodules are conditionally created based on configuration</p> <p>Test Scenarios (8 total):</p> <ol> <li><code>no_submodules</code> - No configuration provided, no resources created</li> <li><code>p4_server_only</code> - Only P4 Server deployed</li> <li><code>p4_auth_only</code> - Only P4 Auth deployed</li> <li><code>p4_code_review_only</code> - Only P4 Code Review deployed (note: depends on P4 Server for credentials)</li> <li><code>server_and_auth</code> - P4 Server + P4 Auth combination</li> <li><code>server_and_code_review</code> - P4 Server + P4 Code Review combination</li> <li><code>full_stack</code> - All three submodules deployed together</li> <li><code>full_stack_existing_ecs_cluster</code> - Full stack using an existing ECS cluster</li> </ol> <p>Key Validations:</p> <ul> <li><code>length(module.p4_server)</code> equals 0 or 1 based on configuration</li> <li><code>length(module.p4_auth)</code> equals 0 or 1 based on configuration</li> <li><code>length(module.p4_code_review)</code> equals 0 or 1 based on configuration</li> <li>ECS cluster creation logic based on web service deployment</li> </ul>"},{"location":"modules/perforce/tests/unit/index.html#02_shared_resourcestftesthcl","title":"<code>02_shared_resources.tftest.hcl</code>","text":"<p>Purpose: Validates shared resource creation logic</p> <p>Test Scenarios (6 total):</p> <ol> <li><code>ecs_cluster_auth_only</code> - ECS cluster created for Auth service</li> <li><code>ecs_cluster_code_review_only</code> - ECS cluster created for Code Review service</li> <li><code>ecs_cluster_shared</code> - Single ECS cluster shared by both services</li> <li><code>route53_private_zone</code> - Private hosted zone and DNS records</li> <li><code>load_balancer_access_logs</code> - S3 bucket for LB access logs</li> <li><code>no_ecs_cluster_server_only</code> - No ECS cluster when only P4 Server is deployed</li> </ol> <p>Key Validations:</p> <ul> <li><code>local.create_shared_ecs_cluster</code> logic correctness</li> <li>Route53 zone and record configurations</li> <li>S3 bucket creation for access logs</li> <li>Load balancer configurations</li> </ul>"},{"location":"modules/perforce/tests/unit/index.html#running-tests","title":"Running Tests","text":""},{"location":"modules/perforce/tests/unit/index.html#run-all-unit-tests","title":"Run All Unit Tests","text":"<p>From the module root directory:</p> <pre><code>cd /path/to/modules/perforce\nterraform test\n</code></pre>"},{"location":"modules/perforce/tests/unit/index.html#run-specific-test-file","title":"Run Specific Test File","text":"<pre><code>terraform test -filter=tests/unit/01_conditional_creation.tftest.hcl\n</code></pre>"},{"location":"modules/perforce/tests/unit/index.html#run-specific-test-scenario","title":"Run Specific Test Scenario","text":"<pre><code>terraform test -filter=tests/unit/01_conditional_creation.tftest.hcl -verbose\n</code></pre>"},{"location":"modules/perforce/tests/unit/index.html#mock-providers","title":"Mock Providers","text":"<p>All test files use mock providers to simulate AWS resources without making actual API calls:</p> <ul> <li>aws - Mocks AWS provider with data sources for region, caller identity, ELB service account, ECS cluster, IAM policy documents, and AMI</li> <li>awscc - Mocks AWS Cloud Control provider</li> <li>random - Mocks random provider</li> <li>null - Mocks null provider</li> <li>local - Mocks local provider</li> <li>netapp-ontap - Mocks NetApp ONTAP provider (for FSxN storage)</li> </ul>"},{"location":"modules/perforce/tests/unit/index.html#benefits-of-unit-tests","title":"Benefits of Unit Tests","text":"<p>\u2705 Fast execution - No actual resources created, tests complete in seconds \u2705 No AWS credentials required - Mock providers eliminate need for authentication \u2705 Safe to run anywhere - No risk of creating unexpected AWS resources \u2705 Comprehensive coverage - Tests all conditional logic paths \u2705 Easy to debug - Clear assertions with descriptive error messages \u2705 CI/CD friendly - Can run in GitHub Actions without AWS access</p>"},{"location":"modules/perforce/tests/unit/index.html#test-maintenance","title":"Test Maintenance","text":"<p>When modifying the Perforce module:</p> <ol> <li>Adding new submodules - Add test scenarios to <code>01_conditional_creation.tftest.hcl</code></li> <li>Adding shared resources - Add test scenarios to <code>02_shared_resources.tftest.hcl</code></li> <li>Changing conditional logic - Update assertions to match new behavior</li> <li>Adding required variables - Update all test scenarios with new variables</li> </ol>"},{"location":"modules/perforce/tests/unit/index.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Integration Tests - Tests that deploy actual infrastructure</li> <li>Module README - Main module documentation</li> <li>Terraform Testing Documentation</li> </ul>"},{"location":"modules/teamcity/index.html","title":"TeamCity Module","text":"<p>TeamCity is a user-friendly continuous integration (CI) server for developers and build engineers created by JetBrains. This module deploys a TeamCity server on AWS Elastic Container Service.</p> <p>The TeamCity server relies on shared file system for persistent storage of configuration, build results, and current operation files as well as a SQL database to store build history, user data, build results, and runtime data. This module provides these services by provisioning an Amazon Elastic File System and an Amazon Amazon Aurora Serverless V2 cluster running the PostgreSQL engine.</p>"},{"location":"modules/teamcity/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/teamcity/index.html#examples","title":"Examples","text":"<p>For example configurations, please see the examples.</p>"},{"location":"modules/teamcity/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.9 aws ~&gt; 6.6 random ~&gt; 3.7"},{"location":"modules/teamcity/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 random ~&gt; 3.7"},{"location":"modules/teamcity/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/teamcity/index.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.teamcity_agent resource aws_cloudwatch_log_group.teamcity_log_group resource aws_db_subnet_group.teamcity_db_subnet_group resource aws_ecs_cluster.teamcity_cluster resource aws_ecs_service.teamcity resource aws_ecs_service.teamcity_agent resource aws_ecs_task_definition.teamcity_agent resource aws_ecs_task_definition.teamcity_task_definition resource aws_efs_access_point.teamcity_efs_data_access_point resource aws_efs_file_system.teamcity_efs_file_system resource aws_efs_mount_target.teamcity_efs_mount_target resource aws_iam_policy.teamcity_agent_default_policy resource aws_iam_policy.teamcity_default_policy resource aws_iam_policy.teamcity_execution_database_policy resource aws_iam_role.teamcity_agent_default_role resource aws_iam_role.teamcity_agent_task_execution_role resource aws_iam_role.teamcity_default_role resource aws_iam_role.teamcity_task_execution_role resource aws_iam_role_policy_attachment.teamcity_agent_default_role_policy_attachment resource aws_iam_role_policy_attachment.teamcity_agent_task_execution_default_policy resource aws_iam_role_policy_attachment.teamcity_default_role resource aws_iam_role_policy_attachment.teamcity_task_execution_database_policy resource aws_iam_role_policy_attachment.teamcity_task_execution_default_policy resource aws_lb.teamcity_external_lb resource aws_lb_listener.teamcity_listener resource aws_lb_target_group.teamcity_target_group resource aws_rds_cluster.teamcity_db_cluster resource aws_rds_cluster_instance.teamcity_db_cluster_instance resource aws_s3_bucket.teamcity_alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_security_group.teamcity_agent_sg resource aws_security_group.teamcity_alb_sg resource aws_security_group.teamcity_db_sg resource aws_security_group.teamcity_efs_sg resource aws_security_group.teamcity_service_sg resource aws_service_discovery_http_namespace.teamcity resource aws_vpc_security_group_egress_rule.alb_outbound_service resource aws_vpc_security_group_egress_rule.service_outbound_internet resource aws_vpc_security_group_egress_rule.teamcity_agent_outbound resource aws_vpc_security_group_ingress_rule.service_db resource aws_vpc_security_group_ingress_rule.service_efs resource aws_vpc_security_group_ingress_rule.service_inbound_alb resource aws_vpc_security_group_ingress_rule.teamcity_agent_inbound resource random_string.teamcity_alb_access_logs_bucket_suffix resource aws_ecs_cluster.teamcity_cluster data source aws_efs_file_system.efs_file_system data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.teamcity_agent_default_policy data source aws_iam_policy_document.teamcity_default_policy data source aws_iam_policy_document.teamcity_execution_database_policy data source aws_region.current data source"},{"location":"modules/teamcity/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required service_subnets The subnets in which the TeamCity server service will be deployed <code>list(string)</code> n/a yes vpc_id The ID of the VPC in which the service will be deployed <code>string</code> n/a yes agent_log_group_retention_in_days n/a <code>number</code> <code>7</code> no alb_certificate_arn The ARN of the SSL certificate to use for the ALB <code>string</code> <code>null</code> no alb_subnets The subnets in which the ALB will be deployed <code>list(string)</code> <code>[]</code> no aurora_instance_count Number of instances to provision for the Aurora cluster <code>number</code> <code>2</code> no aurora_skip_final_snapshot Flag for whether a final snapshot should be created when the cluster is destroyed. <code>bool</code> <code>true</code> no build_farm_config Map of build agent configurations where each key is the agent name and the value defines:- image: Container image for the build agent- desired_count: Number of agent instances to run- cpu: CPU units to allocate (1024 = 1 vCPU)- memory: Memory in MiB to allocate- environment: Optional map of custom environment variables for non-sensitive configuration- ephemeral_storage_gib: Optional ephemeral storage size in GiB (defaults to 20 GiB) <pre>map(object({    image                 = string    desired_count         = number    cpu                   = number    memory                = number    environment           = optional(map(string), {})    ephemeral_storage_gib = optional(number, 20)  }))</pre> <code>{}</code> no cluster_name The name of the ECS cluster to deploy TeamCity to. <code>string</code> <code>null</code> no container_cpu The number of CPU units to allocate to the TeamCity server container <code>number</code> <code>1024</code> no container_memory The number of MB of memory to allocate to the TeamCity server container <code>number</code> <code>4096</code> no container_name The name of the TeamCity server container <code>string</code> <code>\"teamcity\"</code> no container_port The port on which the TeamCity server container listens <code>number</code> <code>8111</code> no create_external_alb Set this flag to true to create an external load balancer for TeamCity. <code>bool</code> <code>true</code> no database_connection_string The database connection string for TeamCity <code>string</code> <code>null</code> no database_master_password The master password for the database <code>string</code> <code>null</code> no database_master_username The master username for the database <code>string</code> <code>null</code> no debug Set this flag to enable ECS execute permissions on the TeamCity server container and force new service deployments on Terraform apply. <code>bool</code> <code>false</code> no desired_container_count The desired number of containers running TeamCity server. <code>number</code> <code>1</code> no efs_access_point_id The ID of the EFS access point to use for the TeamCity data volume. <code>string</code> <code>null</code> no efs_encryption_enabled Set this flag to true to enable EFS encryption. <code>bool</code> <code>true</code> no efs_id The ID of the EFS file system to use for the TeamCity service. <code>string</code> <code>null</code> no enable_teamcity_alb_access_logs Enables access logging for the TeamCity ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_teamcity_alb_deletion_protection Enables deletion protection for the TeamCity ALB. Defaults to true. <code>bool</code> <code>false</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no name The name applied to resources in the TeamCity module <code>string</code> <code>\"teamcity\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"iac-management\": \"CGD-Toolkit\",  \"iac-module\": \"TeamCity\",  \"iac-provider\": \"Terraform\"}</pre> no teamcity_alb_access_logs_bucket ID of the S3 bucket for TeamCity ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no teamcity_alb_access_logs_prefix Log prefix for TeamCity ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no teamcity_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for TeamCity. <code>string</code> <code>365</code> no teamcity_efs_performance_mode The performance mode of the EFS file system used by the TeamCity service. Defaults to general purpose. <code>string</code> <code>\"generalPurpose\"</code> no teamcity_efs_throughput_mode The throughput mode of the EFS file system used by the TeamCity service. Defaults to bursting. <code>string</code> <code>\"bursting\"</code> no"},{"location":"modules/teamcity/index.html#outputs","title":"Outputs","text":"Name Description external_alb_dns_name DNS endpoint of Application Load Balancer (ALB) external_alb_zone_id Zone ID for internet facing load balancer security_group_id The default security group of your Teamcity service. teamcity_cluster_id The ID of the ECS cluster"},{"location":"modules/unity/accelerator/index.html","title":"Unity Accelerator Module","text":"<p>Unity Accelerator is a caching service that speeds up project operations in Unity by storing and sharing asset cache data among team members, significantly reducing build times and asset imports by allowing team members to reuse previously processed assets instead of having to process the same assets independently.</p> <p>This Unity Accelerator deployment uses an Elastic Container Service cluster for task deployment, with Amazon Elastic File System providing persistent storage for configurations and cache. Access is managed through two load balancers: an Application Load Balancer for secure, password-protected web dashboard access via HTTPS, and a Network Load Balancer for efficient cache-related protobuf traffic.</p> <p>A Secrets Manager password entry, to be used as the Unity Accelerator's web dashboard password, is required beforehand. Password must be stored as a plaintext secret, not as key/value JSON secret, and the password's ARN must be provided as the value for the <code>unity_accelerator_dashboard_password_arn</code> variable.</p>"},{"location":"modules/unity/accelerator/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/unity/accelerator/index.html#examples","title":"Examples","text":"<p>For example configurations, please see the examples.</p>"},{"location":"modules/unity/accelerator/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.9 aws ~&gt; 6.6 awscc ~&gt; 1.51 random ~&gt; 3.7"},{"location":"modules/unity/accelerator/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 awscc ~&gt; 1.51 random ~&gt; 3.7"},{"location":"modules/unity/accelerator/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/unity/accelerator/index.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.unity_accelerator_log_group resource aws_ecs_cluster.unity_accelerator_cluster resource aws_ecs_service.unity_accelerator resource aws_ecs_task_definition.unity_accelerator_task_definition resource aws_efs_access_point.unity_accelerator_efs_data_access_point resource aws_efs_file_system.unity_accelerator_efs resource aws_efs_mount_target.unity_accelerator_efs_mount_target resource aws_iam_policy.cloudwatch_logs_policy resource aws_iam_policy.secret_access_policy resource aws_iam_policy.unity_accelerator_default_policy resource aws_iam_role.unity_accelerator_default_role resource aws_iam_role.unity_accelerator_task_execution_role resource aws_iam_role_policy_attachment.cloudwatch_logs_policy_attachment resource aws_iam_role_policy_attachment.task_execution_role_secret_policy resource aws_iam_role_policy_attachment.unity_accelerator_default_policy_attachment resource aws_iam_role_policy_attachment.unity_accelerator_task_execution_role_policy_attachment resource aws_lb.unity_accelerator_external_alb resource aws_lb.unity_accelerator_external_nlb resource aws_lb_listener.unity_accelerator_cache_listener resource aws_lb_listener.unity_accelerator_https_dashboard_listener resource aws_lb_listener.unity_accelerator_https_dashboard_redirect resource aws_lb_target_group.unity_accelerator_cache_target_group resource aws_lb_target_group.unity_accelerator_dashboard_target_group resource aws_s3_bucket.unity_accelerator_lb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.lb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_security_group.unity_accelerator_alb_sg resource aws_security_group.unity_accelerator_efs_sg resource aws_security_group.unity_accelerator_service_sg resource aws_security_group.vpc_endpoint_sg resource aws_vpc_endpoint.ec2messages_vpce resource aws_vpc_endpoint.ssm_vpce resource aws_vpc_endpoint.ssmmessages_vpce resource aws_vpc_security_group_egress_rule.unity_accelerator_alb_egress_service_80 resource aws_vpc_security_group_egress_rule.unity_accelerator_service_egress_all resource aws_vpc_security_group_ingress_rule.service_efs resource aws_vpc_security_group_ingress_rule.unity_accelerator_ingress_to_vpce resource aws_vpc_security_group_ingress_rule.unity_accelerator_service_ingress_from_alb_80 resource aws_vpc_security_group_ingress_rule.unity_accelerator_service_ingress_from_nlb_10080 resource aws_vpc_security_group_ingress_rule.unity_accelerator_service_ingress_from_nlb_80 resource aws_vpc_security_group_ingress_rule.vpc_endpoint_https resource awscc_secretsmanager_secret.dashboard_password_arn resource awscc_secretsmanager_secret.dashboard_username_arn resource random_string.unity_accelerator_lb_access_logs_bucket_suffix resource aws_caller_identity.current data source aws_ecs_cluster.unity_accelerator_cluster data source aws_efs_file_system.efs_file_system data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_lb_write data source aws_iam_policy_document.cloudwatch_logs_policy data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.unity_accelerator_default_policy data source aws_region.current data source aws_subnet.nlb_subnets data source"},{"location":"modules/unity/accelerator/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required service_subnets The subnets in which the Unity Accelerator service will be deployed. <code>list(string)</code> n/a yes vpc_id The ID of the VPC in which the service will be deployed. <code>string</code> n/a yes alb_certificate_arn The ARN of the SSL certificate to use for the Application Load Balancer. <code>string</code> <code>null</code> no alb_is_internal Set this flag to determine whether the Application Load Balancer to create is internal (true) or external (false). Value is ignored if no ALB is created. <code>bool</code> <code>false</code> no cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Unity Accelerator. <code>string</code> <code>365</code> no cluster_name The name of the ECS cluster to deploy Unity Accelerator to. <code>string</code> <code>null</code> no container_cpu The number of CPU units to allocate to the Unity Accelerator container. <code>number</code> <code>1024</code> no container_memory The number of MB of memory to allocate to the Unity Accelerator container. <code>number</code> <code>4096</code> no container_name The name of the Unity Accelerator container. <code>string</code> <code>\"unity-accelerator\"</code> no create_alb Set this flag to true to create an Application Load Balancer for the Unity Accelerator dashboard. <code>bool</code> <code>true</code> no create_nlb Set this flag to true to create an external Network Load Balancer for the Unity Accelerator protobuf traffic. <code>bool</code> <code>true</code> no debug Set this flag to enable ECS execute permissions on the Unity Accelerator container and force new service deployments on Terraform apply. <code>bool</code> <code>true</code> no efs_access_point_id The ID of the EFS access point to use for the Unity Accelerator data volume. <code>string</code> <code>null</code> no efs_encryption_enabled Set this flag to true to enable EFS encryption. <code>bool</code> <code>true</code> no efs_id The ID of the EFS file system to use for the Unity Accelerator service. <code>string</code> <code>null</code> no efs_performance_mode The performance mode of the EFS file system used by the Unity Accelerator service. Defaults to general purpose. <code>string</code> <code>\"generalPurpose\"</code> no efs_throughput_mode The throughput mode of the EFS file system used by the Unity Accelerator service. Defaults to bursting. <code>string</code> <code>\"bursting\"</code> no enable_unity_accelerator_lb_access_logs Enables access logging for the Application Load Balancer and Network Load Balancer used by Unity Accelerator. Defaults to true. <code>bool</code> <code>true</code> no enable_unity_accelerator_lb_deletion_protection Enables deletion protection for the Unity Accelerator Application Load Balancer and Network Load Balancer. Defaults to true. <code>bool</code> <code>true</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no lb_subnets The subnets in which the Application Load Balancer and Network Load Balancer will be deployed. <code>list(string)</code> <code>[]</code> no name The name applied to resources in the Unity Accelerator module. <code>string</code> <code>\"unity-accelerator\"</code> no nlb_is_internal Set this flag to determine whether the Network Load Balancer to create is internal (true) or external (false). Value is ignored if no NLB is created. <code>bool</code> <code>false</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"iac-management\": \"CGD-Toolkit\",  \"iac-module\": \"UnityAccelerator\",  \"iac-provider\": \"Terraform\"}</pre> no unity_accelerator_alb_access_logs_prefix Log prefix for Unity Accelerator Application Load Balancer access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no unity_accelerator_dashboard_password_arn ARN of the AWS Secrets Manager secret containing the Unity Accelerator web dashboard password. Password must be the only value and stored as text, not as key/value JSON. If not passed, one will be created randomly. <code>string</code> <code>null</code> no unity_accelerator_dashboard_username_arn ARN of the AWS Secrets Manager secret containing the Unity Accelerator web dashboard username. Username must be the only value and stored as text, not as key/value JSON. If not passed, one will be created and defaulted to 'uauser'. <code>string</code> <code>null</code> no unity_accelerator_debug_mode Enables debug output for the Unity Accelerator service. <code>string</code> <code>\"false\"</code> no unity_accelerator_docker_image Docker image to use for Unity Accelerator. <code>string</code> <code>\"unitytechnologies/accelerator:latest\"</code> no unity_accelerator_lb_access_logs_bucket ID of the S3 bucket for Unity Accelerator Application Load Balancer and Network Load Balancer access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no unity_accelerator_log_stdout When true, outputs logs to stdout only. When false, writes logs to the persist directory. <code>string</code> <code>\"true\"</code> no unity_accelerator_nlb_access_logs_prefix Log prefix for Unity Accelerator Network Load Balancer access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no"},{"location":"modules/unity/accelerator/index.html#outputs","title":"Outputs","text":"Name Description alb_dns_name DNS endpoint of Application Load Balancer (ALB) alb_security_group_id ID of the Application Load Balancer's (ALB) security group alb_zone_id Zone ID for Application Load Balancer (ALB) nlb_dns_name DNS endpoint of Network Load Balancer (NLB) nlb_zone_id Zone ID for Network Load Balancer (NLB) unity_accelerator_dashboard_password_arn AWS Secrets Manager secret's ARN containing the Unity Accelerator web dashboard's username. unity_accelerator_dashboard_username_arn AWS Secrets Manager secret's ARN containing the Unity Accelerator web dashboard's password."},{"location":"modules/unity/floating-license-server/index.html","title":"Unity Licensing Server Module","text":"<p>The Unity Licensing Server is an application that manages a pool of floating licenses for the Unity Editor within an organization. It functions as a centralized system, allowing administrators to assign and track Unity Editor licenses to users, providing greater control and flexibility compared to node-locked or named-user licenses. The server uses an HTTP or HTTPS connection to communicate with the Unity Licensing Client on user machines and offers a dashboard for administrators to monitor license usage and server health. The Unity Licensing Client is included with the Unity Editor (2019.4 or later) and the Unity Hub (3.1 or later). Floating licensing is available exclusively for Enterprise plans subscribers.</p> <p>While the Unity Licensing Server deployment has been simplified and streamlined in this module, it remains a multi-step process requiring manual intervention. A local zip file of the Unity Licensing Server is required when configuring and running the module. It can be downloaded from the <code>Organizations</code> section in the Unity ID portal. For simplicity and cost optimization, this module requires the Linux version of the executable.</p> <p>The deployment uses an Amazon EC2 instance to run the Unity Licensing Server. An Amazon Elastic Network Interface (ENI) is attached to the instance to provide a static private (and public if configured) IP to separate network settings from the compute instance, as the Unity Licensing Server binds the license to the machine's identity, which includes the following information:</p> <ul> <li>The server's MAC address</li> <li>The server's operating system</li> <li>The number of processor cores on the server</li> <li>The name of the server</li> </ul> <p>Note that if the Licensing Server needs to be hosted by a new machine, a support ticket will need to be submitted to Unity Support to revoke the Licensing Server so a new one can be configured.</p> <p>The Licensing Server provides an admin dashboard. The username is <code>admin</code> and cannot be changed. The password is generated securely by the module and stored in AWS Secrets Manager.</p> <p>An Amazon S3 bucket is created and mounted to the instance. This bucket will be used to provide the user with files created by the Unity Licensing Server, as well as to provide files to the Licensing Server that will need to be imported in later steps.</p> <p>Upon deployment, two created files will be copied to the S3 bucket:</p> <ul> <li>The server registration request file <code>server-registration-request.xml</code>, which contains machine binding information from the server.</li> <li>The services configuration file <code>services-config.json</code>, which must be deployed to all client computers intending to use the Licensing Server.</li> <li> <p>Note that this file must be placed in the following location:</p> OS Path Windows <code>%PROGRAMDATA%\\Unity\\config\\</code> Linux <code>/usr/share/unity3d/config/</code> Mac <code>/Library/Application Support/Unity/config/</code> </li> </ul> <p>The <code>server-registration-request.xml</code> will need to be uploaded to the Unity ID portal (where the Licensing Server executable was downloaded from) to register the server. Once successful, download the licenses zip file. Without renaming the file, upload it to the S3 bucket. A background process will detect the uploaded licenses file, and import them into the Unity Licensing Server, finishing the process.</p>"},{"location":"modules/unity/floating-license-server/index.html#deployment-architectures","title":"Deployment Architectures","text":"<p> Simple architecture where clients connect directly to an EC2 instance in a public (or private) subnet through a primary ENI with static public/private IPs. The EC2 instance hosts the Unity Licensing Server and connects to AWS Secrets Manager and Amazon S3 (mounted via s3fs).</p> <p> Architecture providing enhanced security through network isolation where client traffic is routed through Amazon Route 53 to an Application Load Balancer before reaching the Unity Licensing Server.</p>"},{"location":"modules/unity/floating-license-server/index.html#examples","title":"Examples","text":"<p>For example configurations, please see the examples.</p>"},{"location":"modules/unity/floating-license-server/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.9 aws ~&gt; 6.0 awscc ~&gt; 1.51 local ~&gt; 2.4 null ~&gt; 3.2 random ~&gt; 3.7"},{"location":"modules/unity/floating-license-server/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.0 awscc ~&gt; 1.51 local ~&gt; 2.4 null ~&gt; 3.2 random ~&gt; 3.7"},{"location":"modules/unity/floating-license-server/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/unity/floating-license-server/index.html#resources","title":"Resources","text":"Name Type aws_eip.unity_license_server_eip resource aws_iam_instance_profile.ec2_profile resource aws_iam_role.ec2_access_role resource aws_iam_role_policy.access_policy resource aws_iam_role_policy_attachment.ssm_policy resource aws_instance.unity_license_server resource aws_lb.unity_license_server_alb resource aws_lb_listener.unity_license_server_https_dashboard_listener resource aws_lb_listener.unity_license_server_https_dashboard_redirect resource aws_lb_target_group.unity_license_server_tg resource aws_lb_target_group_attachment.unity_license_server resource aws_network_interface.unity_license_server_eni resource aws_s3_bucket.alb_access_logs_bucket resource aws_s3_bucket.unity_license_server_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.lb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_s3_bucket_public_access_block.block_public_access resource aws_s3_bucket_server_side_encryption_configuration.bucket_encryption resource aws_s3_object.unity_license_file resource aws_security_group.unity_license_server_alb_sg resource aws_security_group.unity_license_server_sg resource aws_vpc_security_group_egress_rule.unity_license_server_alb_egress_service_8080 resource aws_vpc_security_group_egress_rule.unity_license_server_egress_all resource aws_vpc_security_group_ingress_rule.unity_license_server_ingress_from_alb_8080 resource awscc_secretsmanager_secret.admin_password_arn resource null_resource.generate_presigned_urls resource null_resource.wait_for_user_data resource random_string.alb_access_logs_bucket_suffix resource aws_ami.ubuntu_latest data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_instance.unity_license_server data source aws_network_interface.existing_eni data source local_file.config_url data source local_file.registration_url data source"},{"location":"modules/unity/floating-license-server/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required unity_license_server_file_path Local path to the Linux version of the Unity Floating License Server zip file. <code>string</code> n/a yes vpc_id The ID of the VPC in which the Unity Floating License Server will be deployed. <code>string</code> n/a yes vpc_subnet The subnet where the EC2 instance running the Unity Floating License Server will be deployed. <code>string</code> n/a yes add_eni_public_ip If true and \"existing_eni_id\" is not provided, an Elastic IP (EIP) will be created and associated with the newly created Elastic Network Interface (ENI) to be used with the Unity Floating License Server. If \"existing_eni_id\" is provided, this variable is ignored and no new EIP will be added to the provided ENI. <code>bool</code> <code>true</code> no alb_access_logs_bucket ID of the S3 bucket for Application Load Balancer access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no alb_access_logs_prefix Log prefix for Unity License Server Application Load Balancer access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no alb_certificate_arn The ARN of the SSL certificate to use for the Application Load Balancer. <code>string</code> <code>null</code> no alb_is_internal Set this flag to determine whether the Application Load Balancer to create is internal (true) or external (false). Value is ignored if no ALB is created. <code>bool</code> <code>false</code> no alb_subnets The subnets in which the Application Load Balancer will be deployed. <code>list(string)</code> <code>[]</code> no create_alb Set this flag to true to create an Application Load Balancer for the Unity License Server dashboard. <code>bool</code> <code>true</code> no enable_alb_access_logs Enables access logging for the Application Load Balancer used by Unity License Server. Defaults to true. <code>bool</code> <code>true</code> no enable_alb_deletion_protection Enables deletion protection for the Application Load Balancer. Defaults to true. <code>bool</code> <code>true</code> no enable_instance_detailed_monitoring Enables detailed monitoring for the instance by increasing the frequency of metric collection from 5-minute intervals to 1-minute intervals in CloudWatch to provide more granular data. Note this will result in increased cost. <code>bool</code> <code>false</code> no enable_instance_termination_protection If true, enables EC2 instance termination protection from AWS APIs and console. <code>bool</code> <code>true</code> no existing_eni_id ID of an existing Elastic Network Interface (ENI) to use for the EC2 instance running the Unity Floating License Server, as its registration will be binded to it. If not provided, a new ENI will be created. <code>string</code> <code>null</code> no name The name applied to resources in the Unity Floating License Server module. <code>string</code> <code>\"unity-license-server\"</code> no tags Tags to apply to resources created by this module. <code>map(any)</code> <pre>{  \"environment\": \"Dev\",  \"iac-management\": \"CGD-Toolkit\",  \"iac-module\": \"UnityFloatingLicenseServer\",  \"iac-provider\": \"Terraform\"}</pre> no unity_license_server_admin_password_arn ARN of the AWS Secrets Manager secret containing the Unity Floating License Server admin dashboard password. Password must be the only value and stored as text, not as key/value JSON. If not passed, one will be created randomly. Password must be between 8-12 characters. <code>string</code> <code>null</code> no unity_license_server_bucket_name Name of the Unity Floating License Server-specific S3 bucket to create. <code>string</code> <code>\"unity-license-server-\"</code> no unity_license_server_instance_ami_id The Ubuntu-based AMI ID to use in the EC2 instance running the Unity Floating License Server. Defaults to the latest Ubuntu Server 24.04 LTS AMI. <code>string</code> <code>null</code> no unity_license_server_instance_ebs_size The size of the EBS volume in GB. <code>string</code> <code>\"20\"</code> no unity_license_server_instance_type The instance type to use for the Unity Floating License Server. Defaults to t3.small. <code>string</code> <code>\"t3.small\"</code> no unity_license_server_name Name of the Unity Floating License Server. <code>string</code> <code>\"UnityLicenseServer\"</code> no unity_license_server_port Port the Unity Floating License Server will listen on (between 1025 and 65535). Defaults to 8080. <code>string</code> <code>\"8080\"</code> no"},{"location":"modules/unity/floating-license-server/index.html#outputs","title":"Outputs","text":"Name Description alb_dns_name DNS endpoint of Application Load Balancer (ALB). alb_security_group_id ID of the Application Load Balancer's (ALB) security group. alb_zone_id Zone ID for Application Load Balancer (ALB). created_unity_license_server_security_group_id Id of the security group created by the script, for the Unity License Server instance. Null if an ENI was provided externally instead of created through the script. dashboard_password_secret_arn ARN of the secret containing the dashboard password. eni_id Elastic Network ID (ENI) used when binding the Unity Floating License Server. instance_private_ip The EC2 instance's private IP address. instance_public_ip The resulting EC2 instance's public IP, if configured. registration_request_filename Filename for the server registration request file. registration_request_presigned_url Presigned URL for downloading the server registration request file (valid for 1 hour). services_config_filename Filename for the services config file. services_config_presigned_url Presigned URL for downloading the services configuration file (valid for 1 hour). unity_license_server_port Port the Unity Floating License Server will listen on. unity_license_server_s3_bucket S3 bucket name used by the Unity License Server service."},{"location":"modules/unreal/horde/index.html","title":"Unreal Horde","text":"<p>Unreal Engine Horde is a set of services supporting workflows Epic uses to develop Fortnite, Unreal Engine, and other titles. This module deploys the Unreal Engine Horde server on AWS Elastic Container Service using the image available from the Epic Games Github organization (requires Epic Games organization membership). Unreal Engine Horde relies on a Redis cache and a MongoDB compatible database. This module provides these services by provisioning an Amazon Elasticache with Redis OSS Compatibility cluster and an Amazon DocumentDB cluster.</p> <p>Check out this video from Unreal Fest 2024 to learn more about the Unreal Horde module:</p> <p></p>"},{"location":"modules/unreal/horde/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/unreal/horde/index.html#prerequisites","title":"Prerequisites","text":"<p>Unreal Engine Horde is only available through the Epic Games Github organization's package registry or the Unreal Engine source code. In order to get access to this software you will need to join the Epic Games organization on Github and accept the Unreal Engine EULA.</p>"},{"location":"modules/unreal/horde/index.html#examples","title":"Examples","text":"<p>For example configurations, please see the examples.</p>"},{"location":"modules/unreal/horde/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws ~&gt; 6.6 random ~&gt; 3.7"},{"location":"modules/unreal/horde/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 random ~&gt; 3.7"},{"location":"modules/unreal/horde/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/unreal/horde/index.html#resources","title":"Resources","text":"Name Type aws_autoscaling_group.unreal_horde_agent_asg resource aws_cloudwatch_log_group.unreal_horde_log_group resource aws_docdb_cluster.horde resource aws_docdb_cluster_instance.horde resource aws_docdb_cluster_parameter_group.horde resource aws_docdb_subnet_group.horde resource aws_ecs_cluster.unreal_horde_cluster resource aws_ecs_service.unreal_horde resource aws_ecs_task_definition.unreal_horde_task_definition resource aws_elasticache_cluster.horde resource aws_elasticache_replication_group.horde resource aws_elasticache_subnet_group.horde resource aws_iam_instance_profile.unreal_horde_agent_instance_profile resource aws_iam_policy.horde_agents_ec2_policy resource aws_iam_policy.horde_agents_s3_policy resource aws_iam_policy.unreal_horde_default_policy resource aws_iam_policy.unreal_horde_elasticache_policy resource aws_iam_policy.unreal_horde_recycle_policy resource aws_iam_policy.unreal_horde_secrets_manager_policy resource aws_iam_role.unreal_horde_agent_default_role resource aws_iam_role.unreal_horde_default_role resource aws_iam_role.unreal_horde_task_execution_role resource aws_iam_role_policy_attachment.unreal_horde_agent_policy_attachments resource aws_iam_role_policy_attachment.unreal_horde_agents_ec2_policy resource aws_iam_role_policy_attachment.unreal_horde_agents_s3_policy resource aws_iam_role_policy_attachment.unreal_horde_default_policy_attachment resource aws_iam_role_policy_attachment.unreal_horde_elasticache_policy_attachment resource aws_iam_role_policy_attachment.unreal_horde_recycle_attachment resource aws_iam_role_policy_attachment.unreal_horde_secrets_manager_policy_attachment resource aws_iam_role_policy_attachment.unreal_horde_task_execution_policy_attachment resource aws_launch_template.unreal_horde_agent_template resource aws_lb.unreal_horde_external_alb resource aws_lb.unreal_horde_internal_alb resource aws_lb_listener.unreal_horde_external_alb_http_listener resource aws_lb_listener.unreal_horde_external_alb_https_listener resource aws_lb_listener.unreal_horde_internal_alb_http_listener resource aws_lb_listener.unreal_horde_internal_alb_https_listener resource aws_lb_listener_rule.unreal_horde_external_alb_grpc_rule resource aws_lb_listener_rule.unreal_horde_internal_alb_grpc_rule resource aws_lb_target_group.unreal_horde_api_target_group_external resource aws_lb_target_group.unreal_horde_api_target_group_internal resource aws_lb_target_group.unreal_horde_grpc_target_group_external resource aws_lb_target_group.unreal_horde_grpc_target_group_internal resource aws_s3_bucket.ansible_playbooks resource aws_s3_bucket.unreal_horde_alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_s3_bucket_public_access_block.ansible_playbooks_bucket_public_block resource aws_s3_bucket_versioning.ansible_playbooks_versioning resource aws_s3_object.unreal_horde_agent_playbook resource aws_s3_object.unreal_horde_agent_service resource aws_security_group.unreal_horde_agent_sg resource aws_security_group.unreal_horde_docdb_sg resource aws_security_group.unreal_horde_elasticache_sg resource aws_security_group.unreal_horde_external_alb_sg resource aws_security_group.unreal_horde_internal_alb_sg resource aws_security_group.unreal_horde_sg resource aws_ssm_association.configure_unreal_horde_agent resource aws_ssm_document.ansible_run_document resource aws_vpc_security_group_egress_rule.unreal_horde_agents_outbound_ipv4 resource aws_vpc_security_group_egress_rule.unreal_horde_agents_outbound_ipv6 resource aws_vpc_security_group_egress_rule.unreal_horde_external_alb_outbound_service_api resource aws_vpc_security_group_egress_rule.unreal_horde_external_alb_outbound_service_grpc resource aws_vpc_security_group_egress_rule.unreal_horde_internal_alb_outbound_service_api resource aws_vpc_security_group_egress_rule.unreal_horde_internal_alb_outbound_service_grpc resource aws_vpc_security_group_egress_rule.unreal_horde_outbound_ipv4 resource aws_vpc_security_group_egress_rule.unreal_horde_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.unreal_horde_agents_inbound_agents resource aws_vpc_security_group_ingress_rule.unreal_horde_docdb_ingress resource aws_vpc_security_group_ingress_rule.unreal_horde_elasticache_ingress resource aws_vpc_security_group_ingress_rule.unreal_horde_inbound_external_alb_api resource aws_vpc_security_group_ingress_rule.unreal_horde_inbound_external_alb_grpc resource aws_vpc_security_group_ingress_rule.unreal_horde_inbound_internal_alb_api resource aws_vpc_security_group_ingress_rule.unreal_horde_inbound_internal_alb_grpc resource aws_vpc_security_group_ingress_rule.unreal_horde_service_inbound_agents resource aws_vpc_security_group_ingress_rule.unreal_horde_service_inbound_containers resource random_string.unreal_horde resource random_string.unreal_horde_alb_access_logs_bucket_suffix resource random_string.unreal_horde_ansible_playbooks_bucket_suffix resource aws_ami.unreal_horde_agent_ami data source aws_ecs_cluster.unreal_horde_cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.ec2_trust_relationship data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.horde_agents_ec2_policy data source aws_iam_policy_document.horde_agents_s3_policy data source aws_iam_policy_document.unreal_horde_default_policy data source aws_iam_policy_document.unreal_horde_elasticache_policy data source aws_iam_policy_document.unreal_horde_recycle_policy data source aws_iam_policy_document.unreal_horde_secrets_manager_policy data source aws_region.current data source"},{"location":"modules/unreal/horde/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required certificate_arn The TLS certificate ARN for the Unreal Horde load balancer. <code>string</code> n/a yes fully_qualified_domain_name The fully qualified domain name where your Unreal Engine Horde server will be available. This agents will use this to enroll. <code>string</code> n/a yes unreal_horde_service_subnets A list of subnets to deploy the Unreal Horde service into. Private subnets are recommended. <code>list(string)</code> n/a yes vpc_id The ID of the existing VPC you would like to deploy Unreal Horde into. <code>string</code> n/a yes admin_claim_type The claim type for administrators. <code>string</code> <code>null</code> no admin_claim_value The claim value for administrators. <code>string</code> <code>null</code> no agent_dotnet_runtime_version The dotnet-runtime-{} package to install (see your engine version's release notes for supported version) <code>string</code> <code>\"6.0\"</code> no agents Configures autoscaling groups to be used as build agents by Unreal Engine Horde. <pre>map(object({    ami             = string    instance_type   = string    horde_pool_name = optional(string)    create_asg      = optional(bool, true)    block_device_mappings = list(      object({        device_name = string        ebs = object({          volume_size = number        })      })    )    min_size = optional(number, 0)    max_size = optional(number, 1)  }))</pre> <code>{}</code> no auth_method The authentication method for the Horde server. <code>string</code> <code>null</code> no cluster_name The name of the cluster to deploy the Unreal Horde into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no container_api_port The container port for the Unreal Horde web server. <code>number</code> <code>5000</code> no container_cpu The CPU allotment for the Unreal Horde container. <code>number</code> <code>1024</code> no container_grpc_port The container port for the Unreal Horde GRPC channel. <code>number</code> <code>5002</code> no container_memory The memory allotment for the Unreal Horde container. <code>number</code> <code>4096</code> no container_name The name of the Unreal Horde container. <code>string</code> <code>\"unreal-horde-container\"</code> no create_external_alb Set this flag to true to create an external load balancer for Unreal Horde. <code>bool</code> <code>true</code> no create_internal_alb Set this flag to true to create an internal load balancer for Unreal Horde. <code>bool</code> <code>true</code> no create_unreal_horde_default_policy Optional creation of Unreal Horde default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_unreal_horde_default_role Optional creation of Unreal Horde default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no create_unreal_horde_recycle_policy Optional creation of Unreal Horde IAM Policy allowing usage of the AwsReuse/AwsRecycle fleet manager. <code>bool</code> <code>false</code> no custom_cache_connection_config The redis-compatible connection configuration that Horde should use. <code>string</code> <code>null</code> no custom_unreal_horde_role ARN of the custom IAM Role you wish to use with Unreal Horde. <code>string</code> <code>null</code> no database_connection_string The database connection string that Horde should use. <code>string</code> <code>null</code> no debug Set this flag to enable ECS execute permissions on the Unreal Horde container and force new service deployments on Terraform apply. <code>bool</code> <code>false</code> no desired_container_count The desired number of containers running Unreal Horde. <code>number</code> <code>1</code> no docdb_backup_retention_period Number of days to retain backups for DocumentDB cluster. <code>number</code> <code>7</code> no docdb_instance_class The instance class for the Horde DocumentDB cluster. <code>string</code> <code>\"db.t4g.medium\"</code> no docdb_instance_count The number of instances to provision for the Horde DocumentDB cluster. <code>number</code> <code>2</code> no docdb_master_password Master password created for DocumentDB cluster. <code>string</code> <code>\"mustbeeightchars\"</code> no docdb_master_username Master username created for DocumentDB cluster. <code>string</code> <code>\"horde\"</code> no docdb_preferred_backup_window The preferred window for DocumentDB backups to be created. <code>string</code> <code>\"07:00-09:00\"</code> no docdb_skip_final_snapshot Flag for whether a final snapshot should be created when the cluster is destroyed. <code>bool</code> <code>true</code> no docdb_storage_encrypted Configure DocumentDB storage at rest. <code>bool</code> <code>true</code> no elasticache_cluster_count Number of cache cluster to provision in the Elasticache cluster. <code>number</code> <code>2</code> no elasticache_engine The engine to use for ElastiCache (redis or valkey) <code>string</code> <code>\"redis\"</code> no elasticache_node_count Number of cache nodes to provision in the Elasticache cluster. <code>number</code> <code>1</code> no elasticache_node_type The type of nodes provisioned in the Elasticache cluster. <code>string</code> <code>\"cache.t4g.micro\"</code> no elasticache_port The port for the ElastiCache cluster. <code>number</code> <code>6379</code> no elasticache_redis_engine_version The version of the Redis engine to use. <code>string</code> <code>\"7.0\"</code> no elasticache_redis_parameter_group_name The name of the Redis parameter group to use. <code>string</code> <code>\"default.redis7\"</code> no elasticache_snapshot_retention_limit The number of Elasticache snapshots to retain. <code>number</code> <code>5</code> no elasticache_valkey_engine_version The version of the ElastiCache engine to use. <code>string</code> <code>\"7.2\"</code> no elasticache_valkey_parameter_group_name The name of the Valkey parameter group to use. <code>string</code> <code>\"default.valkey7\"</code> no enable_new_agents_by_default Set this flag to automatically enable new agents that enroll with the Horde Server. <code>bool</code> <code>false</code> no enable_unreal_horde_alb_access_logs Enables access logging for the Unreal Horde ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_unreal_horde_alb_deletion_protection Enables deletion protection for the Unreal Horde ALB. Defaults to true. <code>bool</code> <code>false</code> no environment The current environment (e.g. Development, Staging, Production, etc.). This will tag ressources and set ASPNETCORE_ENVIRONMENT variable. <code>string</code> <code>\"Development\"</code> no existing_security_groups A list of existing security group IDs to attach to the Unreal Horde load balancer. <code>list(string)</code> <code>[]</code> no github_credentials_secret_arn A secret containing the Github username and password with permissions to the EpicGames organization. <code>string</code> <code>null</code> no image The Horde Server image to use in the ECS service. <code>string</code> <code>\"ghcr.io/epicgames/horde-server:latest-bundled\"</code> no name The name attached to Unreal Engine Horde module resources. <code>string</code> <code>\"unreal-horde\"</code> no oidc_audience The audience used for validating externally issued tokens. <code>string</code> <code>null</code> no oidc_authority The authority for the OIDC authentication provider used. <code>string</code> <code>null</code> no oidc_client_id The client ID used for authenticating with the OIDC provider. <code>string</code> <code>null</code> no oidc_client_secret The client secret used for authenticating with the OIDC provider. <code>string</code> <code>null</code> no oidc_signin_redirect The sign-in redirect URL for the OIDC provider. <code>string</code> <code>null</code> no p4_port The Perforce server to connect to. <code>string</code> <code>null</code> no p4_super_user_password_secret_arn Optionally provide the ARN of an AWS Secret for the p4d super user password. <code>string</code> <code>null</code> no p4_super_user_username_secret_arn Optionally provide the ARN of an AWS Secret for the p4d super user username. <code>string</code> <code>null</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"iac-management\": \"CGD-Toolkit\",  \"iac-module\": \"unreal-horde\",  \"iac-provider\": \"Terraform\"}</pre> no unreal_horde_alb_access_logs_bucket ID of the S3 bucket for Unreal Horde ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no unreal_horde_alb_access_logs_prefix Log prefix for Unreal Horde ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no unreal_horde_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Unreal Horde. <code>string</code> <code>365</code> no unreal_horde_external_alb_subnets A list of subnets to deploy the Unreal Horde load balancer into. Public subnets are recommended. <code>list(string)</code> <code>[]</code> no unreal_horde_internal_alb_subnets A list of subnets to deploy the Unreal Horde internal load balancer into. Private subnets are recommended. <code>list(string)</code> <code>[]</code> no"},{"location":"modules/unreal/horde/index.html#outputs","title":"Outputs","text":"Name Description agent_security_group_id n/a external_alb_dns_name n/a external_alb_sg_id n/a external_alb_zone_id n/a internal_alb_dns_name n/a internal_alb_sg_id n/a internal_alb_zone_id n/a service_security_group_id n/a"},{"location":"modules/unreal/horde/tests/index.html","title":"Horde Module Tests","text":"<p>This directory contains Terraform unit tests for the Unreal Engine Horde module using mocked providers.</p>"},{"location":"modules/unreal/horde/tests/index.html#overview","title":"Overview","text":"<p>Tests use <code>mock_provider</code> blocks to validate module logic without making actual AWS API calls or creating resources. This approach provides:</p> <ul> <li>Zero AWS costs: No resources created</li> <li>Fast execution: Tests run in seconds</li> <li>No cleanup required: No resources to clean up</li> <li>No credentials needed: Tests run without AWS authentication</li> <li>Logic validation: Focus on module conditionals, variable validation, and resource configuration</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#test-files","title":"Test Files","text":"<ul> <li><code>01_basic.tftest.hcl</code> - Basic deployment with minimal configuration</li> <li><code>02_complete.tftest.hcl</code> - Complete deployment with all features</li> <li><code>03_auth_methods.tftest.hcl</code> - Authentication method configurations</li> <li><code>04_agents.tftest.hcl</code> - Build agent configurations</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#running-tests","title":"Running Tests","text":""},{"location":"modules/unreal/horde/tests/index.html#locally","title":"Locally","text":"<pre><code># From the horde module directory\ncd modules/unreal/horde\n\n# Initialize (downloads providers)\nterraform init\n\n# Run all tests\nterraform test\n\n# Run specific test\nterraform test -filter=tests/01_basic.tftest.hcl\n\n# Verbose output\nterraform test -verbose\n\n# Skip initialization for faster iteration\nterraform test -no-init\n</code></pre>"},{"location":"modules/unreal/horde/tests/index.html#in-ci","title":"In CI","text":"<p>Tests run automatically on pull requests that modify the Horde module. The CI workflow:</p> <ol> <li>Detects changes to <code>modules/unreal/horde/**</code></li> <li>Runs <code>terraform init</code></li> <li>Runs <code>terraform test -verbose</code></li> <li>Reports results on the PR</li> </ol> <p>No AWS credentials required - mocked providers eliminate the need for authentication.</p>"},{"location":"modules/unreal/horde/tests/index.html#what-gets-tested","title":"What Gets Tested","text":""},{"location":"modules/unreal/horde/tests/index.html#module-logic-what-we-test","title":"\u2705 Module Logic (What We Test)","text":"<ul> <li>Variable validation: Validation rules trigger correctly</li> <li>Conditional resource creation: Resources created/skipped based on variables</li> <li>Resource counts: Correct number of resources based on configuration</li> <li>Dynamic blocks: Dynamic blocks generate expected configuration</li> <li>for_each logic: Loops create expected resource instances</li> <li>count logic: Count expressions evaluate correctly</li> <li>Dependency chains: Resources reference each other correctly</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#aws-behavior-what-we-dont-test","title":"\u274c AWS Behavior (What We Don't Test)","text":"<ul> <li>Actual resource creation in AWS</li> <li>AWS API functionality</li> <li>Network connectivity</li> <li>IAM permission validation</li> <li>Resource state after apply</li> <li>Real DNS resolution</li> <li>Certificate validation</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#test-scenarios","title":"Test Scenarios","text":""},{"location":"modules/unreal/horde/tests/index.html#01_basictftesthcl","title":"01_basic.tftest.hcl","text":"<p>Tests minimal deployment configuration:</p> <ul> <li>Internal ALB only</li> <li>Default DocumentDB and ElastiCache settings</li> <li>No authentication</li> <li>No build agents</li> </ul> <p>Key Assertions:</p> <ul> <li>ECS cluster and service created</li> <li>Internal ALB created, external ALB not created</li> <li>DocumentDB and ElastiCache clusters created</li> <li>Security groups and IAM roles created</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#02_completetftesthcl","title":"02_complete.tftest.hcl","text":"<p>Tests full-featured deployment:</p> <ul> <li>External and internal ALBs</li> <li>Custom database and cache configurations</li> <li>ALB access logging</li> <li>All optional features enabled</li> </ul> <p>Key Assertions:</p> <ul> <li>Both ALBs created with target groups</li> <li>Custom DocumentDB instance count (3)</li> <li>Valkey cache engine</li> <li>S3 bucket for ALB logs</li> <li>Security groups for both ALBs</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#03_auth_methodstftesthcl","title":"03_auth_methods.tftest.hcl","text":"<p>Tests authentication configurations:</p> <ul> <li>Anonymous authentication</li> <li>OIDC authentication</li> <li>Okta authentication</li> <li>Perforce integration</li> <li>Variable validation for auth methods</li> </ul> <p>Key Assertions:</p> <ul> <li>Valid auth methods work</li> <li>Invalid auth methods fail validation</li> <li>OIDC requires all parameters</li> <li>Perforce integration configures correctly</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#04_agentstftesthcl","title":"04_agents.tftest.hcl","text":"<p>Tests build agent configurations:</p> <ul> <li>Single agent pool</li> <li>Multiple agent pools</li> <li>No agents configured</li> <li>Custom dotnet runtime versions</li> </ul> <p>Key Assertions:</p> <ul> <li>Correct number of ASGs and launch templates</li> <li>S3 bucket created when agents configured</li> <li>No resources when agents map is empty</li> <li>Custom configurations applied correctly</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#mocked-providers","title":"Mocked Providers","text":"<p>All test files use identical <code>mock_provider</code> blocks to simulate AWS and random providers. While this creates duplication across test files, it's required by the Terraform test framework - mock providers cannot be shared or imported.</p>"},{"location":"modules/unreal/horde/tests/index.html#standard-mock-configuration","title":"Standard Mock Configuration","text":"<p>Each test file includes these mocks:</p> <pre><code>mock_provider \"aws\" {\n  mock_data \"aws_region\" {\n    defaults = {\n      name = \"us-east-1\"\n      id   = \"us-east-1\"\n    }\n  }\n\n  mock_data \"aws_caller_identity\" {\n    defaults = {\n      account_id = \"123456789012\"\n      arn        = \"arn:aws:iam::123456789012:user/test\"\n      user_id    = \"AIDACKCEVSQ6C2EXAMPLE\"\n    }\n  }\n\n  mock_data \"aws_elb_service_account\" {\n    defaults = {\n      arn = \"arn:aws:iam::127311923021:root\"\n      id  = \"127311923021\"\n    }\n  }\n\n  mock_data \"aws_ecs_cluster\" {\n    defaults = {\n      arn                = \"arn:aws:ecs:us-east-1:123456789012:cluster/test\"\n      id                 = \"test\"\n      name               = \"test\"\n      status             = \"ACTIVE\"\n      pending_tasks_count = 0\n      running_tasks_count = 0\n    }\n  }\n}\n\nmock_provider \"random\" {}\n</code></pre>"},{"location":"modules/unreal/horde/tests/index.html#why-duplication-is-necessary","title":"Why Duplication is Necessary","text":"<ul> <li>Terraform limitation: Mock providers cannot be shared across test files</li> <li>Self-contained tests: Each test file must be independently runnable</li> <li>Framework requirement: Mock providers must be defined in the same file as the tests</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#benefits","title":"Benefits","text":"<p>This approach eliminates the need for:</p> <ul> <li>AWS credentials</li> <li>SSM Parameter Store</li> <li>Actual AWS resources</li> <li>Resource cleanup</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#adding-new-tests","title":"Adding New Tests","text":"<ol> <li>Create a new <code>.tftest.hcl</code> file following the naming convention</li> <li>Add <code>mock_provider</code> blocks for AWS and random providers</li> <li>Add <code>run</code> blocks with <code>command = plan</code></li> <li>Provide test values directly in variables</li> <li>Add assertions for critical behavior</li> </ol> <p>Example:</p> <pre><code>mock_provider \"aws\" {\n  # Mock data sources\n}\n\nmock_provider \"random\" {}\n\nrun \"unit_test_feature\" {\n  command = plan\n\n  variables {\n    vpc_id = \"vpc-test123\"\n    # ... other variables\n  }\n\n  assert {\n    condition     = length(aws_resource.main) &gt; 0\n    error_message = \"Resource should be created\"\n  }\n}\n</code></pre>"},{"location":"modules/unreal/horde/tests/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"modules/unreal/horde/tests/index.html#tests-fail-with-data-source-not-found","title":"Tests fail with \"data source not found\"","text":"<p>Cause: Module uses a data source that isn't mocked Solution: Add the data source to the <code>mock_provider</code> block</p>"},{"location":"modules/unreal/horde/tests/index.html#variable-validation-errors","title":"Variable validation errors","text":"<p>Cause: Test values don't meet validation rules Solution: Check validation rules in <code>variables.tf</code> and adjust test values</p>"},{"location":"modules/unreal/horde/tests/index.html#assertion-failures","title":"Assertion failures","text":"<p>Cause: Module logic doesn't match expected behavior Solution: Review module code or adjust assertions</p>"},{"location":"modules/unreal/horde/tests/index.html#best-practices","title":"Best Practices","text":"<ol> <li>Test logic, not AWS: Focus on conditionals, counts, and variable validation</li> <li>Use descriptive assertions: Clear error messages help debugging</li> <li>Test edge cases: Empty maps, null values, boundary conditions</li> <li>Keep tests fast: Mocked providers ensure tests run in seconds</li> <li>One scenario per run block: Makes failures easier to diagnose</li> </ol>"},{"location":"modules/unreal/horde/tests/index.html#integration-testing","title":"Integration Testing","text":"<p>Integration tests (using <code>terraform apply</code>) are not implemented due to Terraform test framework limitations:</p> <ul> <li>No automatic cleanup functionality</li> <li>Resource cleanup must be manual</li> <li>Risk of orphaned resources</li> </ul> <p>For integration testing:</p> <ul> <li>Use dedicated test environments</li> <li>Manual deployment and validation</li> <li>Proper cleanup procedures</li> </ul>"},{"location":"modules/unreal/horde/tests/index.html#references","title":"References","text":"<ul> <li>Terraform Test Documentation</li> <li>Mock Providers</li> <li>CGD Toolkit Design Standards</li> </ul>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html","title":"Unreal Cloud DDC Infra Module","text":"<p>Unreal Cloud Derived Data Cache (source code) is a caching system that stores additional data required to use assets, such as compiled shaders. This allows the engine to quickly retrieve this data instead of having to regenerate it, saving time and disk space for the development team. For distributed teams, a cloud-hosted DDC enables efficient collaboration by ensuring all team members have access to the same cached data regardless of their location. This module deploys the core infrastructure for Unreal Engine's Cloud Derived Data Cache (DDC) on AWS. It creates a scalable, secure, and high-performance environment that optimizes asset processing and distribution throughout your game development pipeline, reducing build times and improving team collaboration.</p> <p>The Unreal Cloud Derived Data Cache (DDC) infrastructure module implements Epic's recommended architecture using ScyllaDB, a high-performance Cassandra-compatible database. This module provisions the following AWS resources:</p> <ol> <li> <p>ScyllaDB Database Layer:</p> <ul> <li>Deployed on EC2 instances</li> <li>Supports both single-node and multi-node cluster configurations</li> <li>Optimized for high-throughput DDC operations</li> <li>Configured with AWS Systems Manager Session Manager to provide secure shell access without requiring SSH or bastion hosts</li> </ul> </li> <li> <p>ScyllaDB Monitoring Stack:</p> <ul> <li>Deployed on an EC2 instance</li> <li>Uses Prometheus for metrics collection, Alertmanager for handling alerts, and Grafana for visualization</li> <li>Creates a Application Load Balancer for accessing the Grafana UI for real-time insights into ScyllaDB node performance</li> </ul> </li> <li> <p>Amazon EKS Cluster with specialized node groups:</p> <ul> <li>System node group: Handles core Kubernetes components and system workloads</li> <li>NVME node group: Optimized for high-performance storage operations</li> <li>Worker node group: Manages regional data replication and distribution</li> <li>Configured with AWS Systems Manager Session Manager to provide secure shell access without requiring SSH or bastion hosts</li> </ul> </li> <li> <p>S3 Bucket:</p> <ul> <li>Provides durable storage for cached assets</li> <li>Enables cross-region asset availability</li> <li>Serves as a persistent backup layer</li> </ul> </li> </ol>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#prerequisites","title":"Prerequisites","text":""},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#network-infrastructure-requirements","title":"Network Infrastructure Requirements","text":"<p>At a minimum, the Cloud DDC Module requires a Virtual Private Cloud (VPC) with a specific subnet configuration. The suggested configuration includes:</p> <ul> <li>2 public subnets</li> <li>2 private subnets</li> <li>Coverage across 2 Availability Zones</li> <li>An S3 interface endpoint</li> </ul> <p>This architecture ensures high availability and secure communication patterns for your DDC infrastructure.</p> <p></p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#configuring-node-groups-and-scylladb-deployment","title":"Configuring Node Groups and ScyllaDB Deployment","text":"<p>The footprint of your Cloud DDC deployment can be configured through 2 variables:</p> <p></p> <p>EKS Node Group Configuration: <code>eks_node_group_subnets</code></p> <p>The <code>eks_node_group_subnets</code> variable defines the subnet distribution for your EKS node groups. Each specified subnet serves as a potential target for node placement, providing granular control over the geographical distribution of your EKS infrastructure. Adding more subnets to this configuration increases deployment flexibility and enables broader availability zone coverage for your workloads at the cost of increased network complexity and potential inter-AZ data transfer charges.</p> <p></p> <p>ScyllaDB Instance Distribution: <code>scylla_subnets</code></p> <p>The <code>scylla_subnets</code> variable determines the deployment topology of your ScyllaDB instances. Each specified subnet receives a dedicated ScyllaDB instance, with multiple subnet configurations automatically establishing a distributed cluster architecture. Configurations of two or more subnets enable high availability and data resilience through native ScyllaDB clustering at the cost of increased infrastructure complexity and proportionally higher operational expenses.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10.3 aws ~&gt; 6.0 random ~&gt; 3.7 tls ~&gt; 4.0"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.0 random ~&gt; 3.7 tls ~&gt; 4.0"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.unreal_cluster_cloudwatch resource aws_eks_cluster.unreal_cloud_ddc_eks_cluster resource aws_eks_node_group.nvme_node_group resource aws_eks_node_group.system_node_group resource aws_eks_node_group.worker_node_group resource aws_iam_instance_profile.scylla_instance_profile resource aws_iam_instance_profile.scylla_monitoring_profile resource aws_iam_openid_connect_provider.unreal_cloud_ddc_oidc_provider resource aws_iam_role.eks_cluster_role resource aws_iam_role.nvme_node_group_role resource aws_iam_role.scylla_monitoring_role resource aws_iam_role.scylla_role resource aws_iam_role.system_node_group_role resource aws_iam_role.worker_node_group_role resource aws_iam_role_policy.scylla_monitoring_policy resource aws_iam_role_policy_attachments_exclusive.eks_cluster_policy_attachement resource aws_iam_role_policy_attachments_exclusive.nvme_policy_attachement resource aws_iam_role_policy_attachments_exclusive.scylla_policy_attachement resource aws_iam_role_policy_attachments_exclusive.system_policy_attachement resource aws_iam_role_policy_attachments_exclusive.worker_policy_attachement resource aws_instance.scylla_ec2_instance_other_nodes resource aws_instance.scylla_ec2_instance_seed resource aws_instance.scylla_monitoring resource aws_launch_template.nvme_launch_template resource aws_launch_template.system_launch_template resource aws_launch_template.worker_launch_template resource aws_lb.scylla_monitoring_alb resource aws_lb_listener.scylla_monitoring_listener resource aws_lb_target_group.scylla_monitoring_alb_target_group resource aws_lb_target_group_attachment.scylla_monitoring resource aws_s3_bucket.scylla_monitoring_lb_access_logs_bucket resource aws_s3_bucket.unreal_ddc_s3_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_s3_bucket_public_access_block.unreal_ddc_s3_acls resource aws_security_group.cluster_security_group resource aws_security_group.nvme_security_group resource aws_security_group.scylla_monitoring_lb_sg resource aws_security_group.scylla_monitoring_sg resource aws_security_group.scylla_security_group resource aws_security_group.system_security_group resource aws_security_group.worker_security_group resource aws_vpc_security_group_egress_rule.cluster_egress_sg_rule resource aws_vpc_security_group_egress_rule.nvme_egress_sg_rules resource aws_vpc_security_group_egress_rule.scylla_monitoring_lb_sg_egress_rule resource aws_vpc_security_group_egress_rule.scylla_monitoring_sg_egress_rule resource aws_vpc_security_group_egress_rule.self_scylla_egress_sg_rules resource aws_vpc_security_group_egress_rule.ssm_egress_sg_rules resource aws_vpc_security_group_egress_rule.system_egress_sg_rules resource aws_vpc_security_group_egress_rule.worker_egress_sg_rules resource aws_vpc_security_group_ingress_rule.cluster_lb_ingress_sg_rule resource aws_vpc_security_group_ingress_rule.scylla_monitoring_ingress_node_exporter resource aws_vpc_security_group_ingress_rule.scylla_monitoring_ingress_prometheus resource aws_vpc_security_group_ingress_rule.scylla_monitoring_lb_monitoring resource aws_vpc_security_group_ingress_rule.self_ingress_cluster_sg_rule resource aws_vpc_security_group_ingress_rule.self_ingress_sg_rules resource random_string.scylla_monitoring_lb_access_logs_bucket_suffix resource aws_ami.amazon_linux data source aws_ami.scylla_ami data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.scylla_monitoring_assume_role data source aws_iam_policy_document.scylla_monitoring_policy_doc data source tls_certificate.eks_tls_certificate data source"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required vpc_id String for VPC ID <code>string</code> n/a yes alb_certificate_arn The ARN of the certificate to use on the ALB <code>string</code> <code>null</code> no create_application_load_balancer Whether to create an application load balancer for the Scylla monitoring dashboard. <code>bool</code> <code>true</code> no create_scylla_monitoring_stack Whether to create the Scylla monitoring stack <code>bool</code> <code>true</code> no debug Enable debug mode <code>bool</code> <code>false</code> no eks_cluster_cloudwatch_log_group_prefix Prefix to be used for the EKS cluster CloudWatch log group. <code>string</code> <code>\"/aws/eks/unreal-cloud-ddc/cluster\"</code> no eks_cluster_logging_types List of EKS cluster log types to be enabled. <code>list(string)</code> <pre>[  \"api\",  \"audit\",  \"authenticator\",  \"controllerManager\",  \"scheduler\"]</pre> no eks_cluster_private_access Allows private access of the EKS Control Plane from subnets attached to EKS Cluster <code>bool</code> <code>true</code> no eks_cluster_public_access Allows public access of EKS Control Plane should be used with <code>bool</code> <code>false</code> no eks_cluster_public_endpoint_access_cidr List of the CIDR Ranges you want to grant public access to the EKS Cluster's public endpoint. <code>list(string)</code> <code>[]</code> no eks_node_group_subnets A list of subnets ids you want the EKS nodes to be installed into. Private subnets are strongly recommended. <code>list(string)</code> <code>[]</code> no enable_scylla_monitoring_lb_access_logs Whether to enable access logs for the Scylla monitoring load balancer. <code>bool</code> <code>false</code> no enable_scylla_monitoring_lb_deletion_protection Whether to enable deletion protection for the Scylla monitoring load balancer. <code>bool</code> <code>false</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_security_groups List of existing security groups to add to the monitoring and Unreal DDC load balancers <code>list(string)</code> <code>[]</code> no internal_facing_application_load_balancer Whether the application load balancer should be internal-facing. <code>bool</code> <code>false</code> no kubernetes_version Kubernetes version to be used by the EKS cluster. <code>string</code> <code>\"1.31\"</code> no monitoring_application_load_balancer_subnets The subnets in which the ALB will be deployed <code>list(string)</code> <code>null</code> no name Unreal Cloud DDC Workload Name <code>string</code> <code>\"unreal-cloud-ddc\"</code> no nvme_managed_node_desired_size Desired number of nvme managed node group instances <code>number</code> <code>2</code> no nvme_managed_node_instance_type Nvme managed node group instance type <code>string</code> <code>\"i3en.large\"</code> no nvme_managed_node_max_size Max number of nvme managed node group instances <code>number</code> <code>2</code> no nvme_managed_node_min_size Min number of nvme managed node group instances <code>number</code> <code>1</code> no nvme_node_group_label Label applied to nvme node group. These will need to be matched in values for taints and tolerations for the worker pod definition. <code>map(string)</code> <pre>{  \"unreal-cloud-ddc/node-type\": \"nvme\"}</pre> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no region The AWS region to deploy to <code>string</code> <code>\"us-west-2\"</code> no scylla_ami_name Name of the Scylla AMI to be used to get the AMI ID <code>string</code> <code>\"ScyllaDB 6.0.1\"</code> no scylla_architecture The chip architecture to use when finding the scylla image. Valid <code>string</code> <code>\"x86_64\"</code> no scylla_db_storage Size of gp3 ebs volumes attached to Scylla DBs <code>number</code> <code>100</code> no scylla_db_throughput Throughput of gp3 ebs volumes attached to Scylla DBs <code>number</code> <code>200</code> no scylla_instance_type The type and size of the Scylla instance. <code>string</code> <code>\"i4i.2xlarge\"</code> no scylla_monitoring_instance_storage Size of gp3 ebs volumes in GB attached to Scylla monitoring instance <code>number</code> <code>20</code> no scylla_monitoring_instance_type The type and size of the Scylla monitoring instance. <code>string</code> <code>\"t3.xlarge\"</code> no scylla_monitoring_lb_access_logs_bucket Name of the S3 bucket to store the access logs for the Scylla monitoring load balancer. <code>string</code> <code>null</code> no scylla_monitoring_lb_access_logs_prefix Prefix to use for the access logs for the Scylla monitoring load balancer. <code>string</code> <code>null</code> no scylla_subnets A list of subnet IDs where Scylla will be deployed. Private subnets are strongly recommended. <code>list(string)</code> <code>[]</code> no system_managed_node_desired_size Desired number of system managed node group instances. <code>number</code> <code>1</code> no system_managed_node_instance_type Monitoring managed node group instance type. <code>string</code> <code>\"m5.large\"</code> no system_managed_node_max_size Max number of system managed node group instances. <code>number</code> <code>2</code> no system_managed_node_min_size Min number of system managed node group instances. <code>number</code> <code>1</code> no system_node_group_label Label applied to system node group <code>map(string)</code> <pre>{  \"pool\": \"system-pool\"}</pre> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"Unreal DDC\"}</pre> no worker_managed_node_desired_size Desired number of worker managed node group instances. <code>number</code> <code>1</code> no worker_managed_node_instance_type Worker managed node group instance type. <code>string</code> <code>\"c5.large\"</code> no worker_managed_node_max_size Max number of worker managed node group instances. <code>number</code> <code>1</code> no worker_managed_node_min_size Min number of worker managed node group instances. <code>number</code> <code>0</code> no worker_node_group_label Label applied to worker node group. These will need to be matched in values for taints and tolerations for the worker pod definition. <code>map(string)</code> <pre>{  \"unreal-cloud-ddc/node-type\": \"worker\"}</pre> no"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#outputs","title":"Outputs","text":"Name Description cluster_arn ARN of the EKS Cluster cluster_certificate_authority_data Public key for the EKS Cluster cluster_endpoint EKS Cluster Endpoint cluster_name Name of the EKS Cluster external_alb_dns_name DNS endpoint of Application Load Balancer (ALB) external_alb_zone_id Zone ID for internet facing load balancer nvme_node_group_label Label for the NVME node group oidc_provider_arn OIDC provider for the EKS Cluster peer_security_group_id ID of the Peer Security Group s3_bucket_id Bucket to be used for the Unreal Cloud DDC assets scylla_ips IPs of the Scylla EC2 instances system_node_group_label Label for the System node group worker_node_group_label Label for the Worker node group"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html","title":"Unreal Engine Cloud DDC Intra Cluster Module","text":"<p>Warning</p> <p>Many of the links in this document lead back to the Unreal Engine source code hosted on GitHub. Access to the Unreal Engine source code requires that you connect your existing GitHub account to your Epic account. If you are seeing 404 errors when opening certain links, follow the instructions here to connect your accounts.</p> <p>Unreal Cloud Derived Data Cache (source code) is a caching system that stores additional data required to use assets, such as compiled shaders. This allows the engine to quickly retrieve this data instead of having to regenerate it, saving time and disk space for the development team. For distributed teams, a cloud-hosted DDC enables efficient collaboration by ensuring all team members have access to the same cached data regardless of their location. This Terraform module deploys the Unreal Cloud DDC container image provided by the Epic Games GitHub organization. It also configures the necessary service accounts and IAM roles required to run the Unreal Cloud DDC service on AWS.</p> <p>This module currently utilizes the Terraform EKS Blueprints Addons repository to install the following addons to the Kubernetes cluster, with the required IAM roles and service accounts:</p> <ul> <li>CoreDNS: Provides DNS services for the Kubernetes cluster, enabling reliable name resolution for the Unreal Cloud DDC service.     Kube-Proxy: Manages network traffic routing within the cluster, ensuring seamless communication between the Unreal Cloud DDC service and other components.</li> <li>VPC-CNI: Implements the Kubernetes networking model within the AWS VPC, allowing the Unreal Cloud DDC service to be properly integrated with the network infrastructure.</li> <li>EBS CSI Driver: Provides persistent storage capabilities using Amazon Elastic Block Store (EBS), enabling the Unreal Cloud DDC service to store and retrieve cached data.</li> </ul>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>This module is designed to be used in conjunction with the Unreal Cloud DDC Infra Module which deploys the required infrastructure to host the Cloud DDC service.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#github-secret","title":"GitHub Secret","text":"<p>Next, for the module to be able to access the Unreal Cloud DDC container image, there are 2 things you must do. First, if you have not done so, you must connect your GitHub account to your Epic account, thereby granting you access to the container images in the Unreal Engine repository. Next, you will need to create a <code>github_credentials</code> secret which includes a <code>username</code> and <code>access-token</code> field.</p> <p>Note</p> <p>Instructions on creating a new access token can be found here. You will need to provide the <code>read:package</code> and <code>repo</code> permissions to the access token you create.</p> <p>You can then upload the secret to AWS Secret Manager using the following AWS CLI command:</p> <pre><code>aws secretsmanager create-secret --name \"ecr-pullthroughcache/github-credentials\" --secret-string '{\"username\":\"USERNAME-PLACEHOLDER\",\"access-token\":\"ACCESS-TOKEN-PLACEHOLDER\"}'\n</code></pre> <p>Note</p> <p>Make sure to replace the <code>GITHUB-USERNAME-PLACEHOLDER</code> and <code>GITHUB-ACCESS-TOKEN-PLACEHOLDER</code> with the appropriate values from your GitHub account prior to running the command.</p> <p>Warning</p> <p>Note that the name of the secret must be prefixed with <code>ecr-pullthroughcache/</code> and the fields must be called <code>username</code> and <code>access-token</code> for ECR to properly detect the secrets. If making changes to the above command, you must adhere to these rules.</p> <p>Once the secret is created, pass the newly uploaded secret's ARN into the <code>ghcr_credentials_secret_manager_arn</code> variable.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#customizing-your-deployment","title":"Customizing Your Deployment","text":""},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#oidc-secret","title":"OIDC Secret","text":"<p>To use client secrets for OIDC authentication, a new secret must be uploaded to AWS Secrets Manager. You can upload the new secret to AWS Secret Manager using the following AWS CLI command:</p> <p>Note</p> <p>Make sure to replace the <code>CLIENT-SECRET-PLACEHOLDER</code> and <code>CLIENT-ID-PLACEHOLDER</code> with the appropriate values from your IDP prior to running the command.</p> <pre><code>aws secretsmanager create-secret --name \"external-idp-oidc-credentials\" --secret-string '{\"client_secret\":\"CLIENT-SECRET-PLACEHOLDER\",\"client_id\":\"CLIENT-ID-PLACEHOLDER\"}'\n</code></pre> <p>The ARN for the newly created secret must then be passed to the <code>oidc_credentials_secret_manager_arn</code> variable. The secret is referenced using the following format and should be passed into the variable using the same format:</p> <p><code>aws!arn:aws:secretsmanager:&lt;region&gt;:&lt;aws-account-number&gt;:secret:&lt;secret-name&gt;|&lt;json-field&gt;</code></p> <p>Note</p> <p>Note the prefix <code>aws!</code> and the postfix <code>|&lt;json-field&gt;</code> are added to the ARN of the newly created secret.</p> <p>Note</p> <p>While we highly encourage the use of OIDC tokens for production environments, users can use a bearer token in its place by providing the token to the <code>unreal_cloud_ddc_helm_values</code> variable. See DDC sample for an example implementation.</p> <pre><code>unreal_cloud_ddc_helm_values = [\n    templatefile(\"${path.module}/assets/unreal_cloud_ddc_single_region.yaml\", {\n        token = &lt;bearer-token&gt;\n        # Other templatefile parameters...\n    })\n]\n</code></pre>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#chart-values-helm-configurations","title":"Chart Values (Helm Configurations)","text":"<p>The <code>unreal_cloud_ddc_helm_values</code> variable provides an open-ended way to configure the Unreal Cloud DDC deployment through the use of YAML files. We generally recommend you to use a template file. An example of a template file configuration can be found in the <code>unreal-cloud-ddc-single-region</code> sample located here. You can also find additional example templates provided by Epic here.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10.3 aws ~&gt; 6.0 helm ~&gt; 2.16 kubernetes ~&gt; 2.33"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.0 helm ~&gt; 2.16 kubernetes ~&gt; 2.33"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#modules","title":"Modules","text":"Name Source Version eks_blueprints_all_other_addons git::https://github.com/aws-ia/terraform-aws-eks-blueprints-addons.git a9963f4a0e168f73adb033be594ac35868696a91"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#resources","title":"Resources","text":"Name Type aws_ecr_pull_through_cache_rule.unreal_cloud_ddc_ecr_pull_through_cache_rule resource aws_iam_policy.s3_secrets_manager_iam_policy resource aws_iam_role.ebs_csi_iam_role resource aws_iam_role.unreal_cloud_ddc_sa_iam_role resource aws_iam_role_policy_attachment.ebs_csi_policy_attacment resource aws_iam_role_policy_attachment.unreal_cloud_ddc_sa_iam_role_s3_secrets_policy_attachment resource helm_release.unreal_cloud_ddc resource kubernetes_namespace.unreal_cloud_ddc resource kubernetes_service_account.unreal_cloud_ddc_service_account resource aws_caller_identity.current data source aws_eks_cluster.unreal_cloud_ddc_cluster data source aws_iam_openid_connect_provider.oidc_provider data source aws_iam_policy_document.unreal_cloud_ddc_policy data source aws_lb.unreal_cloud_ddc_load_balancer data source aws_region.current data source aws_s3_bucket.unreal_cloud_ddc_bucket data source"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required cluster_name Name of the EKS Cluster <code>string</code> n/a yes cluster_oidc_provider_arn ARN of the OIDC Provider from EKS Cluster <code>string</code> n/a yes ghcr_credentials_secret_manager_arn Arn for credentials stored in secret manager. Needs to be prefixed with 'ecr-pullthroughcache/' to be compatible with ECR pull through cache. <code>string</code> n/a yes s3_bucket_id ID of the S3 Bucket for Unreal Cloud DDC to use <code>string</code> n/a yes certificate_manager_hosted_zone_arn ARN of the Certificate Manager for Ingress. <code>list(string)</code> <code>[]</code> no enable_certificate_manager Enable Certificate Manager for Ingress. Required for TLS termination. <code>bool</code> <code>false</code> no name Unreal Cloud DDC Workload Name <code>string</code> <code>\"unreal-cloud-ddc\"</code> no oidc_credentials_secret_manager_arn Arn for oidc credentials stored in secret manager. <code>string</code> <code>null</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"Unreal DDC\"}</pre> no unreal_cloud_ddc_helm_values List of YAML files for Unreal Cloud DDC <code>list(string)</code> <code>[]</code> no unreal_cloud_ddc_namespace Namespace for Unreal Cloud DDC <code>string</code> <code>\"unreal-cloud-ddc\"</code> no unreal_cloud_ddc_service_account_name Name of Unreal Cloud DDC service account. <code>string</code> <code>\"unreal-cloud-ddc-sa\"</code> no unreal_cloud_ddc_version Version of the Unreal Cloud DDC Helm chart. <code>string</code> <code>\"1.2.0\"</code> no"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#outputs","title":"Outputs","text":"Name Description unreal_cloud_ddc_load_balancer_name n/a unreal_cloud_ddc_load_balancer_zone_id n/a"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.10.3 aws &gt;=5.73.0 helm &gt;=2.16.0 kubernetes &gt;=2.33.0"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#providers_1","title":"Providers","text":"Name Version aws 6.4.0 helm 3.0.2 kubernetes 2.37.1"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#modules_1","title":"Modules","text":"Name Source Version eks_blueprints_all_other_addons git::https://github.com/aws-ia/terraform-aws-eks-blueprints-addons.git a9963f4a0e168f73adb033be594ac35868696a91"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#resources_1","title":"Resources","text":"Name Type aws_ecr_pull_through_cache_rule.unreal_cloud_ddc_ecr_pull_through_cache_rule resource aws_iam_policy.s3_secrets_manager_iam_policy resource aws_iam_role.ebs_csi_iam_role resource aws_iam_role.unreal_cloud_ddc_sa_iam_role resource aws_iam_role_policy_attachment.ebs_csi_policy_attacment resource aws_iam_role_policy_attachment.unreal_cloud_ddc_sa_iam_role_s3_secrets_policy_attachment resource helm_release.unreal_cloud_ddc resource kubernetes_namespace.unreal_cloud_ddc resource kubernetes_service_account.unreal_cloud_ddc_service_account resource aws_caller_identity.current data source aws_eks_cluster.unreal_cloud_ddc_cluster data source aws_iam_openid_connect_provider.oidc_provider data source aws_iam_policy_document.unreal_cloud_ddc_policy data source aws_lb.unreal_cloud_ddc_load_balancer data source aws_region.current data source aws_s3_bucket.unreal_cloud_ddc_bucket data source"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#inputs_1","title":"Inputs","text":"Name Description Type Default Required certificate_manager_hosted_zone_arn ARN of the Certificate Manager for Ingress. <code>list(string)</code> <code>[]</code> no cluster_name Name of the EKS Cluster <code>string</code> n/a yes cluster_oidc_provider_arn ARN of the OIDC Provider from EKS Cluster <code>string</code> n/a yes enable_certificate_manager Enable Certificate Manager for Ingress. Required for TLS termination. <code>bool</code> <code>false</code> no ghcr_credentials_secret_manager_arn Arn for credentials stored in secret manager. Needs to be prefixed with 'ecr-pullthroughcache/' to be compatible with ECR pull through cache. <code>string</code> n/a yes name Unreal Cloud DDC Workload Name <code>string</code> <code>\"unreal-cloud-ddc\"</code> no oidc_credentials_secret_manager_arn Arn for oidc credentials stored in secret manager. <code>string</code> <code>null</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no s3_bucket_id ID of the S3 Bucket for Unreal Cloud DDC to use <code>string</code> n/a yes tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"Unreal DDC\"}</pre> no unreal_cloud_ddc_helm_values List of YAML files for Unreal Cloud DDC <code>list(string)</code> <code>[]</code> no unreal_cloud_ddc_namespace Namespace for Unreal Cloud DDC <code>string</code> <code>\"unreal-cloud-ddc\"</code> no unreal_cloud_ddc_service_account_name Name of Unreal Cloud DDC service account. <code>string</code> <code>\"unreal-cloud-ddc-sa\"</code> no unreal_cloud_ddc_version Version of the Unreal Cloud DDC Helm chart. <code>string</code> <code>\"1.2.0\"</code> no"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#outputs_1","title":"Outputs","text":"Name Description unreal_cloud_ddc_load_balancer_name n/a unreal_cloud_ddc_load_balancer_zone_id n/a"},{"location":"modules/vdi/index.html","title":"VDI (Virtual Desktop Infrastructure) Module","text":"<p>\u2139\ufe0f Prerequisites: You need a Windows Server AMI. The examples use Packer-built AMIs from this repo's Packer templates (<code>lightweight/</code> and <code>ue-gamedev/</code>), but any Windows Server 2019/2022/2025 AMI works. See Amazon DCV Documentation for complete setup guidance.</p>"},{"location":"modules/vdi/index.html#features","title":"Features","text":"<ul> <li>Amazon DCV Integration - High-performance streaming protocol optimized for graphics workloads</li> <li>Dual Connectivity - Public internet or private VPN access with custom DNS support</li> <li>Game Development Ready - GPU instances, high-performance storage, UE-optimized AMIs</li> <li>Intelligent Drive Management - Automatic Windows drive letter assignment and volume lifecycle</li> <li>Complete VDI Infrastructure - EC2 workstations, security, IAM, and user management</li> <li>Security by Default - Least privilege IAM, encrypted storage, restricted network access, termination protection</li> <li>Flexible Authentication - EC2 key pairs (emergency) and Secrets Manager (production)</li> <li>Runtime Software Installation - Automated package installation via SSM and Chocolatey</li> </ul>"},{"location":"modules/vdi/index.html#connectivity-patterns","title":"Connectivity Patterns","text":""},{"location":"modules/vdi/index.html#public-connectivity","title":"Public Connectivity","text":"<p>When: Workstations in public subnets with Internet Gateway routes Access: Direct internet with IP restrictions</p> <pre><code>workstations = {\n  \"vdi-001\" = {\n    preset_key = \"my-preset\"\n    assigned_user = \"naruto-uzumaki\"\n    subnet_id = aws_subnet.public_subnet.id\n    allowed_cidr_blocks = [\"198.51.100.1/32\"]  # Replace with user's public IP\n  }\n}\n</code></pre>"},{"location":"modules/vdi/index.html#private-connectivity","title":"Private Connectivity","text":"<p>When: Workstations in private subnets with NAT Gateway routes Access: Via VPN, Direct Connect, or Site-to-Site VPN DNS Requirement: AWS Client VPN connection required to resolve private DNS names (<code>username.vdi.internal</code>)</p> <pre><code>module \"vdi\" {\n  create_client_vpn = true  # Creates Client VPN infrastructure\n\n  users = {\n    \"naruto-uzumaki\" = {\n      given_name = \"Naruto\"\n      family_name = \"Uzumaki\"\n      email = \"naruto@example.com\"\n      type = \"administrator\"\n      use_client_vpn = true  # Gets VPN access + certificates\n    }\n  }\n\n  workstations = {\n    \"vdi-001\" = {\n      preset_key = \"my-preset\"\n      assigned_user = \"naruto-uzumaki\"\n      subnet_id = aws_subnet.private_subnet.id\n      allowed_cidr_blocks = [\"10.0.0.0/16\"]  # VPC CIDR only\n    }\n  }\n}\n</code></pre>"},{"location":"modules/vdi/index.html#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Remote User   \u2502    \u2502   VPN Client     \u2502    \u2502  VDI Workstation\u2502\n\u2502  Web Browser    \u2502\u2500\u2500\u2500\u25b6\u2502  (Private Mode)  \u2502\u2500\u2500\u2500\u25b6\u2502   EC2 Instance  \u2502\n\u2502                 \u2502    \u2502   .ovpn file     \u2502    \u2502   Windows + DCV \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2502 (Public Mode)         \u2502                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502         AWS VPC           \u2502\n                    \u2502  Security Groups, IAM,    \u2502\n                    \u2502  SSM, S3, Secrets Mgr     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modules/vdi/index.html#account-creation-pattern","title":"Account Creation Pattern","text":"Account Type Created When Password Storage Scope Use Case \ud83c\udd98 Administrator (built-in) Windows boot EC2 Key Pair All workstations Emergency break-glass only \ud83d\udee1\ufe0f Fleet Admin (<code>fleet_administrator</code>) SSM (if defined) Secrets Manager All workstations Fleet management \ud83d\udd27 Local Admin (<code>administrator</code>) SSM (if defined) Secrets Manager Assigned workstation Local administration \ud83d\udcbb Standard User (<code>user</code>) SSM (if defined) Secrets Manager Assigned workstation Daily usage <p>Key Point: Only the built-in Administrator exists automatically. All other accounts must be explicitly defined in the <code>users</code> variable.</p>"},{"location":"modules/vdi/index.html#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>AWS Account Setup</p> </li> <li> <p>AWS CLI configured with deployment permissions</p> </li> <li>VPC with public and private subnets</li> <li> <p>Basic understanding of AWS services (VPC, EC2)</p> </li> <li> <p>Network Planning</p> </li> <li>Public connectivity: User public IP addresses for security group access</li> <li>Private connectivity: VPN setup and VPC CIDR planning</li> </ol>"},{"location":"modules/vdi/index.html#cost-estimates","title":"Cost Estimates","text":"<p>\u26a0\ufe0f Cost Warning: These examples deploy expensive GPU instances (~$1,430/month per workstation). Review costs before deployment.</p> <p>Example Configuration Costs (per workstation/month):</p> <ul> <li>g4dn.4xlarge instance: ~$1,200/month</li> <li>EBS storage (300GB root + 2TB projects with 3000 IOPS): ~$230/month</li> <li>Total per workstation: ~$1,430/month</li> </ul> <p>3-workstation example total: ~$4,290/month</p> <p>Cost optimization options:</p> <ul> <li>Reduce volume sizes for development/testing</li> <li>Use smaller instance types (g4dn.xlarge, g4dn.2xlarge)</li> <li>Leverage Spot instances for non-production workloads</li> <li>Stop instances manually via AWS Console/CLI when not in use (EBS storage costs continue)</li> </ul> <p>For accurate pricing: Use the AWS Pricing Calculator with your specific requirements and region.</p>"},{"location":"modules/vdi/index.html#examples","title":"Examples","text":"<p>For a quickstart, please review the examples. They provide complete Terraform configuration with VPC setup, security groups, and detailed connection instructions.</p> <p>Available Examples:</p> <ul> <li>Public Connectivity - Direct internet access with IP restrictions</li> <li>Private Connectivity - AWS Client VPN with internal DNS</li> </ul>"},{"location":"modules/vdi/index.html#quick-start","title":"Quick Start","text":""},{"location":"modules/vdi/index.html#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/aws-games/cloud-game-development-toolkit.git\ncd cloud-game-development-toolkit\n</code></pre>"},{"location":"modules/vdi/index.html#2-choose-ami","title":"2. Choose AMI","text":"<ul> <li>Option A: Build a toolkit AMI (e.g., UE GameDev for game development)</li> <li>Option B: Use any existing Windows Server AMI</li> </ul> <p>Example - To build the UE GameDev AMI:</p> <pre><code>cd assets/packer/virtual-workstations/ue-gamedev/\npacker build windows-server-2025-ue-gamedev.pkr.hcl\n</code></pre>"},{"location":"modules/vdi/index.html#3-deploy-example","title":"3. Deploy Example","text":"<pre><code>cd modules/vdi/examples/public-connectivity/  # or private-connectivity/\nterraform init\nterraform plan\nterraform apply\n</code></pre>"},{"location":"modules/vdi/index.html#4-get-connection-info","title":"4. Get Connection Info","text":"<pre><code>terraform output connection_info\n</code></pre>"},{"location":"modules/vdi/index.html#5-connect-private-only","title":"5. Connect (Private Only)","text":""},{"location":"modules/vdi/index.html#aws-client-vpn-setup-private-connectivity","title":"AWS Client VPN Setup (Private Connectivity)","text":"<p>Why AWS Client VPN is Required:</p> <p>For private connectivity, AWS Client VPN is essential because:</p> <ul> <li>Custom DNS Resolution: Private workstation URLs like <code>https://username.vdi.internal:8443</code> only resolve when connected to the VPN</li> <li>Network Access: Private subnets are not accessible from the internet - VPN provides secure tunnel access</li> <li>Security: Eliminates need to expose workstations to public internet</li> </ul> <p>Setup Process:</p> <ol> <li>Deploy with VPN enabled: Set <code>create_client_vpn = true</code> in your Terraform configuration</li> <li>Download VPN configuration: The module automatically generates <code>.ovpn</code> files in S3</li> <li>Install VPN client: Download AWS VPN Client (recommended)</li> <li>Import configuration: Use the <code>.ovpn</code> file generated for your user</li> </ol> <p>Using the Generated .ovpn File:</p> <pre><code># Download your VPN configuration\naws s3 cp s3://cgd-vdi-vpn-configs-XXXXXXXX/your-username/your-username.ovpn ~/Downloads/\n\n# Import into AWS VPN Client or OpenVPN\n# AWS VPN Client: File \u2192 Manage Profiles \u2192 Add Profile\n# OpenVPN: Import the .ovpn file directly\n</code></pre> <p>For detailed setup instructions: See AWS Client VPN User Guide</p> <p>Connection Flow:</p> <ol> <li>Connect to AWS Client VPN using your <code>.ovpn</code> file</li> <li>Access workstation via private DNS: <code>https://username.vdi.internal:8443</code></li> <li>Login with credentials from Secrets Manager</li> </ol>"},{"location":"modules/vdi/index.html#connection-guide","title":"Connection Guide","text":""},{"location":"modules/vdi/index.html#critical-wait-for-windows-boot","title":"\u26a0\ufe0f CRITICAL: Wait for Windows Boot","text":"<p>After <code>terraform apply</code> completes, wait 5-10 minutes for Windows initialization before attempting login.</p> <p>During boot, you'll see:</p> <ul> <li>\"Wrong username or password\" errors (expected)</li> <li>DCV connection failures (expected)</li> <li>Certificate warnings (expected)</li> </ul> <p>Check boot status:</p> <pre><code>aws ec2 get-console-output --instance-id $(terraform output -json connection_info | jq -r '.\"vdi-001\".instance_id') --latest\n</code></pre> <p>Ready when you see:</p> <ul> <li><code>EC2Launch: EC2 Launch has completed</code></li> <li>User creation script completion</li> <li>DCV service startup messages</li> </ul>"},{"location":"modules/vdi/index.html#get-credentials","title":"Get Credentials","text":"<pre><code># Get connection info\nterraform output connection_info\n\n# Get user password from Secrets Manager\naws secretsmanager get-secret-value \\\n  --secret-id \"cgd/vdi-001/users/your-username\" \\\n  --query SecretString --output text | jq .\n</code></pre>"},{"location":"modules/vdi/index.html#connect-via-dcv","title":"Connect via DCV","text":"<ol> <li>Download DCV Client (recommended) or use web browser</li> <li>Connect to VPN (private connectivity only)</li> <li>Required for private DNS: To access workstations via <code>https://username.vdi.internal:8443</code>, you must be connected to AWS Client VPN</li> <li>Private DNS resolution: Custom DNS names only resolve when connected to the VPN</li> <li>Open DCV:</li> <li>Public: <code>https://workstation-public-ip:8443</code></li> <li>Private (VPN required): <code>https://username.vdi.internal:8443</code> or <code>https://workstation-private-ip:8443</code></li> <li>Accept certificate warning (self-signed certificates)</li> <li>Login with credentials from Secrets Manager</li> </ol>"},{"location":"modules/vdi/index.html#emergency-access","title":"Emergency Access","text":"<pre><code># Get Administrator password\nterraform output -json private_keys | jq -r '.\"vdi-001\"' &gt; temp_key.pem\nchmod 600 temp_key.pem\naws ec2 get-password-data \\\n  --instance-id $(terraform output -json connection_info | jq -r '.\"vdi-001\".instance_id') \\\n  --priv-launch-key temp_key.pem --query 'PasswordData' --output text\nrm temp_key.pem\n</code></pre>"},{"location":"modules/vdi/index.html#password-management","title":"Password Management","text":""},{"location":"modules/vdi/index.html#password-details","title":"Password Details","text":"<ul> <li>Auto-generated: 16-character secure passwords (letters + numbers + special characters)</li> <li>Initial storage: AWS Secrets Manager (source of truth for first login only)</li> <li>User changes: Users can change passwords in Windows - Secrets Manager will not update without additional configuration/custom logic (out of scope for this module)</li> <li>Lifecycle: Users can manage passwords in Windows or continue using Secrets Manager passwords</li> </ul>"},{"location":"modules/vdi/index.html#automatic-script-re-execution","title":"Automatic Script Re-execution","text":"<p>The module automatically re-runs configuration scripts when you modify infrastructure. Changes to volumes, users, or software packages trigger the appropriate scripts to run via AWS SSM - no manual intervention required.</p> <p>Scripts only execute when infrastructure actually changes, providing clean Terraform plans and predictable behavior.</p> <p>Manual Alternative: For immediate results or troubleshooting, you can RDP to the instance as Administrator and run operations manually.</p>"},{"location":"modules/vdi/index.html#volume-configuration","title":"Volume Configuration","text":""},{"location":"modules/vdi/index.html#ebs-volume-management","title":"EBS Volume Management","text":"<p>Volume changes do NOT trigger instance replacement. Instances continue running during volume operations.</p>"},{"location":"modules/vdi/index.html#required-root-volume","title":"Required Root Volume","text":"<pre><code>volumes = {\n  Root = {                    # \u2190 MUST be exactly \"Root\" (case-sensitive)\n    capacity = 256            # \u2190 Root volume automatically gets C: drive\n    type = \"gp3\"\n  }\n  # Add additional drives as needed (auto-assigned D:, E:, F:, etc.)\n  # Projects = { capacity = 1000, type = \"gp3\" }\n  # Cache = { capacity = 500, type = \"gp3\" }\n}\n</code></pre>"},{"location":"modules/vdi/index.html#drive-letter-assignment","title":"Drive Letter Assignment","text":"<p>Automatic Assignment: The module uses Windows auto-assignment for all drive letters:</p> <ul> <li>Root Volume \u2192 C: drive (Windows boot requirement)</li> <li>EBS Volumes \u2192 Auto-assigned (typically D:, E:, F:, etc.)</li> <li>Instance Store \u2192 Auto-assigned (typically next available letter)</li> </ul> <p>G4dn Instance Store Sizes:</p> <ul> <li><code>g4dn.xlarge</code>: 125GB NVMe SSD (auto-assigned)</li> <li><code>g4dn.2xlarge</code>: 225GB NVMe SSD (auto-assigned)</li> <li><code>g4dn.4xlarge</code>: 225GB NVMe SSD (auto-assigned)</li> <li><code>g4dn.8xlarge</code>: 900GB NVMe SSD (auto-assigned)</li> </ul> <p>Benefits: Simple configuration with no drive letter conflicts, Windows native behavior, user customizable via Disk Management, and cost efficiency by utilizing included instance store.</p>"},{"location":"modules/vdi/index.html#volume-change-lifecycle","title":"Volume Change Lifecycle","text":"Change Type Automatic Handling Data Safety User Action Required Add Volume \u2705 Fully automatic \u2705 Safe Wait 5-10 minutes after apply Increase Size \u2705 Fully automatic \u2705 Safe Wait for AWS optimization + SSM (5-15 min) Reduce Size \u274c BLOCKED BY AWS \u26a0\ufe0f Not Supported See Volume Size Reduction Remove Volume \u2705 Immediate and reliable \u274c Volume data lost None (drive letters cleaned up) Change Volume Type \u2705 Auto-applied \u2705 Safe Wait for optimization (5-15 min typical, up to 6 hrs) Rename Volume \u2705 Terraform only \u2705 Safe None"},{"location":"modules/vdi/index.html#volume-limitations","title":"Volume Limitations","text":"<p>Volume Size Reduction: Not supported by AWS - EBS volumes cannot be reduced in size.</p> <p>Volume Modification Rate Limit: AWS enforces a 6-hour wait between volume modifications. This is a hard platform limitation that cannot be overridden.</p>"},{"location":"modules/vdi/index.html#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"modules/vdi/index.html#on-demand-capacity-reservations-odcr","title":"On-Demand Capacity Reservations (ODCR)","text":"<p>Use existing capacity reservations if available. See AWS ODCR Documentation for details.</p> <pre><code># Module-level default (applies to all workstations)\nmodule \"vdi\" {\n  capacity_reservation_preference = \"open\"  # Use ODCR if available\n\n  workstations = {\n    \"ws1\" = { subnet_id = \"subnet-123\" }  # Inherits \"open\"\n    \"ws2\" = { subnet_id = \"subnet-456\" }  # Inherits \"open\"\n  }\n}\n\n# Per-workstation control\nworkstations = {\n  \"prod-ws\" = {\n    capacity_reservation_preference = \"open\"  # Use ODCR\n    subnet_id = \"subnet-123\"\n  }\n  \"dev-ws\" = {\n    # capacity_reservation_preference = \"none\"  # Optional - omit if you don't use ODCR\n    subnet_id = \"subnet-456\"\n  }\n}\n</code></pre>"},{"location":"modules/vdi/index.html#software-installation","title":"Software Installation","text":"<p>Available packages: Any valid Chocolatey package. Common examples: <code>git</code>, <code>vscode</code>, <code>notepadplusplus</code>, <code>7zip</code></p> <pre><code>presets = {\n  \"ue-dev\" = {\n    instance_type = \"g4dn.2xlarge\"\n    software_packages = [\"git\", \"vscode\", \"notepadplusplus\"]\n  }\n}\n</code></pre>"},{"location":"modules/vdi/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"modules/vdi/index.html#common-issues","title":"Common Issues","text":""},{"location":"modules/vdi/index.html#instance-launch-failures","title":"Instance Launch Failures","text":"<ul> <li>Verify AMI exists: <code>aws ec2 describe-images --owners self --filters \"Name=name,Values=*windows-server-2025*\"</code></li> <li>Check AMI is in correct region</li> <li>Ensure Packer build completed successfully</li> </ul>"},{"location":"modules/vdi/index.html#drive-letter-issues","title":"Drive Letter Issues","text":"<ul> <li>Check drive assignment: <code>Get-Disk | Format-Table Number, Size, BusType</code></li> <li>Volume scripts re-run automatically when volume configuration changes</li> </ul>"},{"location":"modules/vdi/index.html#connection-timeouts","title":"Connection Timeouts","text":"<ul> <li>Check security group allows your IP: <code>curl https://checkip.amazonaws.com/</code></li> <li>Verify instance is running: <code>aws ec2 describe-instances</code></li> <li>Test port connectivity: <code>telnet &lt;instance-ip&gt; 8443</code></li> </ul>"},{"location":"modules/vdi/index.html#password-retrieval-issues","title":"Password Retrieval Issues","text":"<ul> <li>Wait 5-10 minutes after instance launch for password generation</li> <li>Check Secrets Manager if user passwords not available</li> <li>Use S3 backup key if Terraform output fails</li> </ul>"},{"location":"modules/vdi/index.html#dcv-connecting-spinner","title":"DCV \"Connecting\" Spinner","text":"<ul> <li>Connect via SSM: <code>aws ssm start-session --target &lt;instance-id&gt;</code></li> <li>Check DCV sessions: <code>dcv list-sessions</code></li> <li>Restart DCV service: <code>Restart-Service dcvserver</code></li> </ul>"},{"location":"modules/vdi/index.html#vpn-connection-issues","title":"VPN Connection Issues","text":"<ul> <li>Check VPN endpoint DNS resolves: <code>nslookup [endpoint].prod.clientvpn.us-east-1.amazonaws.com</code></li> <li>Wait 5-15 minutes for AWS to activate endpoint</li> <li>Check for CIDR conflicts with local network</li> <li>Disconnect from other VPNs</li> </ul>"},{"location":"modules/vdi/index.html#user-accounts-not-created","title":"User Accounts Not Created","text":"<ul> <li>Check SSM command status: <code>aws ssm list-command-invocations --instance-id &lt;id&gt;</code></li> <li>Check user creation status: <code>aws ssm get-parameter --name \"/{project}/{workstation}/users/{username}/status_user_creation\"</code></li> <li>Scripts re-run automatically when user configuration changes</li> </ul>"},{"location":"modules/vdi/index.html#volume-initialization-issues","title":"Volume Initialization Issues","text":"<ul> <li>Check volume status: <code>aws ssm get-parameter --name \"/{project}/{workstation}/volume_status\"</code></li> <li>Check volume messages: <code>aws ssm get-parameter --name \"/{project}/{workstation}/volume_message\"</code></li> <li>Scripts re-run automatically when volume configuration changes</li> </ul>"},{"location":"modules/vdi/index.html#volume-resize-issues","title":"Volume Resize Issues","text":"<ul> <li>Check disk sizes vs partition sizes: <code>Get-Disk | Format-Table Number, Size, BusType</code></li> <li>Check partition sizes: <code>Get-Partition | Format-Table DiskNumber, DriveLetter, Size</code></li> <li>Manual partition extension: <code>Resize-Partition -DriveLetter F -Size (Get-PartitionSupportedSize -DriveLetter F).SizeMax</code></li> </ul>"},{"location":"modules/vdi/index.html#manual-volume-initialization-if-ssm-script-failed","title":"Manual Volume Initialization (if SSM script failed)","text":"<pre><code># Initialize any RAW disks\nGet-Disk | Where-Object { $_.PartitionStyle -eq 'RAW' } |\nInitialize-Disk -PartitionStyle MBR -PassThru |\nNew-Partition -AssignDriveLetter -UseMaximumSize |\nFormat-Volume -FileSystem NTFS -Confirm:$false\n</code></pre>"},{"location":"modules/vdi/index.html#software-installation-problems","title":"Software Installation Problems","text":"<ul> <li>Check software status: <code>aws ssm get-parameter --name \"/{project}/{workstation}/software_status\"</code></li> <li>Check failed packages: <code>aws ssm get-parameter --name \"/{project}/{workstation}/software_message\"</code></li> <li>Scripts re-run automatically when software configuration changes</li> </ul>"},{"location":"modules/vdi/index.html#debug-commands","title":"Debug Commands","text":"<pre><code># Basic connectivity\ncurl https://checkip.amazonaws.com/\ntelnet &lt;instance-ip&gt; 8443\n\n# SSM access (no network needed)\naws ssm start-session --target &lt;instance-id&gt;\n\n# VPN testing\nping naruto-uzumaki.vdi.internal\nnslookup naruto-uzumaki.vdi.internal\n\n# Volume troubleshooting\nINSTANCE_ID=$(terraform output -json connection_info | jq -r '.\"vdi-001\".instance_id')\naws ssm list-command-invocations --instance-id $INSTANCE_ID --filters Key=DocumentName,Values=cgd-dev-initialize-volumes\naws ssm get-command-invocation --command-id &lt;COMMAND_ID&gt; --instance-id $INSTANCE_ID\n</code></pre>"},{"location":"modules/vdi/index.html#password-retrieval","title":"Password Retrieval","text":"<pre><code># Administrator password\nterraform output -json private_keys | jq -r '.\"vdi-001\"' &gt; temp_key.pem\naws ec2 get-password-data --instance-id &lt;id&gt; --priv-launch-key temp_key.pem\n\n# User passwords\naws secretsmanager get-secret-value --secret-id \"cgd/vdi-001/users/naruto-uzumaki\"\n</code></pre>"},{"location":"modules/vdi/index.html#contributing","title":"Contributing","text":"<p>See the Contributing Guidelines for information on how to contribute to this project.</p>"},{"location":"modules/vdi/index.html#license","title":"License","text":"<p>This project is licensed under the MIT-0 License. See the LICENSE file for details.</p>"},{"location":"modules/vdi/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.13 aws ~&gt; 6.0 awscc ~&gt; 1.0 http ~&gt; 3.0 null ~&gt; 3.0 random ~&gt; 3.0 time ~&gt; 0.9 tls ~&gt; 4.0"},{"location":"modules/vdi/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.0 awscc ~&gt; 1.0 random ~&gt; 3.0 time ~&gt; 0.9 tls ~&gt; 4.0"},{"location":"modules/vdi/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/vdi/index.html#resources","title":"Resources","text":"Name Type aws_acm_certificate.client_vpn_ca resource aws_acm_certificate.client_vpn_server resource aws_cloudwatch_log_group.client_vpn_logs resource aws_cloudwatch_log_group.vdi_logs resource aws_cloudwatch_log_stream.client_vpn_logs resource aws_ebs_volume.workstation_volumes resource aws_ec2_client_vpn_authorization_rule.vdi resource aws_ec2_client_vpn_endpoint.vdi resource aws_ec2_client_vpn_network_association.vdi resource aws_eip.workstation_eips resource aws_iam_instance_profile.vdi_instance_profile resource aws_iam_role.vdi_instance_role resource aws_iam_role_policy.vdi_instance_access resource aws_iam_role_policy_attachment.additional_policies resource aws_iam_role_policy_attachment.vdi_cloudwatch_agent resource aws_iam_role_policy_attachment.vdi_ssm_managed_instance_core resource aws_instance.workstations resource aws_key_pair.workstation_keys resource aws_route53_record.user_dns_records resource aws_route53_zone.private resource aws_route53_zone.vdi_internal resource aws_s3_bucket.keys resource aws_s3_bucket.scripts resource aws_s3_bucket.vpn_configs resource aws_s3_bucket_public_access_block.keys resource aws_s3_bucket_public_access_block.scripts resource aws_s3_bucket_public_access_block.vpn_configs resource aws_s3_bucket_server_side_encryption_configuration.keys resource aws_s3_bucket_server_side_encryption_configuration.scripts resource aws_s3_bucket_versioning.keys resource aws_s3_bucket_versioning.scripts resource aws_s3_object.emergency_private_keys resource aws_s3_object.user_ca_certificates resource aws_s3_object.user_certificates resource aws_s3_object.user_private_keys resource aws_s3_object.vpn_client_configs resource aws_security_group.workstation resource aws_ssm_association.software_installation resource aws_ssm_association.vdi_user_creation resource aws_ssm_association.volume_initialization resource aws_ssm_document.create_vdi_users resource aws_ssm_document.initialize_volumes resource aws_ssm_document.install_software resource aws_ssm_parameter.vdi_dns resource aws_volume_attachment.workstation_volume_attachments resource aws_vpc_security_group_egress_rule.all_outbound resource aws_vpc_security_group_ingress_rule.dcv_https_access resource aws_vpc_security_group_ingress_rule.dcv_quic_access resource aws_vpc_security_group_ingress_rule.https_access resource aws_vpc_security_group_ingress_rule.rdp_access resource aws_vpc_security_group_ingress_rule.rdp_access_additional resource awscc_secretsmanager_secret.user_passwords resource random_id.suffix resource random_string.bucket_suffix resource time_sleep.wait_for_ssm_agent resource tls_cert_request.client_vpn_server resource tls_cert_request.client_vpn_users resource tls_locally_signed_cert.client_vpn_server resource tls_locally_signed_cert.client_vpn_users resource tls_private_key.client_vpn_ca resource tls_private_key.client_vpn_server resource tls_private_key.client_vpn_users resource tls_private_key.workstation_keys resource tls_self_signed_cert.client_vpn_ca resource aws_iam_policy_document.vdi_instance_access data source aws_region.current data source aws_subnet.workstation_subnets data source aws_vpc.selected data source"},{"location":"modules/vdi/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required region AWS region for deployment <code>string</code> n/a yes vpc_id VPC ID where VDI instances will be deployed <code>string</code> n/a yes capacity_reservation_preference Capacity reservation preference for EC2 instances <code>string</code> <code>null</code> no client_vpn_config Client VPN configuration for private connectivity <pre>object({    client_cidr_block       = optional(string, \"192.168.0.0/16\")    generate_client_configs = optional(bool, true)    split_tunnel            = optional(bool, true)  })</pre> <code>{}</code> no create_client_vpn Create AWS Client VPN endpoint infrastructure (VPN endpoint, certificates, S3 bucket for configs) <code>bool</code> <code>false</code> no create_default_security_groups Create default security groups for VDI workstations <code>bool</code> <code>true</code> no debug Enable debug mode. When true, disables termination protection for CI/CD environments. When false, enables termination protection for production environments. <code>bool</code> <code>false</code> no ebs_kms_key_id KMS key ID for EBS encryption (if encryption enabled) <code>string</code> <code>null</code> no enable_centralized_logging Enable centralized logging with CloudWatch log groups following CGD Toolkit patterns <code>bool</code> <code>false</code> no environment Environment name (dev, staging, prod, etc.) <code>string</code> <code>\"dev\"</code> no log_retention_days CloudWatch log retention period in days <code>number</code> <code>30</code> no presets Configuration blueprints defining instance types and named volumes with Windows drive mapping.KEY BECOMES PRESET NAME: The map key (e.g., \"ue-developer\") becomes the preset name referenced by workstations.Presets provide reusable configurations that can be referenced by multiple workstations via preset_key.Example:presets = {  \"ue-developer\" = {           # \u2190 This key becomes the preset name    instance_type = \"g4dn.2xlarge\"    gpu_enabled   = true    volumes = {      Root = { capacity = 256, type = \"gp3\" }  # Root volume automatically gets C:      Projects = { capacity = 1024, type = \"gp3\", windows_drive = \"Z:\" }  # Specify drive letter      Cache = { capacity = 500, type = \"gp3\" }  # Auto-assigned high-alphabet letter (Y:, X:, etc.)    }  }  \"basic-workstation\" = {      # \u2190 Another preset name    instance_type = \"g4dn.xlarge\"    gpu_enabled   = true    volumes = {      Root = { capacity = 200, type = \"gp3\" }  # Root volume automatically gets C:      UserData = { capacity = 500, type = \"gp3\" }  # Auto-assigned high-alphabet letter    }  }}# Referenced by workstations:workstations = {  \"alice-ws\" = {    preset_key = \"ue-developer\"      # \u2190 References preset by key  }}Valid volume types: \"gp2\", \"gp3\", \"io1\", \"io2\"Drive letters are auto-assigned by Windows (typically C: for root, D:, E:, F:, etc. for additional volumes).additional_policy_arns: List of additional IAM policy ARNs to attach to the VDI instance role.Example: [\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\", \"arn:aws:iam::123456789012:policy/MyCustomPolicy\"] <pre>map(object({    # Core compute configuration    instance_type = string    ami           = optional(string, null)    # Hardware configuration    gpu_enabled = optional(bool, true)    # Named volumes with auto-assigned drive letters    volumes = map(object({      capacity   = number      type       = string      iops       = optional(number, 3000)      throughput = optional(number, 125)      encrypted  = optional(bool, true)    }))    # Optional configuration    iam_instance_profile   = optional(string, null)    additional_policy_arns = optional(list(string), []) # Additional IAM policy ARNs to attach to the VDI instance role    software_packages      = optional(list(string), null)    tags                   = optional(map(string), {})  }))</pre> <code>{}</code> no project_prefix Prefix for resource names <code>string</code> <code>\"cgd\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"terraform-aws-vdi\",  \"ModuleSource\": \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/vdi\",  \"RootModuleName\": \"-\"}</pre> no users Local Windows user accounts with Windows group types and network connectivity (managed via Secrets Manager)KEY BECOMES WINDOWS USERNAME: The map key (e.g., \"john-doe\") becomes the actual Windows username created on VDI instances.type options (Windows groups):- \"fleet_administrator\": User added to Windows Administrators group, created on ALL workstations (fleet management)- \"administrator\": User added to Windows Administrators group, created only on assigned workstation- \"user\": User added to Windows Users group, created only on assigned workstationuse_client_vpn options (VPN access):- false: User accesses VDI via public internet or external VPN (default)- true: User accesses VDI via module's Client VPN (generates VPN config)Example:users = {  \"vdiadmin\" = {              # \u2190 This key becomes Windows username \"vdiadmin\"    given_name = \"VDI\"    family_name = \"Administrator\"    email = \"admin@example.com\"    type = \"fleet_administrator\" # Windows Administrators group on ALL workstations    use_client_vpn = false      # Accesses via public internet/external VPN  }  \"alice\" = {                 # \u2190 Public connectivity user    given_name = \"Alice\"    family_name = \"Smith\"    email = \"alice@example.com\"    type = \"user\"               # Windows Users group    use_client_vpn = false      # Accesses via public internet (allowed_cidr_blocks)  }  \"bob\" = {                   # \u2190 Private connectivity user    given_name = \"Bob\"    family_name = \"Johnson\"    email = \"bob@example.com\"    type = \"user\"               # Windows Users group    use_client_vpn = true       # Accesses via module's Client VPN  }}# User assignment is now direct:# assigned_user = \"naruto-uzumaki\"  # References users{} key directly in workstation <pre>map(object({    given_name     = string    family_name    = string    email          = string    type           = optional(string, \"user\") # \"administrator\" or \"user\" (Windows group)    use_client_vpn = optional(bool, false)    # Whether this user connects via module's Client VPN    tags           = optional(map(string), {})  }))</pre> <code>{}</code> no workstations Physical infrastructure instances with template references and placement configuration.KEY BECOMES WORKSTATION NAME: The map key (e.g., \"alice-workstation\") becomes the workstation identifier used throughout the module.Workstations inherit configuration from templates via preset_key reference.Example:workstations = {  # Public connectivity - user accesses via internet  \"alice-workstation\" = {    preset_key = \"ue-developer\"    subnet_id = \"subnet-public-123\"     # Public subnet    security_groups = [\"sg-vdi-public\"]    assigned_user = \"alice\"    allowed_cidr_blocks = [\"203.0.113.1/32\"]  # Alice's home IP  }  # Private connectivity - user accesses via VPN  \"bob-workstation\" = {    preset_key = \"basic-workstation\"    subnet_id = \"subnet-private-456\"    # Private subnet    security_groups = [\"sg-vdi-private\"]    assigned_user = \"bob\"    # No allowed_cidr_blocks - accessed via Client VPN  }  # Additional volumes at workstation level  \"dev-workstation\" = {    preset_key = \"basic-workstation\"    subnet_id = \"subnet-private-789\"    security_groups = [\"sg-vdi-private\"]    volumes = {      ExtraStorage = { capacity = 2000, type = \"gp3\", windows_drive = \"Y:\" }    }  }}# User assignment is now direct:# assigned_user = \"alice\"  # References users{} key directly in workstationDrive letters are auto-assigned by Windows. Users can reassign them via Disk Management if needed.additional_policy_arns: List of additional IAM policy ARNs to attach to the VDI instance role.Example: [\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\", \"arn:aws:iam::123456789012:policy/MyCustomPolicy\"] <pre>map(object({    # Preset reference (optional - can use direct config instead)    preset_key = optional(string, null)    # Infrastructure placement    subnet_id       = string    security_groups = list(string)    assigned_user   = optional(string, null) # User assigned to this workstation (for administrator/user types only)    # Direct configuration (used when preset_key is null or as overrides)    ami           = optional(string, null)    instance_type = optional(string, null)    gpu_enabled   = optional(bool, null)    volumes = optional(map(object({      capacity   = number      type       = string      iops       = optional(number, 3000)      throughput = optional(number, 125)      encrypted  = optional(bool, true)    })), null)    iam_instance_profile   = optional(string, null)    additional_policy_arns = optional(list(string), []) # Additional IAM policy ARNs to attach to the VDI instance role    software_packages      = optional(list(string), null)    # Optional overrides    allowed_cidr_blocks             = optional(list(string), null)    capacity_reservation_preference = optional(string, null)    tags                            = optional(map(string), null)  }))</pre> <code>{}</code> no"},{"location":"modules/vdi/index.html#outputs","title":"Outputs","text":"Name Description ami_id AMI ID used for workstations connection_info Complete connection information for VDI workstations emergency_key_paths S3 paths for emergency private keys private_keys Private keys for emergency access (sensitive) private_zone_id Private hosted zone ID for creating additional VPC associations private_zone_name Private hosted zone name public_ips Map of workstation public IP addresses vpn_configs_bucket S3 bucket name for VPN configuration files"},{"location":"modules/vdi/index.html#volume-management","title":"Volume Management","text":""},{"location":"modules/vdi/index.html#dynamic-volume-operations","title":"Dynamic Volume Operations","text":"<p>Adding/Resizing Volumes:</p> <ol> <li>Add or modify volumes in Terraform configuration</li> <li>Run <code>terraform apply</code></li> <li>Wait 5-10 minutes for automatic SSM volume script execution</li> <li>Verify volumes are initialized via RDP</li> </ol> <p>How it works:</p> <ul> <li>\u2705 Fully automated - Lifecycle rules handle all triggering</li> <li>\u2705 Reliable triggering - Automatically detects volume changes</li> <li>\u2705 Predictable timing - 5-10 minutes for script execution</li> <li>\u2705 Proper cleanup - Drive letters managed automatically</li> <li>\u2705 Clean plans - No continuous Terraform drift</li> </ul>"},{"location":"modules/vdi/index.html#alternative-manual-administration","title":"Alternative: Manual Administration","text":"<p>For immediate results, you can skip waiting for SSM and manually initialize volumes:</p> <ol> <li>RDP to instance as Administrator after <code>terraform apply</code></li> <li>Run PowerShell commands to initialize volumes immediately</li> <li>Complete in under 2 minutes with full control</li> </ol>"},{"location":"modules/vdi/index.html#volume-limitations_1","title":"Volume Limitations","text":""},{"location":"modules/vdi/index.html#volume-size-reduction-not-supported","title":"Volume Size Reduction - NOT SUPPORTED","text":"<p>AWS Limitation: EBS volumes cannot be reduced in size. This is an AWS platform limitation, not a module limitation.</p> <p>What Happens: If you reduce volume capacity in Terraform (e.g., 500GB \u2192 200GB):</p> <pre><code>terraform apply\n# \u274c Error: InvalidParameterValue: Cannot decrease volume size from 500 to 200\n# \u274c The apply will FAIL IMMEDIATELY - no waiting required\n</code></pre> <p>Workaround for Size Reduction:</p> <ol> <li>Create new smaller volume in Terraform config</li> <li>Manually migrate data from old to new volume via RDP</li> <li>Remove old volume from Terraform config</li> <li>Apply changes - old volume will be deleted</li> </ol>"},{"location":"modules/vdi/examples/private-connectivity/index.html","title":"Private Connectivity VDI Example","text":""},{"location":"modules/vdi/examples/private-connectivity/index.html#overview","title":"Overview","text":"<p>Demonstrates VDI deployment with private network access via AWS Client VPN:</p> <ul> <li>Private Network Access: AWS Client VPN with certificate-based authentication</li> <li>Internal DNS: Custom domain names for easy connection</li> <li>Multi-User VPN: Each user gets their own .ovpn configuration file</li> <li>Enterprise Security: No public internet exposure, VPC-only access</li> </ul>"},{"location":"modules/vdi/examples/private-connectivity/index.html#prerequisites","title":"Prerequisites","text":"<ol> <li>AWS credentials configured</li> <li>VPN client software (AWS VPN Client or OpenVPN)</li> <li>Custom AMIs built using Packer templates (required for this example)</li> </ol> <p>Note: This example requires specific custom AMIs because the data sources reference them by name. You can customize the example to use different AMIs by modifying <code>data.tf</code>.</p>"},{"location":"modules/vdi/examples/private-connectivity/index.html#deployment","title":"Deployment","text":"<pre><code>terraform init\nterraform apply\n</code></pre>"},{"location":"modules/vdi/examples/private-connectivity/index.html#what-gets-created","title":"What Gets Created","text":"<ul> <li>3 VDI instances in private subnets (no public IPs)</li> <li>AWS Client VPN endpoint with certificate-based authentication</li> <li>Private DNS zone (cgd.vdi.internal) for easy connection</li> <li>VPN certificates and .ovpn files for each user</li> <li>NAT Gateway for outbound internet access (Windows updates, software downloads)</li> </ul>"},{"location":"modules/vdi/examples/private-connectivity/index.html#connection-private-vpn-access","title":"Connection (Private VPN Access)","text":""},{"location":"modules/vdi/examples/private-connectivity/index.html#step-1-download-vpn-configuration","title":"Step 1: Download VPN Configuration","text":"<pre><code># Get VPN configs bucket\nterraform output vpn_configs_bucket\n\n# Download your .ovpn file\n# macOS/Linux:\naws s3 cp s3://cgd-vdi-vpn-configs-XXXXXXXX/naruto-uzumaki/naruto-uzumaki.ovpn ~/Downloads/\n\n# Windows (PowerShell):\naws s3 cp s3://cgd-vdi-vpn-configs-XXXXXXXX/naruto-uzumaki/naruto-uzumaki.ovpn $env:USERPROFILE\\Downloads\\\n\n# Windows (Command Prompt):\naws s3 cp s3://cgd-vdi-vpn-configs-XXXXXXXX/naruto-uzumaki/naruto-uzumaki.ovpn %USERPROFILE%\\Downloads\\\n</code></pre>"},{"location":"modules/vdi/examples/private-connectivity/index.html#step-2-connect-to-vpn","title":"Step 2: Connect to VPN","text":"<ol> <li>AWS VPN Client (recommended): Import .ovpn file</li> <li>OpenVPN: Use .ovpn file with any OpenVPN client</li> <li>Wait 2-3 minutes for VPN connection to establish</li> </ol>"},{"location":"modules/vdi/examples/private-connectivity/index.html#step-3-get-user-passwords","title":"Step 3: Get User Passwords","text":"<pre><code># Get connection info\nterraform output connection_info\n\n# Get user password from Secrets Manager\naws secretsmanager get-secret-value --secret-id \"cgd/vdi-001/users/naruto-uzumaki\" --query SecretString --output text | jq -r '.password'\n</code></pre>"},{"location":"modules/vdi/examples/private-connectivity/index.html#step-4-connect-via-dcv-private-dns","title":"Step 4: Connect via DCV (Private DNS)","text":"<ol> <li>vdi-001 (UE GameDev): <code>https://naruto-uzumaki.cgd.vdi.internal:8443</code></li> <li>vdi-002 (DevOps): <code>https://sasuke-uchiha.cgd.vdi.internal:8443</code></li> <li>vdi-003 (Junior Dev): <code>https://boruto-uzumaki.cgd.vdi.internal:8443</code></li> </ol> <p>Alternative: Use private IPs directly if DNS doesn't resolve</p>"},{"location":"modules/vdi/examples/private-connectivity/index.html#software-packages","title":"Software Packages","text":"<ul> <li>Chocolatey (package manager)</li> <li>Visual Studio 2022 Community</li> <li>Git</li> <li>Perforce client tools</li> </ul> <p>Check installation progress via CloudWatch logs or SSM status commands in outputs.</p>"},{"location":"modules/vdi/examples/private-connectivity/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.13 aws ~&gt; 6.0"},{"location":"modules/vdi/examples/private-connectivity/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.0"},{"location":"modules/vdi/examples/private-connectivity/index.html#modules","title":"Modules","text":"Name Source Version vdi ../../ n/a"},{"location":"modules/vdi/examples/private-connectivity/index.html#resources","title":"Resources","text":"Name Type aws_eip.nat_eip resource aws_internet_gateway.vdi_igw resource aws_nat_gateway.vdi_nat resource aws_route_table.vdi_private_rt resource aws_route_table.vdi_public_rt resource aws_route_table_association.vdi_private_rta resource aws_route_table_association.vdi_public_rta resource aws_security_group.vdi_private_sg resource aws_subnet.vdi_private_subnet resource aws_subnet.vdi_public_subnet resource aws_vpc.vdi_vpc resource aws_vpc_security_group_egress_rule.vdi_all_outbound resource aws_vpc_security_group_ingress_rule.vdi_dcv resource aws_vpc_security_group_ingress_rule.vdi_https resource aws_vpc_security_group_ingress_rule.vdi_rdp resource aws_ami.vdi_lightweight_ami data source aws_ami.vdi_ue_gamedev_ami data source aws_availability_zones.available data source aws_region.current data source"},{"location":"modules/vdi/examples/private-connectivity/index.html#inputs","title":"Inputs","text":"<p>No inputs.</p>"},{"location":"modules/vdi/examples/private-connectivity/index.html#outputs","title":"Outputs","text":"Name Description connection_info VDI connection information"},{"location":"modules/vdi/examples/public-connectivity/index.html","title":"Public Connectivity VDI Example","text":""},{"location":"modules/vdi/examples/public-connectivity/index.html#overview","title":"Overview","text":"<p>Demonstrates VDI deployment with public internet access and multiple AMI types for different user roles:</p> <ul> <li>Public Internet Access: Direct connection via Internet Gateway with IP restrictions</li> <li>UE GameDev AMI: Pre-built with Visual Studio 2022 + Epic Games Launcher</li> <li>Lightweight AMI: Basic Windows for runtime software installation</li> <li>Multi-User Support: Different workstation configurations for different roles</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#architecture","title":"Architecture","text":""},{"location":"modules/vdi/examples/public-connectivity/index.html#public-connectivity-multi-ami-pattern","title":"Public Connectivity + Multi-AMI Pattern","text":"<pre><code>vdi-001 (naruto-uzumaki)  \u2192 UE GameDev AMI    \u2192 g4dn.4xlarge (Game Developer)\nvdi-002 (sasuke-uchiha)   \u2192 Lightweight AMI   \u2192 g4dn.xlarge  (DevOps Engineer)\nvdi-003 (boruto-uzumaki)  \u2192 Lightweight AMI   \u2192 g4dn.xlarge  (Junior Developer)\n                    \u2193\n            Public Internet Access\n         (Your IP: Security Group Rules)\n</code></pre>"},{"location":"modules/vdi/examples/public-connectivity/index.html#user-management","title":"User Management","text":"<ul> <li>Local Windows users created via EC2 user data (immediate execution)</li> <li>Secrets Manager stores all passwords</li> <li>Multi-user support: Admin users on ALL workstations, standard users on assigned workstation</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#prerequisites","title":"Prerequisites","text":""},{"location":"modules/vdi/examples/public-connectivity/index.html#required-amis","title":"Required AMIs","text":"<p>Note: This example requires specific custom AMIs because the data sources reference them by name. You can customize the example to use different AMIs by modifying <code>data.tf</code>.</p> <p>Build custom AMIs using Packer templates:</p> <pre><code># Build UE GameDev AMI (45-60 minutes)\ncd ../../../../assets/packer/virtual-workstations/ue-gamedev/\npacker build windows-server-2025-ue-gamedev.pkr.hcl\n\n# Build Lightweight AMI (20-30 minutes)\ncd ../lightweight/\npacker build windows-server-2025-lightweight.pkr.hcl\n</code></pre>"},{"location":"modules/vdi/examples/public-connectivity/index.html#aws-setup","title":"AWS Setup","text":"<ol> <li>AWS credentials configured</li> <li>Custom AMIs built and available</li> <li>VPC with public subnet</li> </ol>"},{"location":"modules/vdi/examples/public-connectivity/index.html#deployment","title":"Deployment","text":"<pre><code>terraform init\nterraform apply\n</code></pre>"},{"location":"modules/vdi/examples/public-connectivity/index.html#what-gets-created","title":"What Gets Created","text":""},{"location":"modules/vdi/examples/public-connectivity/index.html#infrastructure","title":"Infrastructure","text":"<ul> <li>3 VDI instances with different AMIs and configurations</li> <li>VPC + subnet + security groups for public internet access</li> <li>Internet Gateway for direct public connectivity</li> <li>S3 buckets for emergency keys and scripts</li> <li>CloudWatch log groups for centralized logging</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#users-created-via-ssm","title":"Users (Created via SSM)","text":"<ul> <li>vdiadmin: Fleet administrator account on ALL instances</li> <li>naruto-uzumaki: Administrator on vdi-001 (UE GameDev workstation)</li> <li>sasuke-uchiha: Administrator on vdi-002 (DevOps workstation)</li> <li>boruto-uzumaki: Standard user on vdi-003 (Junior developer workstation)</li> </ul> <p>Status Tracking: User creation status tracked in SSM Parameter Store at <code>/{project}/{workstation}/users/{username}/status_*</code></p>"},{"location":"modules/vdi/examples/public-connectivity/index.html#authentication","title":"Authentication","text":"<ul> <li>Secrets Manager: All user passwords stored securely</li> <li>EC2 Key Pairs: Emergency break-glass access</li> <li>DCV Sessions: Created automatically for assigned users</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#connection","title":"Connection","text":""},{"location":"modules/vdi/examples/public-connectivity/index.html#get-connection-info","title":"Get Connection Info","text":"<pre><code>terraform output connection_info\n</code></pre>"},{"location":"modules/vdi/examples/public-connectivity/index.html#get-passwords","title":"Get Passwords","text":"<pre><code># Get all password retrieval commands\nterraform output password_retrieval_commands\n\n# Example: Get naruto-uzumaki password\naws secretsmanager get-secret-value --secret-id \"arn:aws:secretsmanager:us-east-1:ACCOUNT:secret:cgd/vdi-001/users/naruto-uzumaki-XXXXX\" --query SecretString --output text | jq -r '.password'\n</code></pre>"},{"location":"modules/vdi/examples/public-connectivity/index.html#connect-via-dcv-public-internet","title":"Connect via DCV (Public Internet)","text":"<ol> <li>vdi-001 (UE GameDev): <code>https://&lt;vdi-001-ip&gt;:8443</code></li> <li>Login: <code>naruto-uzumaki</code> + password from Secrets Manager</li> <li> <p>Pre-installed: Visual Studio 2022, Epic Games Launcher, Git, Perforce</p> </li> <li> <p>vdi-002 (DevOps): <code>https://&lt;vdi-002-ip&gt;:8443</code></p> </li> <li>Login: <code>sasuke-uchiha</code> + password from Secrets Manager</li> <li> <p>Runtime installed: VS Code, Terraform, Docker, Kubernetes CLI</p> </li> <li> <p>vdi-003 (Junior Dev): <code>https://&lt;vdi-003-ip&gt;:8443</code></p> </li> <li>Login: <code>boruto-uzumaki</code> + password from Secrets Manager</li> <li>Basic tools: VS Code, Git, Notepad++</li> </ol>"},{"location":"modules/vdi/examples/public-connectivity/index.html#admin-access","title":"Admin Access","text":"<ul> <li>vdiadmin account available on BOTH instances</li> <li>Use for system administration and troubleshooting</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#key-features","title":"Key Features","text":""},{"location":"modules/vdi/examples/public-connectivity/index.html#public-internet-connectivity","title":"\u2705 Public Internet Connectivity","text":"<ul> <li>Direct access via Internet Gateway (no VPN required)</li> <li>IP-based security with automatic detection of your public IP</li> <li>Cost-effective for individual developers and small teams</li> <li>Simple setup - no certificate management or VPN clients</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#multi-ami-support","title":"\u2705 Multi-AMI Support","text":"<ul> <li>Different AMIs for different roles (GameDev vs General)</li> <li>Automatic AMI discovery via data sources</li> <li>Requires custom AMIs - build using Packer templates</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#infrastructure-focused","title":"\u2705 Infrastructure-Focused","text":"<ul> <li>No runtime software installation (unreliable)</li> <li>Custom AMIs provide consistent, fast boot times</li> <li>Role-based templates with appropriate instance types</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#reliable-user-creation","title":"\u2705 Reliable User Creation","text":"<ul> <li>SSM associations create users with proper timing</li> <li>Status tracking via SSM Parameter Store</li> <li>Force retry capability with <code>force_run_provisioning = \"true\"</code></li> <li>Idempotent user creation with error handling</li> <li>Simplified passwords (letters and numbers only)</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#enterprise-ready","title":"\u2705 Enterprise-Ready","text":"<ul> <li>Multi-user support with proper Windows groups</li> <li>Centralized password management via Secrets Manager</li> <li>Break-glass access via EC2 key pairs</li> <li>Audit logging via CloudWatch</li> <li>ODCR support for cost optimization with capacity reservations</li> </ul>"},{"location":"modules/vdi/examples/public-connectivity/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"modules/vdi/examples/public-connectivity/index.html#user-creation-issues","title":"User Creation Issues","text":"<p>Check SSM command execution and status:</p> <pre><code># Check SSM command status\naws ssm list-command-invocations --instance-id &lt;instance-id&gt;\n\n# Check user creation status\naws ssm get-parameter --name \"/cgd/vdi-001/users/naruto-uzumaki/status_user_creation\"\n\n# Force retry user creation\n# Set force_run_provisioning = \"true\" in main.tf and apply\n</code></pre>"},{"location":"modules/vdi/examples/public-connectivity/index.html#ami-not-found","title":"AMI Not Found","text":"<p>If custom AMIs aren't built, Terraform will fail with data source error:</p> <ol> <li>Required: Build AMIs using Packer templates (see Prerequisites)</li> <li>No fallback: Data sources require specific AMI names to exist</li> </ol>"},{"location":"modules/vdi/examples/public-connectivity/index.html#dcv-connection-issues","title":"DCV Connection Issues","text":"<ol> <li>Check security group allows port 8443 from your IP</li> <li>Verify DCV service is running on instance</li> <li>Confirm user was created successfully</li> </ol>"},{"location":"modules/vdi/examples/public-connectivity/index.html#migration-from-software-packages","title":"Migration from Software Packages","text":"<p>BREAKING CHANGE: Software installation logic removed in favor of custom AMIs.</p> <p>Before: Runtime software installation via SSM After: Pre-built AMIs with software included</p> <p>Benefits:</p> <ul> <li>\u2705 Faster boot times (no installation delays)</li> <li>\u2705 Reliable deployments (no installation failures)</li> <li>\u2705 Consistent environments (known software versions)</li> <li>\u2705 Better user experience (workstations ready immediately)</li> </ul> <p>Migration Path:</p> <ol> <li>Build custom AMIs with required software</li> <li>Update templates to reference custom AMI IDs</li> <li>Remove software_packages variables from configuration</li> </ol>"},{"location":"modules/vdi/examples/public-connectivity/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.13 aws ~&gt; 6.0 http ~&gt; 3.0"},{"location":"modules/vdi/examples/public-connectivity/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.0 http ~&gt; 3.0"},{"location":"modules/vdi/examples/public-connectivity/index.html#modules","title":"Modules","text":"Name Source Version vdi ../../ n/a"},{"location":"modules/vdi/examples/public-connectivity/index.html#resources","title":"Resources","text":"Name Type aws_internet_gateway.vdi_igw resource aws_route_table.vdi_rt resource aws_route_table_association.vdi_rta resource aws_security_group.vdi_sg resource aws_subnet.vdi_subnet resource aws_vpc.vdi_vpc resource aws_vpc_security_group_egress_rule.vdi_all_outbound resource aws_vpc_security_group_ingress_rule.vdi_dcv resource aws_vpc_security_group_ingress_rule.vdi_http resource aws_vpc_security_group_ingress_rule.vdi_https resource aws_vpc_security_group_ingress_rule.vdi_rdp resource aws_ami.vdi_lightweight_ami data source aws_ami.vdi_ue_gamedev_ami data source aws_availability_zones.available data source aws_region.current data source http_http.my_ip data source"},{"location":"modules/vdi/examples/public-connectivity/index.html#inputs","title":"Inputs","text":"<p>No inputs.</p>"},{"location":"modules/vdi/examples/public-connectivity/index.html#outputs","title":"Outputs","text":"Name Description connection_info VDI connection information"},{"location":"samples/index.html","title":"Overview","text":"<p>Samples represent a reference implementation that can be copied, modified and deployed to solve for a specific use-case or workload. These are Terraform configurations and integrations with other common AWS workloads and services. Each sample will provide its own documentation and instructions that follows the template below:</p>"},{"location":"samples/index.html#1-predeployment","title":"1) Predeployment","text":"<p>In the predeployment phase the user is instructed to provision or take note of any necessary pre-existing resources. Creating SSL certificates or keypairs, provisioning Amazon Machine Images (AMIs) with Packer, or documenting existing resource IDs and names all fall into this phase.</p>"},{"location":"samples/index.html#2-deployment","title":"2) Deployment","text":"<p>In the deployment phase the user is instructed to run <code>terraform apply</code> on one or more Terraform configurations with the appropriate variables.</p>"},{"location":"samples/index.html#3-postdeployment","title":"3) Postdeployment","text":"<p>Finally, the postdeployment phase includes any Ansible playbooks or remote execution instructions for configuring the applications that have been deployed. These may be automated or manual steps.</p>"},{"location":"samples/simple-build-pipeline/index.html","title":"Simple Build Pipeline","text":"<p>This sample demonstrates how to use the Cloud Game Development Toolkit to deploy a simple build pipeline on AWS.</p>"},{"location":"samples/simple-build-pipeline/index.html#features","title":"Features","text":"<ul> <li>Deployment of version control with P4 Server, P4 Auth, and Code Review</li> <li>Deployment of CI/CD with Jenkins</li> </ul>"},{"location":"samples/simple-build-pipeline/index.html#step-1-install-prerequisites","title":"Step 1. Install Prerequisites","text":"<p>You will need the following tools to complete this tutorial:</p> <ol> <li>Terraform CLI</li> <li>Packer CLI</li> <li>AWS CLI v2</li> </ol>"},{"location":"samples/simple-build-pipeline/index.html#step-2-create-perforce-p4-server-formerly-helix-core-amazon-machine-image","title":"Step 2. Create Perforce P4 Server (formerly Helix Core) Amazon Machine Image","text":"<p>Prior to deploying the infrastructure for running P4 Server, we need to create an Amazon Machine Image containing the necessary software and tools. The Cloud Game Development Toolkit contains a Packer template for doing just this.</p> <p>IMPORTANT: Uploading shell scripts from Windows to Unix-based systems using Packer can sometimes fail due to line ending differences. Windows uses CRLF (<code>\\r\\n</code>) while Unix systems use LF ( <code>\\n</code>). This discrepancy can cause issues when the script is executed on the target system. In this case, this issue can occur with the shell scripts that are used to configure Perforce on the EC2 Instance.</p> <p>To avoid this, ensure you use WSL or something else similar to allow you to execute Unix commands, or use a Unix-based machine.</p> <ol> <li>From your terminal, run the following commands from the root of the repository (this example assumes usage of <code>x86_64</code> architecture as this is the default instance architecture used in the module):</li> </ol> <pre><code>packer init ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\npacker build ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\n</code></pre> <p>This will use your AWS credentials to provision an EC2 instance in your Default VPC. This instance is only used to create the AMI and will be terminated once the AMI is successfully created. The Region, VPC, and Subnet where this instance is provisioned and the AMI is created are configurable - please consult the <code>example.pkrvars.hcl</code> file and the Packer documentation on assigning variables for more details.</p> <p>Note: The P4 Server template will default to the user's CLI configured region, if a region is not provided. The AWS Region where this AMI is created must be the same Region where you intend to deploy the Simple Build Pipeline.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-3-create-build-agent-amazon-machine-images","title":"Step 3. Create Build Agent Amazon Machine Images","text":"<p>This section covers the creation of Amazon Machine Images used to provision Jenkins build agents. Different studios have different needs at this stage, so we'll cover the creation of three different build agent AMIs.</p> <p>Note: The Build Agent templates will default to the user's CLI configured region, if a region is not provided.</p>"},{"location":"samples/simple-build-pipeline/index.html#amazon-linux-2023-amazon-machine-image","title":"Amazon Linux 2023 Amazon Machine Image","text":"<p>This Amazon Machine Image is provisioned using the Amazon Linux 2023 base operating system. It is highly configurable through variables, but there is only one variable that is required: A public SSH key. This public SSH key is used by the Jenkins orchestration service to establish an initial connection to the agent.</p> <p>This variable can be passed to Packer using the <code>-var-file</code> or <code>-var</code> command line flag. If you are using a variable file, please consult the <code>example.pkrvars.hcl</code> for overridable fields. You can also pass the SSH key directly at the command line:</p>"},{"location":"samples/simple-build-pipeline/index.html#there-are-separate-arm-and-x86-based-packer-scripts-available-for-amazon-linux-2023","title":"There are separate ARM and x86 based Packer scripts available for Amazon Linux 2023","text":""},{"location":"samples/simple-build-pipeline/index.html#arm-based-image","title":"ARM Based Image","text":"<pre><code>packer build -var \"public_key=&lt;include public key here&gt;\" amazon-linux-2023-arm64.pkr.hcl\n</code></pre>"},{"location":"samples/simple-build-pipeline/index.html#x86-based-image","title":"x86 Based Image","text":"<pre><code>packer build -var \"public_key=&lt;include public key here&gt;\" amazon-linux-2023-x86_64.pkr.hcl\n</code></pre> <p>Note: The above commands assume you are running <code>packer</code> from the <code>/assets/packer/build-agents/linux</code> directory.</p> <p>Then securely store the private key value as a secret in AWS Secrets Manager.</p> <pre><code>aws secretsmanager create-secret \\\n    --name JenkinsPrivateSSHKey \\\n    --description \"Private SSH key for Jenkins build agent access.\" \\\n    --secret-string \"&lt;insert private SSH key here&gt;\" \\\n    --tags 'Key=jenkins:credentials:type,Value=sshUserPrivateKey' 'Key=jenkins:credentials:username,Value=ec2-user'\n</code></pre> <p>Take note of the output of this CLI command. You will need the ARN later.</p>"},{"location":"samples/simple-build-pipeline/index.html#ubuntu-jammy-2204-amazon-machine-images","title":"Ubuntu Jammy 22.04 Amazon Machine Images","text":"<p>These Amazon Machine Images are provisioned using the Ubuntu Jammy 22.04 base operating system. Just like the Amazon Linux 2023 AMI above, the only required variable is a public SSH key. All Linux Packer templates use the same variables file, so if you would like to share a public key across all build nodes we recommend using a variables file. In the case you do choose to use a variable file, please consult the <code>example.pkrvars.hcl</code> for overridable fields.</p>"},{"location":"samples/simple-build-pipeline/index.html#there-are-separate-amd64-and-arm-based-packer-scripts-available-for-ubuntu-jammy-2204","title":"There are separate AMD64 and ARM based Packer scripts available for Ubuntu Jammy 22.04","text":""},{"location":"samples/simple-build-pipeline/index.html#amd64-based-image","title":"AMD64 Based Image","text":"<pre><code>packer build -var \"public_key=&lt;include public key here&gt;\" ubuntu-jammy-22.04-amd64-server.pkr.hcl\n</code></pre>"},{"location":"samples/simple-build-pipeline/index.html#arm-based-ubuntu-image","title":"ARM Based Ubuntu Image","text":"<pre><code>packer build -var \"public_key=&lt;include public key here&gt;\" ubuntu-jammy-22.04-arm64-server.pkr.hcl\n</code></pre> <p>Note: The above commands assume you are running <code>packer</code> from the <code>/assets/packer/build-agents/linux</code> directory.</p> <p>Finally, you'll want to upload the private SSH key to AWS Secrets Manager so that the Jenkins orchestration service can use it to connect to this build agent.</p> <pre><code>aws secretsmanager create-secret \\\n    --name JenkinsPrivateSSHKey \\\n    --description \"Private SSH key for Jenkins build agent access.\" \\\n    --secret-string \"&lt;insert private SSH key here&gt;\" \\\n    --tags 'Key=jenkins:credentials:type,Value=sshUserPrivateKey' 'Key=jenkins:credentials:username,Value=ubuntu'\n</code></pre> <p>Note: If you have already created a secret for any of the previous Amazon Linux images, please remember to change the name of this secret to avoid a naming conflict.</p> <p>Take note of the output of this CLI command. You will need the ARN later.</p>"},{"location":"samples/simple-build-pipeline/index.html#windows-2022-x86-based-amazon-machine-image","title":"Windows 2022 X86 based Amazon Machine Image","text":"<p>This Amazon Machine Image is provisioned using the Windows Server 2022 base operating system. It installs all required tooling for Unreal Engine 5 compilation by default. Please consult the release notes for Unreal Engine 5.4 for details on what tools are used for compiling this version of the engine.</p> <p>Again, the only required variable for building this Amazon Machine Image is a public SSH key.</p> <pre><code>packer build -var \"public_key=&lt;include public ssh key here&gt;\" windows.pkr.hcl\n</code></pre> <p>Note: The above command assumes you are running <code>packer</code> from the <code>/assets/packer/build-agents/windows</code> directory.</p> <p>Finally, you'll want to upload the private SSH key to AWS Secrets Manager so that the Jenkins orchestration service can use it to connect to this build agent.</p> <pre><code>aws secretsmanager create-secret \\\n    --name JenkinsPrivateSSHKey \\\n    --description \"Private SSH key for Jenkins build agent access.\" \\\n    --secret-string \"&lt;insert private SSH key here&gt;\" \\\n    --tags 'Key=jenkins:credentials:type,Value=sshUserPrivateKey' 'Key=jenkins:credentials:username,Value=jenkins'\n</code></pre> <p>Note: If you have already created a secret for any of the previous Amazon Linux or Ubuntu Jammy based images, please remember to change the name of this secret to avoid a naming conflict.</p> <p>Take note of the output of this CLI command. You will need the ARN later.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-4-create-route53-hosted-zone","title":"Step 4. Create Route53 Hosted Zone","text":"<p>Now that all of the required Amazon Machine Images exist we are almost ready to move on to provisioning infrastructure. However, the Simple Build Pipeline requires that we create one resource ahead of time: A Route53 Hosted Zone. The Simple Build Pipeline creates DNS records and SSL certificates for all the applications it deploys to support secure communication over the internet. However, these certificates and DNS records rely on the existence of a public hosted zone associated with your company's route domain. Since different studios may use different DNS registrars or DNS providers, the Simple Build Pipeline requires this first step to be completed manually. Everything else will be deployed automatically in the next step.</p> <p>If you do not already have a domain you can register one with Route53. When you register a domain with Route53 a public hosted zone is automatically created.</p> <p>If you already have a domain that you would like to use for the Simple Build Pipeline please consult the documentation for making Amazon Route 53 the DNS service for an existing domain.</p> <p>Once your hosted zone exists you can proceed to the next step.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-5-configure-simple-build-pipeline-variables","title":"Step 5. Configure Simple Build Pipeline Variables","text":"<p>Configurations for the Simple Build Pipeline are split between 2 files: <code>locals.tf</code> and <code>variables.tf</code>. Variables in <code>locals.tf</code> are typically static and can be modified within the file itself. Variables in <code>variables.tf</code> tend to be more dynamic and are passed in through the <code>terraform apply</code> command either directly through a <code>-var</code> flag or as file using the <code>-var-file</code> flag.</p> <p>We'll start by walking through the required configurations in <code>locals.tf</code>.</p> <p>1.</p> <p><code>jenkins_agent_secret_arns</code> is a list of AWS Secrets Manager ARNs that the Jenkins orchestration service will be granted access to. This is primarily used for providing private SSH keys to Jenkins so that the orchestration service can connect to your build agents. When you created build agent AMIs earlier you also uploaded private SSH keys to AWS Secrets Manager. The ARNs of those secrets should be added to the <code>jenkins_agent_secret_arns</code> list so that Jenkins can connect to the provisioned build agents.</p> <ol> <li> <p>The    <code>build_farm_compute</code> map contains all of the information needed to provision your Jenkins build farms. Each entry in this map corresponds to an EC2 Auto Scaling group, and requires two fields to be specified:    <code>ami</code> and <code>instance_type</code>. The    <code>local.tf</code> file contains an example configuration that has been commented out. Using the AMI IDs from Step 3, please specify the build farms you would like to provision. Selecting the right instance type for your build farm is highly dependent on your build process. Larger instances are more expensive, but provide improved performance. For example, large Unreal Engine compilation jobs will perform significantly better on Compute Optimized instances, while cook jobs tend to benefit from the increased RAM available from Memory Optimized instances. It can be a good practice to provision an EC2 instance using your custom AMI, and run your build process locally to determine the right instance size for your build farm. Once you have settled on an instance type, complete the    <code>build_farm_compute</code> map to configure your build farms.</p> </li> <li> <p>Finally, the    <code>build_farm_fsx_openzfs_storage</code> field configures file systems used by your build agents for mounting P4 Server workspaces and shared caches. Again, an example configuration is provided but commented out. Depending on the number of builds you expect to be performing and the size of your project, you may want to adjust the size of the suggested file systems.</p> </li> </ol> <p>The variables in <code>variables.tf</code> are as follows:</p> <p>.<code>route53_public_hosted_zone_name</code> must be set to the public hosted zone you created in Step 4. Your applications will be deployed at subdomains. For example, if <code>route53_public_hosted_zone_name=example.com</code> then Jenkins will be available at <code>jenkins.example.com</code> and P4 Server e will be available at <code>perforce.example.com</code>. These subdomains are configurable via <code>locals.tf</code>.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-6-deploy-simple-build-pipeline","title":"Step 6. Deploy Simple Build Pipeline","text":"<p>Now we are ready to deploy your Simple Build Pipeline! Navigate to the <code>/samples/simple-build-pipeline</code> directory and run the following commands:</p> <pre><code>terraform init\n</code></pre> <p>This will install the modules and required Terraform providers.</p> <pre><code>terraform apply -var \"route53_public_hosted_zone_name=&lt;insert your root domain&gt;\"\n</code></pre> <p>This will create a Terraform plan, and wait for manual approval to deploy the proposed resources. Once approval is given the entire deployment process takes roughly 10 minutes.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-7-configure-jenkins","title":"Step 7. Configure Jenkins","text":"<p>Now that everything is deployed, its time to configure the applications included in the Simple Build Pipeline. First, we will setup Jenkins.</p>"},{"location":"samples/simple-build-pipeline/index.html#initial-access","title":"Initial Access","text":"<p>When accessing Jenkins for the first time, an administrator's password is required. This password is auto-generated and available through the service logs.</p> <ol> <li>Open the AWS console and navigate to the Elastic Container Service (ECS) console.</li> <li>In the <code>Clusters</code> tab, select the <code>build-pipeline-cluster</code></li> <li>Select the <code>cgd-jenkins-service</code></li> <li>Select the <code>Logs</code> tab</li> <li>Scroll through the logs until you find the password, below is an example of what the password section looks like. Note that each line is shown as its own log entry in the console.</li> </ol> <p></p> <p>Open the Jenkins console in your preferred browser by navigating to <code>jenkins.&lt;your fully qualified domain name&gt;</code>, and log in using the administrator's password you just located. Install the suggested plugins and create your first admin user. For the Jenkins URL accept the default value. This URL is provided as an output on the terminal where you ran <code>terraform apply</code>.</p>"},{"location":"samples/simple-build-pipeline/index.html#useful-plugins","title":"Useful Plugins","text":"<p>There are 2 plugins recommended for the solutions: The EC2 Fleet Plugin and the AWS Secrets Manager Credentials Provider Plugin. The <code>EC2 Fleet</code> Plugin is used to integrate Jenkins with AWS and allows EC2 instances to be used as build nodes through an autoscaling group. The <code>AWS Secrets Manager Credentials Provider</code> Plugin will allow users to store their credentials in AWS Secrets Manager and seamlessly access them in Jenkins.</p> <ol> <li>Open the Jenkins console.</li> <li>On the left-hand side, select the <code>Manage Jenkins</code> tab.</li> <li>Then, under the <code>System Configuration</code> section, select <code>Plugins</code>.</li> <li>On the left-hand side, select <code>Available plugins</code>.</li> <li>Using the search bar at the top of the page, search for <code>EC2 Fleet</code>.</li> <li>Select the <code>EC2 Fleet</code> plugin.</li> <li>Using the search bar at the top of the page, search for <code>AWS Secrets Manager Credentials Provider</code>.</li> <li>Select the <code>AWS Secrets Manager Credentials Provider</code> plugin.</li> <li>Click <code>install</code> on the top-right corner of the page.</li> <li>Once the installation is complete, Select <code>Go back to the top page</code> at the bottom of the page</li> </ol>"},{"location":"samples/simple-build-pipeline/index.html#jenkins-cloud-configuration","title":"Jenkins Cloud Configuration","text":"<p>We now need to setup our Auto Scaling groups as Jenkins build agents. To do this, we will create multiple Jenkins \"Cloud\" resources; one for each of the Auto Scaling groups we deployed in the previous step.</p> <ol> <li>From the Jenkins homepage, on the left-hand side, choose <code>Manage Jenkins</code>.</li> <li>Under the <code>System Configuration</code> section, choose <code>Clouds</code></li> <li>Select <code>New Cloud</code></li> <li>Enter a name for your cloud configuration</li> <li>Select <code>Amazon EC2 Fleet</code></li> <li>Click <code>Create</code></li> <li>On the <code>New Cloud</code> configuration page, change the following settings.<ol> <li>Region - Select the region in which you deployed the Simple Build Pipeline</li> <li>EC2 Fleet - Select the autoscaling group you would like to use</li> <li>Launcher - Select <code>Launch agents via SSH</code></li> <li>Launcher -&gt; Credentials - Select the credentials associated with that particular autoscaling group</li> <li>Launcher -&gt; Host Key Verification Strategy - Select <code>Non verifying Verification Strategy</code></li> <li>Connect to instaces via private IP instead of public IP - Select the <code>Private IP</code> check box</li> <li>Max Idle Minutes Before Scaledown - Set this variable to <code>5</code> (minutes). Feel free to change this based on your needs.</li> </ol> </li> </ol> <p>Repeat the process above for each of the Auto Scaling groups you specified in your <code>build_farm_compute</code> configuration. You should now be able to reference these \"Cloud\" agents in your Jenkins pipeline definitions.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-8-configure-p4auth-formerly-helix-authentication-service","title":"Step 8. Configure P4Auth (formerly Helix Authentication Service)","text":"<p>The P4Auth provides integrations with common identity providers so that end-users of P4 Server (formerly Helix Core) and P4 Code Review (formerly Helix Swarm) can use their existing credentials to access version control and code review tools.</p> <p>The Simple Build Pipeline deploys the P4Auth with the administrator web-based UI enabled. You should be able to navigate to <code>auth.perforce.&lt;your fully qualified domain name&gt;/admin</code> to configure your external IDP. This URL is provided as an output on the terminal where you ran <code>terraform apply</code>.</p> <p>With the default configuration, the deployment of the Perforce module as part of the Simple Build Pipeline creates a random administrator password and stores it in AWS Secrets Manager. You can find this password by navigating to the AWS Secrets Manager console and viewing the secret ending in <code>-AdminUserPassword</code> The username is also available within the secret ending in <code>-AdminUsername</code>. Use these credentials to access the web UI and configure your identity provider.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-9-test-p4-server-and-p4-code-review","title":"Step 9. Test P4 Server and P4 Code Review","text":"<p>Like P4Auth, an administrator's password is created for P4 Server. The username and password are available in AWS Secrets Manager under the secrets ending in <code>-SuperUserPassword</code> and <code>-SuperUserUsername</code>. Use these credentials to access P4 Server for the first time. The P4 server connection string and P4 Code Review URLs are provided as outputds on the terminal where you ran <code>terraform apply</code>.</p> <p>Once you have access to P4 Server you should be able to provision new users. You can do this through the P4Admin GUI or from the command line. For more information please consult the P4 Server documentation. Users provisioned with an email address that corresponds with the identity provider configured in P4Auth will be able to use their existing credentials to log in to P4 Server and P4 Code Review.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-10-cleanup","title":"Step 10. Cleanup","text":"<p>Tearing down the resources created by the Simple Build Pipeline is as easy as running <code>terraform destroy</code> in the <code>/samples/simple-build-pipeline</code> directory. However, this will not delete the secrets you've uploaded, the AMIs created with Packer, or the the Route53 hosted zone you set up initially. Those resources will need to be explicitly destroyed using the AWS console or relevant CLI commands.</p>"},{"location":"samples/simple-build-pipeline/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws ~&gt; 6.6 http ~&gt; 3.5 netapp-ontap ~&gt; 2.3"},{"location":"samples/simple-build-pipeline/index.html#providers","title":"Providers","text":"Name Version aws ~&gt; 6.6 http ~&gt; 3.5"},{"location":"samples/simple-build-pipeline/index.html#modules","title":"Modules","text":"Name Source Version jenkins ../../modules/jenkins n/a terraform-aws-perforce ../../modules/perforce n/a"},{"location":"samples/simple-build-pipeline/index.html#resources","title":"Resources","text":"Name Type aws_acm_certificate.shared resource aws_acm_certificate_validation.shared_certificate resource aws_default_security_group.default resource aws_ecs_cluster.build_pipeline_cluster resource aws_ecs_cluster_capacity_providers.providers resource aws_eip.nat_gateway_eip resource aws_internet_gateway.igw resource aws_lb.service_nlb resource aws_lb.web_alb resource aws_lb_listener.internal_https resource aws_lb_listener.public_https resource aws_lb_listener_rule.jenkins resource aws_lb_listener_rule.perforce_auth resource aws_lb_listener_rule.perforce_code_review resource aws_lb_target_group.alb_target resource aws_lb_target_group_attachment.alb_attachment resource aws_nat_gateway.nat_gateway resource aws_route.private_rt_nat_gateway resource aws_route53_record.jenkins_private resource aws_route53_record.jenkins_public resource aws_route53_record.p4_server_private resource aws_route53_record.p4_server_public resource aws_route53_record.p4_web_services_private resource aws_route53_record.p4_web_services_public resource aws_route53_record.shared_certificate resource aws_route53_zone.private_zone resource aws_route_table.private_rt resource aws_route_table.public_rt resource aws_route_table_association.private_rt_asso resource aws_route_table_association.public_rt_asso resource aws_security_group.allow_my_ip resource aws_security_group.internal_shared_application_load_balancer resource aws_security_group.public_network_load_balancer resource aws_subnet.private_subnets resource aws_subnet.public_subnets resource aws_vpc.build_pipeline_vpc resource aws_vpc_security_group_egress_rule.internal_alb_http_to_jenkins resource aws_vpc_security_group_egress_rule.internal_alb_http_to_p4_auth resource aws_vpc_security_group_egress_rule.internal_alb_http_to_p4_code_review resource aws_vpc_security_group_egress_rule.public_nlb_https_to_internal_alb resource aws_vpc_security_group_ingress_rule.allow_https resource aws_vpc_security_group_ingress_rule.allow_perforce resource aws_vpc_security_group_ingress_rule.internal_alb_https_from_p4_server resource aws_vpc_security_group_ingress_rule.internal_alb_https_from_public_nlb resource aws_vpc_security_group_ingress_rule.jenkins_http_from_internal_alb resource aws_vpc_security_group_ingress_rule.p4_auth_http_from_internal_alb resource aws_vpc_security_group_ingress_rule.p4_code_review_http_from_internal_alb resource aws_vpc_security_group_ingress_rule.p4_server_from_jenkins_build_farm resource aws_vpc_security_group_ingress_rule.p4_server_from_jenkins_service resource aws_ami.ubuntu_noble_amd data source aws_availability_zones.available data source aws_route53_zone.root data source http_http.my_ip data source"},{"location":"samples/simple-build-pipeline/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required route53_public_hosted_zone_name The fully qualified domain name of your existing Route53 Hosted Zone. <code>string</code> n/a yes"},{"location":"samples/simple-build-pipeline/index.html#outputs","title":"Outputs","text":"Name Description jenkins_url The URL for the Jenkins service. p4_auth_admin_url The URL for the P4Auth service admin page. p4_code_review_url The URL for the P4 Code Review service. p4_server_connection_string The connection string for the P4 Server. Set your P4PORT environment variable to this value."},{"location":"samples/unreal-cloud-ddc-single-region/index.html","title":"Unreal Cloud DDC Single Region","text":"<p>The Unreal Cloud DDC Single Region is a comprehensive solution that leverages several AWS services to create a robust and efficient data caching system. It uses a well-designed Virtual Private Cloud (VPC) to ensure network isolation and security. The solution employs an Amazon Elastic Kubernetes Service (EKS) Cluster with Node Groups to manage and orchestrate containerized applications.</p> <p>At the heart of the system is an instance of ScyllaDB, a high-performance NoSQL database, running on specially optimized Amazon EC2 instances. The Unreal Cloud Derived Data Cache Container is managed by Helm, a package manager for Kubernetes, and uses Amazon S3 for durable storage.</p>"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#predeployment-set-up-github-content-repository-credentials","title":"Predeployment - Set Up Github Content Repository Credentials","text":"<p>The Unreal Cloud DDC Intra Cluster module utilizes a pull through cache to access the Unreal Cloud DDC image. This requires a secret in Secrets Manager. The secret needs to be prefixed with <code>ecr-pullthroughcache/</code>. Additionally, the secret is required to be in the following format:</p> <pre><code>{\n  \"username\":\"GITHUB-USER-NAME-PLACEHOLDER\",\n  \"access-token\":\"GITHUB-ACCESS-TOKEN-PLACEHOLDER\"\n}\n</code></pre>"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#deployment","title":"Deployment","text":"<p>Once you've completed the prerequisites and set your variables, you can deploy the solution by running:</p> <pre><code>terraform apply\n</code></pre> <p>The deployment can take close to 30 minutes. Creating the EKS Node Groups and EKS Cluster take around 20 minutes to fully deploy.</p>"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#postdeployment","title":"Postdeployment","text":"<p>The sample deploys a Route53 dns record that you can use to access your Unreal DDC cluster. This record points to an NLB which may take more time to become fully available when the deployment is complete. You can view the provisioning status of this NLB on the EC2 load balncing screen.</p> <p>The Unreal Cloud DDC module creates a Service Account and valid bearer token for testing. This bearer token is stored in AWS Secrets Manager. The ARN of this secret is provided as a Terraform output (<code>\"unreal_cloud_ddc_bearer_token_arn\"</code>) on the console following deployment. To fetch the bearer token you can use the aws CLI:</p> <pre><code>aws secretsmanager get-secret-value --secret-id &lt;\"unreal_cloud_ddc_bearer_token_arn\"&gt;\n</code></pre> <p>To validate you can put an object you can run:</p> <pre><code>curl http://&lt;unreal_ddc_url&gt;/api/v1/refs/ddc/default/00000000000000000000000000000000000000aa -X PUT --data 'test' -H 'content-type: application/octet-stream' -H 'X-Jupiter-IoHash: 4878CA0425C739FA427F7EDA20FE845F6B2E46BA' -i -H 'Authorization: ServiceAccount &lt;secret-manager-token&gt;'\n</code></pre> <p>After running this you should get a response that looks as the following:</p> <pre><code>HTTP/1.1 200 OK\nServer: nginx\nDate: Wed, 29 Jan 2025 19:15:05 GMT\nContent-Type: application/json; charset=utf-8\nTransfer-Encoding: chunked\nConnection: keep-alive\nServer-Timing: blob.put.FileSystemStore;dur=0.1451;desc=\"PUT to store: 'FileSystemStore'\",blob.put.AmazonS3Store;dur=267.0449;desc=\"PUT to store: 'AmazonS3Store'\",blob.get-metadata.FileSystemStore;dur=0.0406;desc=\"Blob GET Metadata from: 'FileSystemStore'\",ref.finalize;dur=7.1407;desc=\"Finalizing the ref\",ref.put;dur=25.2064;desc=\"Inserting ref\"\n\n{\"needs\":[]}%\n</code></pre> <p>You can then access the same chunk with the following command:</p> <pre><code>curl http://&lt;unreal_ddc_url&gt;/api/v1/refs/ddc/default/00000000000000000000000000000000000000aa.json -i -H 'Authorization: ServiceAccount &lt;unreal-cloud-ddc-bearer-token&gt;'\n</code></pre> <p>The response should look like the following:</p> <pre><code>HTTP/1.1 200 OK\nServer: nginx\nDate: Wed, 29 Jan 2025 19:16:46 GMT\nContent-Type: application/json\nContent-Length: 66\nConnection: keep-alive\nX-Jupiter-IoHash: 7D873DCC262F62FBAA871FE61B2B52D715A1171E\nX-Jupiter-LastAccess: 01/29/2025 19:16:46\nServer-Timing: ref.get;dur=0.0299;desc=\"Fetching Ref from DB\"\n\n{\"RawHash\":\"4878ca0425c739fa427f7eda20fe845f6b2e46ba\",\"RawSize\":4}%\n</code></pre> <p>For a more comprehensive test of your deployment, we recommend using the bench marking tools. To do so we used a x2idn.32xlarge as it matched Epic's benchmarking instance to test their configuration.</p> <p>With the benchmarking tools we ran the following command after compiling the docker image:</p> <pre><code>docker run --network host jupiter_benchmark --seed --seed-remote --host http://&lt;unreal_ddc_url&gt; --namespace ddc \\\n--header=\"Authorization: ServiceAccount &lt;unreal-cloud-ddc-bearer-token&gt;\" all\n</code></pre> <p>Just a note here, you will have to specify the namespace to be DDC as the token only has access to that namespace.</p> <p>It is recommended that if you are using this in a production capacity you change the authentication mode from Service Account to Bearer and use an IDP to authenticate and TLS termination.</p> <p>This sample also deploys a ScyllaDB monitoring stack, enabling real-time insights into the status and performance of your ScyllaDB nodes. The monitoring stack includes Prometheus for metrics collection, Alertmanager for handling alerts, and Grafana for visualization. You can access the Grafana dashboard by using the <code>\"monitoring_url\"</code> provided in the sample outputs. To learn more about the ScyllaDB monitoring stack, refer to the ScyllaDB Monitoring Stack Documentation.</p>"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10.3 aws &gt;= 5.89.0 awscc &gt;= 1.26.0 helm &gt;= 2.9.0, &lt; 3.0.0 http &gt;= 3.4.5 kubernetes &gt;= 2.24.0 random 3.7.2"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#providers","title":"Providers","text":"Name Version aws &gt;= 5.89.0 awscc &gt;= 1.26.0 http &gt;= 3.4.5"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#modules","title":"Modules","text":"Name Source Version unreal_cloud_ddc_infra ../../modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra n/a unreal_cloud_ddc_intra_cluster ../../modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster n/a unreal_cloud_ddc_vpc ./vpc n/a"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#resources","title":"Resources","text":"Name Type aws_acm_certificate.scylla_monitoring resource aws_acm_certificate_validation.scylla_monitoring resource aws_route53_record.scylla_monitoring resource aws_route53_record.scylla_monitoring_cert resource aws_route53_record.unreal_cloud_ddc resource aws_security_group.unreal_ddc_load_balancer_access_security_group resource aws_vpc_security_group_egress_rule.unreal_ddc_load_balancer_egress_sg_rules resource aws_vpc_security_group_ingress_rule.unreal_ddc_load_balancer_http2_ingress_rule resource aws_vpc_security_group_ingress_rule.unreal_ddc_load_balancer_http_ingress_rule resource aws_vpc_security_group_ingress_rule.unreal_ddc_load_balancer_https_ingress_rule resource awscc_secretsmanager_secret.unreal_cloud_ddc_token resource aws_availability_zones.available data source aws_caller_identity.current data source aws_ecr_authorization_token.token data source aws_region.current data source aws_route53_zone.root data source aws_secretsmanager_secret_version.unreal_cloud_ddc_token data source http_http.public_ip data source"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required github_credential_arn Github Credential ARN <code>string</code> n/a yes route53_public_hosted_zone_name The root domain name for the Hosted Zone where the ScyllaDB monitoring record should be created. <code>string</code> n/a yes allow_my_ip Automatically add your IP to the security groups allowing access to the Unreal DDC and SycllaDB Monitoring load balancers <code>bool</code> <code>true</code> no"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#outputs","title":"Outputs","text":"Name Description monitoring_url n/a unreal_cloud_ddc_bearer_token_arn n/a unreal_ddc_url n/a"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.10.3 aws &gt;= 5.89.0 awscc &gt;= 1.26.0 helm &gt;= 2.9.0 http &gt;= 3.4.5 kubernetes &gt;= 2.24.0 random 3.7.2"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#providers_1","title":"Providers","text":"Name Version aws &gt;= 5.89.0 awscc &gt;= 1.26.0 http &gt;= 3.4.5"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#modules_1","title":"Modules","text":"Name Source Version unreal_cloud_ddc_infra ../../modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra n/a unreal_cloud_ddc_intra_cluster ../../modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster n/a unreal_cloud_ddc_vpc ./vpc n/a"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#resources_1","title":"Resources","text":"Name Type aws_acm_certificate.scylla_monitoring resource aws_acm_certificate_validation.scylla_monitoring resource aws_route53_record.scylla_monitoring resource aws_route53_record.scylla_monitoring_cert resource aws_route53_record.unreal_cloud_ddc resource aws_security_group.unreal_ddc_load_balancer_access_security_group resource aws_vpc_security_group_egress_rule.unreal_ddc_load_balancer_egress_sg_rules resource aws_vpc_security_group_ingress_rule.unreal_ddc_load_balancer_http2_ingress_rule resource aws_vpc_security_group_ingress_rule.unreal_ddc_load_balancer_http_ingress_rule resource aws_vpc_security_group_ingress_rule.unreal_ddc_load_balancer_https_ingress_rule resource awscc_secretsmanager_secret.unreal_cloud_ddc_token resource aws_availability_zones.available data source aws_caller_identity.current data source aws_ecr_authorization_token.token data source aws_region.current data source aws_route53_zone.root data source aws_secretsmanager_secret_version.unreal_cloud_ddc_token data source http_http.public_ip data source"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#inputs_1","title":"Inputs","text":"Name Description Type Default Required allow_my_ip Automatically add your IP to the security groups allowing access to the Unreal DDC and SycllaDB Monitoring load balancers <code>bool</code> <code>true</code> no github_credential_arn Github Credential ARN <code>string</code> n/a yes route53_public_hosted_zone_name The root domain name for the Hosted Zone where the ScyllaDB monitoring record should be created. <code>string</code> n/a yes"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#outputs_1","title":"Outputs","text":"Name Description monitoring_url n/a unreal_cloud_ddc_bearer_token_arn n/a unreal_ddc_url n/a"}]}