{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Cloud Game Development Toolkit","text":"<p>The Cloud Game Development Toolkit (a.k.a. CGD Toolkit) is a collection of templates and configurations for deploying game development infrastructure and tools on AWS.</p> <p>The project is designed for piecemeal usage:</p> <ul> <li>Already have a CI/CD pipeline deployed but need a build machine image? </li> <li>Looking to migrate your Perforce server from on-premise to AWS? </li> <li>Starting your new studio from the ground up and looking for pre-built templates to deploy common infrastructure? </li> </ul> <p>The Toolkit consists of three key components:</p> Component Description Assets Reusable scripts, pipeline definitions, Dockerfiles, Packer templates, Ansible Playbooks to configure workloads after deployment, and other resources that might prove useful or are dependencies of any of the modules. Modules Highly configurable and extensible Terraform modules for simplified deployment of key game development infrastructure on AWS with best-practices by default. Samples Complete Terraform configurations for expedited studio setup that demonstrate module usage and integration with other AWS services."},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>\ud83d\udcd6 Documentation  |  \ud83d\udcbb Contribute to the Project  |  \ud83d\udcac Ask Questions  |  \ud83d\udea7 Roadmap</p>"},{"location":"index.html#security","title":"Security","text":"<p>If you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions here or email AWS security directly.</p>"},{"location":"index.html#license","title":"License","text":"<p>This project is licensed under the MIT-0 License. See the LICENSE file.</p>"},{"location":"CHANGELOG.html","title":"CHANGELOG","text":""},{"location":"CHANGELOG.html#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG.html#latest-2025-06-09","title":"latest - 2025-06-09","text":""},{"location":"CHANGELOG.html#v114-2025-04-28","title":"v1.1.4 - 2025-04-28","text":""},{"location":"CHANGELOG.html#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Remove commented out NetApp volume resources and cleanup IAM managed policies</li> <li>Resolved EC2 DNS self-signed certificate bug in P4 Server packer template</li> <li>Adding cloud DDC sample for mkdocs.yml</li> <li>helix swarm: helix swarm does not support horizontal scaling, so helix swarm container count is now set to 1</li> </ul>"},{"location":"CHANGELOG.html#chore","title":"Chore","text":"<ul> <li>Add Terraform tests for new Perforce module (#604)</li> <li>regenerate CHANGELOG.md for 2025-03-19</li> <li>Minor maintenance to Helix Core module</li> <li>Minor Helix Authentication fixes</li> <li>Addressed IAM policy warnings for Helix Swarm</li> <li>deps: bump xt0rted/pull-request-comment-branch from 1 to 3</li> <li>deps: bump mkdocs-material from 9.6.11 to 9.6.12 in /docs</li> <li>deps: bump mkdocs-material from 9.6.12 to 9.6.14 in /docs</li> <li>deps: bump actions/github-script from 6 to 7</li> <li>deps: bump actions/checkout from 3 to 4</li> <li>deps: bump mkdocs-material from 9.6.9 to 9.6.11 in /docs</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump aquasecurity/trivy-action from 0.29.0 to 0.30.0</li> <li>deps: bump mkdocs-material from 9.6.8 to 9.6.9 in /docs</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.8 to 9.6.9 in /docs</li> <li>deps: bump actions/upload-artifact from 4.6.1 to 4.6.2</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> </ul>"},{"location":"CHANGELOG.html#code-refactoring","title":"Code Refactoring","text":"<ul> <li>Update Simple Build Pipeline sample to use new Perforce parent module (#608)</li> <li>Perforce modules consolidated to simplify shared resource creation (#585)</li> <li>Updated Perforce complete example to remove NLB front for Helix Core</li> <li>reorganize unreal cloud ddc module structure</li> </ul>"},{"location":"CHANGELOG.html#docs","title":"Docs","text":"<ul> <li>Adjustments to mkdocs structure, and updates to \"getting started\" and Perforce documentation. (#612)</li> <li>updates and expands on <code>unreal-cloud-ddc-intra-cluster</code> installation and usage docs</li> <li>fixes relative path for <code>unreal-cloud-ddc-infra</code> and <code>unreal-cloud-ddc-intra-cluster</code> Terraform module docs</li> <li>add unreal fest video to horde module</li> <li>TeamCity: Adding TeamCity module docs and example architecture</li> </ul>"},{"location":"CHANGELOG.html#features","title":"Features","text":"<ul> <li>Adds debug variable and flag</li> <li>Simple example deployment of Helix Core backed by FSxN</li> <li>FSxN ISCSI provisioning for Helix Core module</li> <li>Modified p4_configure.sh to mount ISCSI volumes from FSxN</li> </ul>"},{"location":"CHANGELOG.html#v113-alpha-2025-03-19","title":"v1.1.3-alpha - 2025-03-19","text":""},{"location":"CHANGELOG.html#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>create_external_alb shouldn't block internal SG Ingress rules</li> <li>alb_subnet variables should not be required if create boolean is false</li> <li>Attaching perforce web service ALB to target group</li> <li>use provided admin password secret for Helix Authentication Service ADMIN_PASSWD, instead of the username secret</li> <li>AMI version bump for Helix Core, region variable made optional</li> </ul>"},{"location":"CHANGELOG.html#chore_1","title":"Chore","text":"<ul> <li>update dependabot configuration to include unreal modules</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.4 to 9.6.7 in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.4 to 9.6.7 in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump actions/upload-artifact from 4.6.0 to 4.6.1</li> <li>deps: bump ossf/scorecard-action from 2.4.0 to 2.4.1</li> <li>deps: bump hashicorp/random</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.7 to 9.6.8 in /docs</li> <li>deps: bump the random-provider group across 4 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.3 to 9.6.4 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.2 to 9.6.4 in /docs</li> <li>deps: bump actions/upload-artifact from 4.5.0 to 4.6.0</li> <li>deps: bump mkdocs-material from 9.6.7 to 9.6.8 in /docs</li> <li>deps: bump hashicorp/aws</li> <li>deps: bump mkdocs-material from 9.6.2 to 9.6.3 in /docs</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.1 to 9.6.2 in /docs</li> <li>deps: bump mkdocs-material from 9.5.50 to 9.6.2 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump release-drafter/release-drafter from 6.0.0 to 6.1.0</li> <li>deps: bump mkdocs-material from 9.5.49 to 9.5.50 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> </ul>"},{"location":"CHANGELOG.html#docs_1","title":"Docs","text":"<ul> <li>fix broken link in readme</li> <li>Perforce: Updating documentation for Perforce Complete example reference architecture</li> </ul>"},{"location":"CHANGELOG.html#features_1","title":"Features","text":"<ul> <li>Helix Authentication Service: Shifting ALB creation to support external networking configuration</li> <li>Helix Core: Plaintext support for Helix Core, optional EIP creation</li> <li>Helix Core: Adding plaintext variable to p4_configre.sh</li> <li>Helix Swarm: Shifting ALB creation to support external networking configuration</li> <li>Perforce Example: Update complete example for shared networking configuration across services</li> <li>TeamCity Example: example terraform configuration for deploying TeamCity module</li> <li>TeamCity Server: terraform module for deploying TeamCity server on ECS Fargate</li> </ul>"},{"location":"CHANGELOG.html#v112-alpha-2024-12-20","title":"v1.1.2-alpha - 2024-12-20","text":""},{"location":"CHANGELOG.html#chore_2","title":"Chore","text":"<ul> <li>regenerate CHANGELOG.md for 2024-12-20</li> <li>ignore tf backend.tf files in .gitignore</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump actions/upload-artifact from 4.4.3 to 4.5.0</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> </ul>"},{"location":"CHANGELOG.html#docs_2","title":"Docs","text":"<ul> <li>removed READMEs from source directories and moved them to their own dedicated docs pages in docs/ dir</li> <li>update contributor documentation to include table of contents</li> <li>updates to doc formatting and fixed broken links</li> </ul>"},{"location":"CHANGELOG.html#v111-alpha-2024-12-17","title":"v1.1.1-alpha - 2024-12-17","text":""},{"location":"CHANGELOG.html#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Added service target group ARNs as outputs for HAS and Swarm</li> <li>Adds defaults to <code>vpc_id</code> and <code>subnet_id</code> variables</li> <li>bash error causing build failure when running p4_configure.sh (#367)</li> <li>horde: add JwtIssuer to ensure container retains agents on restart</li> <li>horde: allow inbound access to horde agents on ports 7000-7010 from other horde agents</li> <li>perforce: fixed minor issues in p4_configure.sh</li> <li>perforce: add Unicode support and fix main module to handle existing security groups</li> </ul>"},{"location":"CHANGELOG.html#chore_3","title":"Chore","text":"<ul> <li>make SELinux label updates configurable</li> <li>remove packer assets .ci directory (#337)</li> <li>fix tag names so that they match recommended best practices (#343)</li> <li>define nat gateway routes for private route tables outside of aws_route_table resources in samples and modules (#354)</li> <li>adds triage label to our issue templates</li> <li>regenerate CHANGELOG.md for 2024-12-17</li> <li>document parameter values for '--unicode' flag</li> <li>provide appropriate association name for configuring Helix Core via SSM</li> <li>fix naming</li> <li>checkov: Suppresses CKV_AWS_378 rule (#339)</li> <li>deps: bump mkdocs-material from 9.5.42 to 9.5.44 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump aquasecurity/trivy-action from 0.28.0 to 0.29.0</li> <li>deps: bump mkdocs-material from 9.5.45 to 9.5.46 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.5.44 to 9.5.45 in /docs</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.7 to 1.0.8 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump actions/checkout from 3.0.0 to 4.2.2</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump hashicorp/setup-terraform from 1 to 3</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump mkdocs-material from 9.5.41 to 9.5.42 in /docs</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.6 to 1.0.7 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump aquasecurity/trivy-action from 0.24.0 to 0.28.0</li> <li>deps: bump mkdocs-material from 9.5.40 to 9.5.41 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump python from 3.12.7 to 3.13.0 in /docs (#349)</li> <li>deps: bump actions/upload-artifact from 4.4.0 to 4.4.3 (#356)</li> <li>deps: bump mkdocs-material from 9.5.39 to 9.5.40 in /docs (#359)</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.5 to 1.0.6 in /docs (#345)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.5.37 to 9.5.39 in /docs (#335)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#344)</li> <li>deps: bump mkdocs-material from 9.5.46 to 9.5.48 in /docs</li> <li>deps: bump python from 3.12.6 to 3.12.7 in /docs (#340)</li> <li>deps: bump mkdocs-material from 9.5.48 to 9.5.49 in /docs</li> <li>deps: bump python from 3.13.0 to 3.13.1 in /docs</li> </ul>"},{"location":"CHANGELOG.html#docs_3","title":"Docs","text":"<ul> <li>clarify that modules are intended to be depended on, and samples are reference implementations meant to be copied and modified</li> <li>fix formatting of simple build pipeline docs</li> <li>fix formatting of local.tf in simple build pipeline docs</li> <li>fix formatting of jenkins pipeline assets page</li> <li>clarify use case of Ansible playbooks vs Packer templates</li> <li>clarify that deploying multiple samples independently is not supported</li> <li>point users explicitly to a Classic GitHub Personal Access Token</li> <li>fix typo in getting started guide</li> <li>Updates the getting started instructions for the simple build pipeline sample</li> </ul>"},{"location":"CHANGELOG.html#features_2","title":"Features","text":"<ul> <li>perforce: implement Helix Core setup playbook</li> </ul>"},{"location":"CHANGELOG.html#v110-alpha-2024-10-01","title":"v1.1.0-alpha - 2024-10-01","text":""},{"location":"CHANGELOG.html#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>improve stability of build agent packer scripts, adjust winrm timeout to 15 minutes, remove packer variables that aren't needed (#318)</li> </ul>"},{"location":"CHANGELOG.html#chore_4","title":"Chore","text":"<ul> <li>update changelog (#305)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#323)</li> <li>deps: bump mkdocs-material from 9.5.35 to 9.5.37 in /docs (#314)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#324)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#298)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#291)</li> <li>deps: bump the random-provider group across 5 directories with 1 update (#310)</li> <li>deps: bump mkdocs-material from 9.5.34 to 9.5.35 in /docs (#287)</li> </ul>"},{"location":"CHANGELOG.html#docs_4","title":"Docs","text":"<ul> <li>add perforce complete example in docs (#333)</li> <li>updates to documentation (#329)</li> </ul>"},{"location":"CHANGELOG.html#features_3","title":"Features","text":"<ul> <li>install requirements for (auto)mounting FSx volumes on Jenkins Windows build agents (#319)</li> <li>helix-core: add ARM64 support (#239)</li> </ul>"},{"location":"CHANGELOG.html#v101-alpha-2024-09-16","title":"v1.0.1-alpha - 2024-09-16","text":""},{"location":"CHANGELOG.html#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>changelog automation (#261)</li> <li>adding branch creation to workflow (#259)</li> <li>dependabot grouping terraform providers (#228)</li> <li>wait for cloud-init to complete prior to installing packages during Perforce Helix Core AMI creation (#193)</li> <li>changelog: GHA bot committer (#255)</li> <li>changelog: Add automated PR creation (#252)</li> <li>fsx_automounter: when FSx automounter can't list tags for an FSx volume, the AccessDenied exception is now treated as a warning (#226)</li> <li>p4_configure: resolve script execution errors and repair broken \u2026 (#232)</li> </ul>"},{"location":"CHANGELOG.html#chore_5","title":"Chore","text":"<ul> <li>adjusting changelog automation to leverage GH api (#266)</li> <li>update changelog workflow (#284)</li> <li>update changelog (#285)</li> <li>deps: bump hashicorp/awscc from 1.10.0 to 1.11.0 in /samples/simple-build-pipeline (#220)</li> <li>deps: bump hashicorp/awscc from 1.9.0 to 1.10.0 in /modules/perforce/helix-core (#207)</li> <li>deps: bump mkdocs-material from 9.5.33 to 9.5.34 in /docs (#236)</li> <li>deps: bump actions/upload-artifact from 4.3.6 to 4.4.0 (#235)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#241)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#242)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#233)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#231)</li> <li>deps: bump mkdocs-material from 9.5.32 to 9.5.33 in /docs (#229)</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.3 to 1.0.5 in /docs (#263)</li> <li>deps: bump mkdocs-material from 9.5.31 to 9.5.32 in /docs (#211)</li> <li>deps: bump python from 3.12 to 3.12.6 in /docs (#243)</li> <li>deps: bump hashicorp/awscc from 1.9.0 to 1.10.0 in /modules/perforce/helix-authentication-service (#205)</li> <li>deps: bump hashicorp/aws from 5.62.0 to 5.63.1 in /samples/simple-build-pipeline (#216)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /modules/perforce/helix-authentication-service (#196)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-authentication-service (#197)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /modules/perforce/helix-core (#198)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-core (#199)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-swarm (#200)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /samples/simple-build-pipeline (#201)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /samples/simple-build-pipeline (#202)</li> <li>deps: bump mike from 2.1.2 to 2.1.3 in /docs (#189)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/jenkins (#195)</li> </ul>"},{"location":"CHANGELOG.html#docs_5","title":"Docs","text":"<ul> <li>add openssf scorecard badge to readme (#219)</li> <li>link to installation instructions for required tools, fix packer command invocation instructions (#194)</li> <li>Windows Build AMI README (#187)</li> </ul>"},{"location":"CHANGELOG.html#v100-alpha-2024-08-07","title":"v1.0.0-alpha - 2024-08-07","text":""},{"location":"CHANGELOG.html#staging-2024-08-07","title":"staging - 2024-08-07","text":""},{"location":"CHANGELOG.html#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>fix issue where SSH public key was not baked into the Windows Jenkins build agent AMI (#150)</li> <li>bug fixes for FSxZ storage in build farm (#152)</li> <li>allow Jenkins build agents to discover FSx volumes/snapshots and make outbound Internet connections (#147)</li> </ul>"},{"location":"CHANGELOG.html#chore_6","title":"Chore","text":"<ul> <li>add CODEOWNERS file (#132)</li> <li>Updates to docs (#63)</li> <li>fix makefile (#65)</li> <li>Modify version handling in Docs (#66)</li> <li>deps: bump mkdocs-material from 9.5.27 to 9.5.28 in /docs (#135)</li> <li>deps: bump mkdocs-material from 9.5.26 to 9.5.27 in /docs (#77)</li> <li>deps: bump aquasecurity/trivy-action from 0.23.0 to 0.24.0 (#137)</li> <li>deps: bump actions/upload-artifact from 4.3.3 to 4.3.4 (#136)</li> <li>deps: bump actions/upload-artifact from 4.3.5 to 4.3.6 (#178)</li> <li>deps: bump mkdocs-material from 9.5.29 to 9.5.30 in /docs (#153)</li> <li>deps: bump mike from 2.1.1 to 2.1.2 in /docs (#110)</li> <li>deps: bump mkdocs-material from 9.5.28 to 9.5.29 in /docs (#144)</li> <li>deps: bump github/codeql-action from 3.25.8 to 3.25.10 (#69)</li> <li>deps: bump ossf/scorecard-action from 2.3.3 to 2.4.0 (#167)</li> <li>deps: bump actions/upload-artifact from 4.3.4 to 4.3.5 (#171)</li> <li>deps: bump mkdocs-material from 9.5.30 to 9.5.31 in /docs (#172)</li> <li>deps: bump github/codeql-action from 3.24.9 to 3.25.8 (#53)</li> <li>deps: bump mkdocs-material from 9.5.25 to 9.5.26 in /docs (#54)</li> </ul>"},{"location":"CHANGELOG.html#code-refactoring_1","title":"Code Refactoring","text":"<ul> <li>Perforce Helix Core AMI revamp, simple build pipeline DNS (#73)</li> </ul>"},{"location":"CHANGELOG.html#docs_6","title":"Docs","text":"<ul> <li>update changelog (#181)</li> <li>update main docs page (#179)</li> <li>update layout of documentation main page theme (#175)</li> <li>update documentation (#163)</li> <li>update workflow for docs (#129)</li> <li>update workflow (#128)</li> <li>fix workflow to use gh inputs from workflow (#127)</li> <li>update to docs and flip release workflow to manual (#126)</li> <li>fix commit depth (#125)</li> <li>modify the workflow for docs release and update documentation (#124)</li> <li>fix docs ci (#123)</li> <li>modify git fetch-depth for docs ci (#121)</li> <li>update README.md (#119)</li> <li>consolidate Ansible playbooks under assets (#117)</li> <li>fix url to documentation to point to /latest (#80)</li> <li>add GH Pull Request template (#67)</li> <li>updates workflow and adds changelog automation (#61)</li> <li>add issue template for RFCs (#57)</li> <li>add git-chglog for changelog generation (#49)</li> <li>enable workflow dispatch (#36)</li> <li>fix docs release workflow (#34)</li> <li>convert docs releases to use mike (#33)</li> <li>adds markdown docs for assets, modules, playbooks, and samples (#32)</li> <li>adds issue template for submitting maintenance issues (#31)</li> <li>Adds documentation and GH workflow for build/publish of docs (#21)</li> <li>Updates to project README (#20)</li> <li>Adds project docs (#13)</li> </ul>"},{"location":"CHANGELOG.html#features_4","title":"Features","text":"<ul> <li>Added getting-started documentation for quickstart with Simple Build Pipeline (#177)</li> <li>Updates to CI configurations for pre-commit and GHA (#154)</li> <li>Helix Authentication Extension (#82)</li> <li>enable web based administration through variables for HAS (#79)</li> <li>complete sample with both Jenkins and Perforce modules (#60)</li> <li>Add packer build agent templates for Linux (Ubuntu Jammy 22.04, Amazon Linux 2023) (#46)</li> <li>devops: Add new DevOps playbook files (#76)</li> <li>packer: switch AMI from Rocky Linux to Amazon Linux 2023 and up\u2026 (#141)</li> </ul>"},{"location":"CODE_OF_CONDUCT.html","title":"CODE OF CONDUCT","text":""},{"location":"CODE_OF_CONDUCT.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"CONTRIBUTING.html","title":"Table of contents","text":"<ul> <li>Contributing Guidelines</li> <li>Reporting Bugs/Feature Requests</li> <li>Contributing via Pull Requests</li> <li>Conventional Commits</li> <li>Finding contributions to work on</li> <li>Code of Conduct</li> <li>Security issue notifications</li> <li>Licensing</li> </ul>"},{"location":"CONTRIBUTING.html#contributing-guidelines","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"CONTRIBUTING.html#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"CONTRIBUTING.html#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages. See the conventional commits section for more details.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p>"},{"location":"CONTRIBUTING.html#conventional-commits","title":"Conventional Commits","text":"<p>This project uses Conventional Commits following the release of v1.0.0-alpha. These conventions ensure that the commit history of the project remains readable, and supports extensive automation around pull request creation, release cadence, and documentation.</p> <p>We do not enforce conventional commits on contributors. We do require that pull request titles follow convention so that the changelog and release automation work as expected.</p>"},{"location":"CONTRIBUTING.html#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"CONTRIBUTING.html#building-and-testing-project-documetation","title":"Building and Testing Project Documetation","text":"<p>This project uses Material for MkDocs to generate versioned documentation automatically. Content for the documentation is auto-generated by referencing the markdown contained in the project's <code>README.md</code> files.</p> <p>Local development:</p> <ul> <li>To build the project documentation, execute the following command: <code>make docs-build-local VERSION=&lt;VERSION&gt; ALIAS=&lt;ALIAS&gt;</code>, including a semantic release version (i.e. <code>v1.0.0</code>) and an alias (i.e. <code>latest</code>). This will build the project using <code>./docs/Dockerfile</code>.</li> <li>To run the documentation locally, execute the following command: <code>make docs-run VERSION=&lt;VERSION&gt; ALIAS=&lt;ALIAS&gt;</code> which runs the documentation in a local container using <code>mkdocs serve</code> on port <code>8000</code>.</li> </ul> <p>Live documentation (GitHub Pages):</p> <ul> <li>A Github workflow is used to build and deploy the documentation to the gh-pages branch: <code>.github/workflows/docs.yml</code></li> </ul>"},{"location":"CONTRIBUTING.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"CONTRIBUTING.html#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"CONTRIBUTING.html#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"SECURITY.html","title":"SECURITY","text":""},{"location":"SECURITY.html#overview","title":"Overview","text":"<p>This project is maintained by members of the AWS for Games technical community within AWS (i.e. Solutions Architects, Technical Account Managers, Software Engineers) who support the gaming industry. Design decisions and tradeoffs made throughout this project reflect our experiences working with game studios to build and maintain their development infrastructure and tools in the cloud. We encourage contributions from the community via Pull Requests, which are manually reviewed and tested by the core maintainers of the project before they are merged.</p> <p>We rely on Open Source Security (OpenSSF) Scorecard assessments to validate our project's security posture against a common set of open source project risks. We also self-certify our security practices against the standards defined by OpenSSF Best Practices Badge Program. The badges above are hyperlinks that you can follow to review the results of each.</p>"},{"location":"SECURITY.html#reporting-a-vulnerability","title":"Reporting a vulnerability","text":"<p>If you discover a potential security issue in this project, we ask that you notify AWS/Amazon Security via our vulnerability reporting page or directly via email to aws-security@amazon.com.</p> <p>Please do not create a public GitHub issue.</p>"},{"location":"assets/ansible-playbooks/perforce/p4-server/index.html","title":"Ansible Playbooks | P4 Server","text":"<p>This page includes documentation for reusable Ansible Playbooks for game development on AWS.</p> <p>Currently the project provides the following playbooks:</p> Playbook Description P4 Server (formerly Helix Core) Commit Playbook Sets up a server as a P4 Server Commit Server P4 Server (formerly Helix Core) Replica Playbook Sets up a server as a P4 Server Replica Server"},{"location":"assets/jenkins-pipelines/index.html","title":"Jenkins example pipelines","text":"<p>This folder contains example Jenkins pipelines for building various pieces of software. To use them, create a new Jenkins pipeline project, and then copy-and-paste the contents of a sample file into the \"Pipeline\" section in the configuration page.</p> <p>You will likely need to change the pipelines slightly to suit your needs, for example to alter the agent node labels on which steps run. Many pipelines also depend on a global Jenkins environment variable to be set: <code>FSX_WORKSPACE_VOLUME_ID</code>, which should be set to the FSx for OpenZFS volume ID of the Workspace volume.</p> <p>The pipelines are primarily written as Declarative Pipelines, with sections of scripted pipeline blocks used to pass variables between stages or to implement try/catch behavior.</p>"},{"location":"assets/jenkins-pipelines/index.html#ue5_build_pipelinegroovy","title":"<code>ue5_build_pipeline.groovy</code>","text":"<p>This pipeline builds Unreal Engine 5 on Linux from its Git repository on GitHub, using an FSx volume as workspace cache and another FSx volume to (optionally) store build cache artifacts to speed up subsequent builds.</p> Note <p>This pipeline requires that you configure GitHub credentials in Jenkins. You also need to get access to the Unreal Engine 5 source code.</p> Note <p>Although this pipeline supports octobuild for caching build artifacts out-of-the-box, octobuild for Linux requires a patch to the Unreal Engine 5 source code. We recommend forking Unreal Engine 5, applying the necessary patch to your fork, and then building from your own fork instead of from the upstream repository. Please refer to the octobuild readme for instructions on which patches to apply.</p> Note <p>You will need to run this on a build node with large /tmp space.</p> <p>The pipeline is divided in two stages:</p> <ol> <li> <p>Prepare - Clones or pulls the Git repository to the FSx workspace volume, then creates an FSx snapshot and waits for it to be available. This stage is skipped if the <code>source_path</code> parameter is provided.</p> </li> <li> <p>Build - Builds Unreal Engine 5 from the snapshot location on x86_64 Linux. Because FSx for OpenZFS snapshots are read-only, on Linux a temporary overlay file system is created.</p> </li> </ol>"},{"location":"assets/jenkins-pipelines/index.html#godotgroovy","title":"<code>godot.groovy</code>","text":"<p>This pipeline builds the Godot engine from its public Git repository, using an FSx volume as workspace cache and another FSx volume to store sccache artifacts to speed up subsequent builds.</p> <p>The pipeline is divided in two stages:</p> <ol> <li> <p>Prepare - Clones or pulls the Git repository to the FSx workspace volume, then creates an FSx snapshot and waits for it to be available. This stage is skipped if the <code>source_path</code> parameter is provided.</p> </li> <li> <p>Build - Builds Godot from the snapshot location on x86_64 Linux and arm64 Linux. Because FSx for OpenZFS snapshots are read-only, and Godot does not build from a read-only filesystem, on Linux a temporary overlay file system is created.</p> </li> </ol>"},{"location":"assets/jenkins-pipelines/index.html#gamelift_sdkgroovy","title":"<code>gamelift_sdk.groovy</code>","text":"<p>This pipeline builds the GameLift Server C++ SDK in 8 different configurations. It uses an FSx volume as workspace cache and (optionally) another FSx volume to store sccache artifacts to speed up subsequent builds.</p> <p>The build configurations are:</p> Operating sytem CPU architecture Build configuration Ubuntu Jammy 22.04 x86_64 Standard build Ubuntu Jammy 22.04 x86_64 Built for Unreal Engine Ubuntu Jammy 22.04 arm64 Standard build Ubuntu Jammy 22.04 arm64 Built for Unreal Engine Amazon Linux 2023 x86_64 Standard build Amazon Linux 2023 x86_64 Built for Unreal Engine Amazon Linux 2023 arm64 Standard build Amazon Linux 2023 arm64 Built for Unreal Engine Note <p>You will most likely not need each of these build configurations to compile the GameLift Server SDK for your game. We recommend that you delete those you don't need from the pipeline manually.</p> <p>The pipeline is divided in two stages:</p> <ol> <li> <p>Prepare - Downloads the GameLift Server SDK .zip to the FSx workspace volume, then creates an FSx snapshot and waits for it to be available. This stage is skipped if the <code>source_path</code> parameter is provided.</p> </li> <li> <p>Build - Builds the SDK from the snapshot location on aforementioned operating systems and configurations.</p> </li> </ol>"},{"location":"assets/jenkins-pipelines/index.html#delete_oldest_snapshotgroovy","title":"<code>delete_oldest_snapshot.groovy</code>","text":"<p>This parameterized pipeline deletes the oldest FSx snapshot that's older than 7 days. Use this to clean up automatically-created snapshots for workspace volumes that you no longer need.</p> <p>This pipeline has the following input parameters:</p> <ol> <li><code>FSX_VOLUME_ID</code> - FSx volume ID of the volume to delete the oldest snapshot from.</li> </ol> <p>Warning</p> <p>This pipeline performs no logic to check whether a snapshot was created automatically, so do not run this pipeline against FSx volumes where you use snapshots for data backup purposes.</p>"},{"location":"assets/jenkins-pipelines/index.html#multiplatform_buildgroovy","title":"<code>multiplatform_build.groovy</code>","text":"<p>This simple multi-stage pipeline demonstrates how to build for multiple platforms by running multiple stages, and how to paralellize steps in a single stage across different build nodes. It can be used to verify that build nodes for various platforms work correctly, and is a great starting point for creating new pipelines.</p>"},{"location":"assets/packer/build-agents/linux/index.html","title":"Packer templates for Unreal Engine Linux build agents","text":"<p>The following templates provide Unreal Engine Linux build agents:</p> Operating system CPU architecture file location Ubuntu Jammy 22.04 x86_64 (a.k.a. amd64) <code>x86_64/ubuntu-jammy-22.04-amd64-server.pkr.hcl</code> Ubuntu Jammy 22.04 aarch64 (a.k.a. arm64) <code>aarch64/ubuntu-jammy-22.04-arm64-server.pkr.hcl</code> Amazon Linux 2023 x86_64 (a.k.a. amd64) <code>x86_64/amazon-linux-2023-x86_64.pkr.hcl</code> Amazon Linux 2023 aarch64 (a.k.a. arm64) <code>aarch64/amazon-linux-2023-arm64.pkr.hcl</code>"},{"location":"assets/packer/build-agents/linux/index.html#usage","title":"Usage","text":"<ol> <li>Make a copy of <code>example.pkrvars.hcl</code> and adjust the input variables as needed</li> <li>Ensure you have active AWS credentials</li> <li>Invoke <code>packer build --var-file=&lt;your .pkrvars.hcl file&gt; &lt;path to .pkr.hcl file&gt;</code>, then wait for the build to complete.</li> </ol>"},{"location":"assets/packer/build-agents/linux/index.html#software-packages-included","title":"Software packages included","text":"<p>The templates install various software packages:</p> <p>common tools</p> <p>Some common tools are installed to enable installing other software, performing maintenance tasks, and compile some C++ software:</p> <ul> <li>git</li> <li>curl</li> <li>jq</li> <li>unzip</li> <li>dos2unix</li> <li>AWS CLI v2</li> <li>AWS Systems Manager Agent</li> <li>Amazon Corretto</li> <li>mount.nfs, to be able to mount FSx volumes over NFS</li> <li>python3</li> <li>python3 packages: 'pip', 'requests', 'boto3' and 'botocore'</li> <li>clang</li> <li>cmake3</li> <li>scons</li> <li>Development libraries for compiling the Amazon GameLift Server SDK for C++</li> <li>Development libraries for compiling the Godot 4 game engine (if available in the OS's package manager)</li> </ul> <p>mold</p> <p>The 'mold' linker is installed to enable faster linking.</p> <p>FSx automounter service</p> <p>The FSx automounter systemd service is a service written in Python that automatically mounts FSx for OpenZFS volumes on instance bootup. The service uses resource tags on FSx volumes to determine if and where to mount volumes on.</p> <p>You can use the following tags on FSx volumes: * 'automount-fsx-volume-name' tag: specifies the name of the local mount point. The mount point specified will be prefixed with 'fsx_' by the service. * 'automount-fsx-volume-on' tag: This tag contains a space-delimited list of EC2 instance names on which the volume will be automatically mounted by this service (if it is running on that instance).</p> <p>For example, if the FSx automounter service is running on an EC2 instance with Name tag 'ubuntu-builder', and an FSx volume has tag <code>automount-fsx-volume-on</code>=<code>al2023-builder ubuntu-builder</code> and tag <code>automount-fsx-volume-name</code>=<code>workspace</code>, then the automounter will automatically mount that volume on <code>/mnt/fsx_workspace</code>.</p> <p>Note that the automounter service makes use of the ListTagsForResource FSx API call, which is rate-limited. If you intend to scale up hundreds of EC2 instances that are running this service, then we recommend automatically mounting FSx volumes using <code>/etc/fstab</code>.</p> <p>mount_ephemeral service</p> <p>The mount_ephemeral service is a systemd service written as a simple bash script that mounts NVMe attached instance storage volume automatically as temporary storage. It does this by formatting <code>/dev/nvme1n1</code> as xfs and then mounting it on <code>/tmp</code>. This service runs on instance bootup.</p> <p>create_swap service</p> <p>The create_swap service is a systemd service written as a simple bash script that creates a 1GB swap file on <code>/swapfile</code>. This service runs on instance bootup.</p> <p>sccache</p> <p>'sccache' is installed to cache c/c++ compilation artefacts, which can speed up builds by avoiding unneeded work.</p> <p>sccache is installed as a systemd service, and configured to use <code>/mnt/fsx_cache/sccache</code> as its cache folder. The service expects this folder to be available or set up by another service.</p> <p>octobuild</p> <p>'Octobuild' is installed to act as a compilation cache for Unreal Engine.</p> <p>Octobuild is configured in octobuild.conf to use <code>/mnt/fsx_cache/octobuild_cache</code> as its cache folder, and expects this folder to be available or set up by another service.</p> <p>NOTE: Octobuild is not supported on aarch64, and therefore not installed there.</p>"},{"location":"assets/packer/build-agents/linux/index.html#processor-architectures-and-naming-conventions","title":"Processor architectures and naming conventions","text":"<p>Within this folder, the processor architecture naming conventions as reported by <code>uname -m</code> are used, hence why there are scripts here with names containing \"x86_64\" or \"aarch64\". The packer template <code>.hcl</code> files are named following the naming conventions of the operating system that they are based on. Unfortunately, because some operating systems don't use the same terminology in their naming conventions throughout, this means that you'll see this lack of consistency here has well.</p>"},{"location":"assets/packer/build-agents/windows/index.html","title":"Packer Templates for Unreal Engine Windows Build Agents","text":"<p>The following template builds a Windows based AMI capable of Unreal Engine 5.4 compilation jobs. Please customize it to your needs.</p>"},{"location":"assets/packer/build-agents/windows/index.html#usage","title":"Usage","text":"<p>This Amazon Machine Image is provisioned using the Windows Server 2022 base operating system. It installs all required tooling for Unreal Engine 5 compilation by default. Please consult the release notes for Unreal Engine 5.4 for details on what tools are used for compiling this version of the engine.</p> <p>The only required variable for building this Amazon Machine Image is a public SSH key.</p> <pre><code>packer build windows.pkr.hcl \\\n    -var \"public_key=&lt;include public ssh key here&gt;\"\n</code></pre> Note <p>The above command assumes you are running <code>packer</code> from the <code>/assets/packer/build-agents/windows</code> directory.</p> <p>You will then want to upload the private SSH key to AWS Secrets Manager so that the Jenkins orchestration service can use it to connect to this build agent.</p> <pre><code>aws secretsmanager create-secret \\\n    --name JenkinsPrivateSSHKey \\\n    --description \"Private SSH key for Jenkins build agent access.\" \\\n    --secret-string \"&lt;insert private SSH key here&gt;\" \\\n    --tags 'Key=jenkins:credentials:type,Value=sshUserPrivateKey' 'Key=jenkins:credentials:username,Value=jenkins'\n</code></pre> <p>Take note of the output of this CLI command. You will need the ARN later.</p> <p>Currently this AMI is designed to work with our Jenkins module. This is why it creates a <code>jenkins</code> user and the associated SSH username for the key you upload is that same <code>jenkins</code> user. Expanded customization of this AMI is currently on the Cloud Game Development Toolkit roadmap.</p>"},{"location":"assets/packer/build-agents/windows/index.html#installed-tooling","title":"Installed Tooling","text":"<ul> <li>Chocolatey package manager</li> <li>OpenJDK used by Jenkins agents</li> <li>Git</li> <li>OpenSSH</li> <li>Python3<ul> <li>Botocore</li> <li>Boto3</li> </ul> </li> <li>Client for Network File System (NFS)</li> <li>Windows Development Kit and Debugging Tools</li> <li>Visual Studio 2022 Build Tools<ul> <li>VCTools Workload; Include Recommended</li> <li>ManagedDesktopBuild Tools; Include Recommended</li> <li>MSVC v143 - VS 2022 C++ x64/x86 build tools</li> <li>Microsoft.Net.Component.4.6.2TargetingPack</li> </ul> </li> </ul> <p>Consult the Visual Studio Build Tools component directory for details.</p>"},{"location":"assets/packer/perforce/p4-server/index.html","title":"P4 Server Packer Template","text":"<p>This Packer template creates an Amazon Machine Image for installing and configuring P4 Server on Linux. It supports both <code>x86</code> and <code>ARM</code> architectures.</p> <p>The <code>p4_configure.sh</code> script contains the majority of P4 Server setup. It performs the following operations:</p> <ol> <li>Pre-Flight Checks: Ensures the script is run with root privileges.</li> <li>Environment Setup: Defines paths and necessary constants for the installation.</li> <li>SELinux Handling: Checks if SELinux is enabled and installs required packages.</li> <li>User and Group Verification: Ensures the 'perforce' user and group exist.</li> <li>Directory Creation and Ownership: Ensures necessary directories exist and have correct ownership.</li> <li>P4 Binaries and SDP Installation: Downloads and extracts SDP, checks for P4 binaries, and downloads them if missing.</li> <li>Systemd Service Configuration: Sets up a systemd service for the p4d server.</li> <li>SSL Configuration: Updates SSL certificate configuration with the EC2 instance DNS name.</li> <li>SELinux Context Management: Updates SELinux context for p4d.</li> <li>Crontab Initialization: Sets up crontab for the 'perforce' user.</li> <li>SDP Verification: Runs a script to verify the SDP installation.</li> <li>P4Auth Extension: Installs the P4Auth Extension and validates successful communication with P4Auth.</li> </ol>"},{"location":"assets/packer/perforce/p4-server/index.html#how-to-use","title":"How to Use","text":"<p>Building this AMI is as easy as running (x86 example):</p> <p><pre><code>packer init ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\n</code></pre> <pre><code>packer validate ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\n</code></pre> <pre><code>packer build ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\n</code></pre></p> <p>Packer will attempt to leverage the default VPC available in the AWS account and Region specified by your CLI credentials. It will provision an instance in a public subnet and communicate with that instance over the public internet. If a default VPC is not available or otherwise provided, the above command will fail. This Packer template can take a number of variables as specified in <code>example.pkrvars.hcl</code>. Variables can be passed individually through the <code>-var</code> command line flag or through a configuration file with the <code>-var-file</code> command line flag.</p> <p>An instance that is provisioned with this AMI will not automatically deploy a P4 Server. Instead, the required installation and configuration scripts are loaded onto this AMI by Packer, and then invoked at boot through EC2 user data. The P4 Server module does this through Terraform, but you can also manually provision an instance off of this AMI and specify the user data yourself:</p> <pre><code>#!/bin/bash\n/home/ec2-user/cloud-game-development-toolkit/p4_configure.sh \\\n   &lt;volume label for hxdepot&gt; \\\n   &lt;volume label for hxmetadata&gt; \\\n   &lt;volume label for hxlogs&gt; \\\n   &lt;p4d server type&gt; \\\n   &lt;super user username ARN from AWS Secrets Manager&gt; \\\n   &lt;super user password ARN from AWS Secrets Manager&gt; \\\n   &lt;fully qualified domain name of P4 Server&gt; \\\n   &lt;URL for P4Auth&gt;\n</code></pre> <p>As you can see, there are quite a few parameters that need to be passed to the <code>p4_configure.sh</code> script. We recommend using the Perforce module for this reason.</p>"},{"location":"assets/packer/perforce/p4-server/index.html#important-notes","title":"Important Notes","text":"<ul> <li>This script is designed for a specific use-case and might require modifications for different environments or requirements.</li> <li>Ensure you have a backup of your system before running the script, as it makes significant changes to users, groups, and services.</li> <li>The script assumes an internet connection for downloading packages and binaries.</li> </ul>"},{"location":"docs/index.html","title":"Welcome to the Cloud Game Development Toolkit","text":"<p>Info</p> <p>This project is under active development and community contributions are welcomed!. If you would like to see something in this repository please create a feature request in the Issues tab. If you'd like to contribute, raise a pull request. You'll find our contribution guidelines here.</p> <p>The Cloud Game Development Toolkit (a.k.a. CGD Toolkit) is a collection of templates and configurations for deploying game development infrastructure and tools on AWS.</p> <p>Below are key tenets driving project's focus:</p> <ul> <li>This is a fork-first, open-source project. We know that every game project is unique, so fork the repo, create your own branches for customization, and sync as appropriate. If you build something that can benefit other game developers, feel free to share via PR, as we encourage contributions!</li> <li>Meet game developers where they are. We aim to minimize learning curves and introducing new technologies where possible by building solutions that incorporate tools and software that are already widely used across the game industry and among our existing AWS for Games customers.</li> <li>Solutions are built for AWS. This project is focused on improving the game development experience on AWS and does not try to standardize solutions for deployment across many hosting platforms. In our experience, doing so is generally difficult, unecessary, and fraught with tradeoffs. If AWS is not your jam, you're welcome to fork and customize as needed (see above)!</li> </ul>"},{"location":"docs/index.html#getting-started","title":"Getting Started","text":"<p>Getting Started</p>"},{"location":"docs/index.html#license","title":"License","text":"<p>This project is licensed under the MIT-0 License.</p>"},{"location":"docs/changelog.html","title":"Changelog","text":""},{"location":"docs/changelog.html#unreleased","title":"Unreleased","text":""},{"location":"docs/changelog.html#latest-2025-06-09","title":"latest - 2025-06-09","text":""},{"location":"docs/changelog.html#v114-2025-04-28","title":"v1.1.4 - 2025-04-28","text":""},{"location":"docs/changelog.html#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Remove commented out NetApp volume resources and cleanup IAM managed policies</li> <li>Resolved EC2 DNS self-signed certificate bug in P4 Server packer template</li> <li>Adding cloud DDC sample for mkdocs.yml</li> <li>helix swarm: helix swarm does not support horizontal scaling, so helix swarm container count is now set to 1</li> </ul>"},{"location":"docs/changelog.html#chore","title":"Chore","text":"<ul> <li>Add Terraform tests for new Perforce module (#604)</li> <li>regenerate CHANGELOG.md for 2025-03-19</li> <li>Minor maintenance to Helix Core module</li> <li>Minor Helix Authentication fixes</li> <li>Addressed IAM policy warnings for Helix Swarm</li> <li>deps: bump xt0rted/pull-request-comment-branch from 1 to 3</li> <li>deps: bump mkdocs-material from 9.6.11 to 9.6.12 in /docs</li> <li>deps: bump mkdocs-material from 9.6.12 to 9.6.14 in /docs</li> <li>deps: bump actions/github-script from 6 to 7</li> <li>deps: bump actions/checkout from 3 to 4</li> <li>deps: bump mkdocs-material from 9.6.9 to 9.6.11 in /docs</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump aquasecurity/trivy-action from 0.29.0 to 0.30.0</li> <li>deps: bump mkdocs-material from 9.6.8 to 9.6.9 in /docs</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.8 to 9.6.9 in /docs</li> <li>deps: bump actions/upload-artifact from 4.6.1 to 4.6.2</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> </ul>"},{"location":"docs/changelog.html#code-refactoring","title":"Code Refactoring","text":"<ul> <li>Update Simple Build Pipeline sample to use new Perforce parent module (#608)</li> <li>Perforce modules consolidated to simplify shared resource creation (#585)</li> <li>Updated Perforce complete example to remove NLB front for Helix Core</li> <li>reorganize unreal cloud ddc module structure</li> </ul>"},{"location":"docs/changelog.html#docs","title":"Docs","text":"<ul> <li>Adjustments to mkdocs structure, and updates to \"getting started\" and Perforce documentation. (#612)</li> <li>updates and expands on <code>unreal-cloud-ddc-intra-cluster</code> installation and usage docs</li> <li>fixes relative path for <code>unreal-cloud-ddc-infra</code> and <code>unreal-cloud-ddc-intra-cluster</code> Terraform module docs</li> <li>add unreal fest video to horde module</li> <li>TeamCity: Adding TeamCity module docs and example architecture</li> </ul>"},{"location":"docs/changelog.html#features","title":"Features","text":"<ul> <li>Adds debug variable and flag</li> <li>Simple example deployment of Helix Core backed by FSxN</li> <li>FSxN ISCSI provisioning for Helix Core module</li> <li>Modified p4_configure.sh to mount ISCSI volumes from FSxN</li> </ul>"},{"location":"docs/changelog.html#v113-alpha-2025-03-19","title":"v1.1.3-alpha - 2025-03-19","text":""},{"location":"docs/changelog.html#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>create_external_alb shouldn't block internal SG Ingress rules</li> <li>alb_subnet variables should not be required if create boolean is false</li> <li>Attaching perforce web service ALB to target group</li> <li>use provided admin password secret for Helix Authentication Service ADMIN_PASSWD, instead of the username secret</li> <li>AMI version bump for Helix Core, region variable made optional</li> </ul>"},{"location":"docs/changelog.html#chore_1","title":"Chore","text":"<ul> <li>update dependabot configuration to include unreal modules</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.4 to 9.6.7 in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.4 to 9.6.7 in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump actions/upload-artifact from 4.6.0 to 4.6.1</li> <li>deps: bump ossf/scorecard-action from 2.4.0 to 2.4.1</li> <li>deps: bump hashicorp/random</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.7 to 9.6.8 in /docs</li> <li>deps: bump the random-provider group across 4 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.6.3 to 9.6.4 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.2 to 9.6.4 in /docs</li> <li>deps: bump actions/upload-artifact from 4.5.0 to 4.6.0</li> <li>deps: bump mkdocs-material from 9.6.7 to 9.6.8 in /docs</li> <li>deps: bump hashicorp/aws</li> <li>deps: bump mkdocs-material from 9.6.2 to 9.6.3 in /docs</li> <li>deps: bump squidfunk/mkdocs-material from 9.6.1 to 9.6.2 in /docs</li> <li>deps: bump mkdocs-material from 9.5.50 to 9.6.2 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump the awscc-provider group across 2 directories with 1 update</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump squidfunk/mkdocs-material in /docs</li> <li>deps: bump release-drafter/release-drafter from 6.0.0 to 6.1.0</li> <li>deps: bump mkdocs-material from 9.5.49 to 9.5.50 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> </ul>"},{"location":"docs/changelog.html#docs_1","title":"Docs","text":"<ul> <li>fix broken link in readme</li> <li>Perforce: Updating documentation for Perforce Complete example reference architecture</li> </ul>"},{"location":"docs/changelog.html#features_1","title":"Features","text":"<ul> <li>Helix Authentication Service: Shifting ALB creation to support external networking configuration</li> <li>Helix Core: Plaintext support for Helix Core, optional EIP creation</li> <li>Helix Core: Adding plaintext variable to p4_configre.sh</li> <li>Helix Swarm: Shifting ALB creation to support external networking configuration</li> <li>Perforce Example: Update complete example for shared networking configuration across services</li> <li>TeamCity Example: example terraform configuration for deploying TeamCity module</li> <li>TeamCity Server: terraform module for deploying TeamCity server on ECS Fargate</li> </ul>"},{"location":"docs/changelog.html#v112-alpha-2024-12-20","title":"v1.1.2-alpha - 2024-12-20","text":""},{"location":"docs/changelog.html#chore_2","title":"Chore","text":"<ul> <li>regenerate CHANGELOG.md for 2024-12-20</li> <li>ignore tf backend.tf files in .gitignore</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump actions/upload-artifact from 4.4.3 to 4.5.0</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> </ul>"},{"location":"docs/changelog.html#docs_2","title":"Docs","text":"<ul> <li>removed READMEs from source directories and moved them to their own dedicated docs pages in docs/ dir</li> <li>update contributor documentation to include table of contents</li> <li>updates to doc formatting and fixed broken links</li> </ul>"},{"location":"docs/changelog.html#v111-alpha-2024-12-17","title":"v1.1.1-alpha - 2024-12-17","text":""},{"location":"docs/changelog.html#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Added service target group ARNs as outputs for HAS and Swarm</li> <li>Adds defaults to <code>vpc_id</code> and <code>subnet_id</code> variables</li> <li>bash error causing build failure when running p4_configure.sh (#367)</li> <li>horde: add JwtIssuer to ensure container retains agents on restart</li> <li>horde: allow inbound access to horde agents on ports 7000-7010 from other horde agents</li> <li>perforce: fixed minor issues in p4_configure.sh</li> <li>perforce: add Unicode support and fix main module to handle existing security groups</li> </ul>"},{"location":"docs/changelog.html#chore_3","title":"Chore","text":"<ul> <li>make SELinux label updates configurable</li> <li>remove packer assets .ci directory (#337)</li> <li>fix tag names so that they match recommended best practices (#343)</li> <li>define nat gateway routes for private route tables outside of aws_route_table resources in samples and modules (#354)</li> <li>adds triage label to our issue templates</li> <li>regenerate CHANGELOG.md for 2024-12-17</li> <li>document parameter values for '--unicode' flag</li> <li>provide appropriate association name for configuring Helix Core via SSM</li> <li>fix naming</li> <li>checkov: Suppresses CKV_AWS_378 rule (#339)</li> <li>deps: bump mkdocs-material from 9.5.42 to 9.5.44 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump aquasecurity/trivy-action from 0.28.0 to 0.29.0</li> <li>deps: bump mkdocs-material from 9.5.45 to 9.5.46 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.5.44 to 9.5.45 in /docs</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.7 to 1.0.8 in /docs</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump actions/checkout from 3.0.0 to 4.2.2</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump hashicorp/setup-terraform from 1 to 3</li> <li>deps: bump aws-actions/configure-aws-credentials</li> <li>deps: bump mkdocs-material from 9.5.41 to 9.5.42 in /docs</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.6 to 1.0.7 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump aquasecurity/trivy-action from 0.24.0 to 0.28.0</li> <li>deps: bump mkdocs-material from 9.5.40 to 9.5.41 in /docs</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update</li> <li>deps: bump python from 3.12.7 to 3.13.0 in /docs (#349)</li> <li>deps: bump actions/upload-artifact from 4.4.0 to 4.4.3 (#356)</li> <li>deps: bump mkdocs-material from 9.5.39 to 9.5.40 in /docs (#359)</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.5 to 1.0.6 in /docs (#345)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update</li> <li>deps: bump mkdocs-material from 9.5.37 to 9.5.39 in /docs (#335)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#344)</li> <li>deps: bump mkdocs-material from 9.5.46 to 9.5.48 in /docs</li> <li>deps: bump python from 3.12.6 to 3.12.7 in /docs (#340)</li> <li>deps: bump mkdocs-material from 9.5.48 to 9.5.49 in /docs</li> <li>deps: bump python from 3.13.0 to 3.13.1 in /docs</li> </ul>"},{"location":"docs/changelog.html#docs_3","title":"Docs","text":"<ul> <li>clarify that modules are intended to be depended on, and samples are reference implementations meant to be copied and modified</li> <li>fix formatting of simple build pipeline docs</li> <li>fix formatting of local.tf in simple build pipeline docs</li> <li>fix formatting of jenkins pipeline assets page</li> <li>clarify use case of Ansible playbooks vs Packer templates</li> <li>clarify that deploying multiple samples independently is not supported</li> <li>point users explicitly to a Classic GitHub Personal Access Token</li> <li>fix typo in getting started guide</li> <li>Updates the getting started instructions for the simple build pipeline sample</li> </ul>"},{"location":"docs/changelog.html#features_2","title":"Features","text":"<ul> <li>perforce: implement Helix Core setup playbook</li> </ul>"},{"location":"docs/changelog.html#v110-alpha-2024-10-01","title":"v1.1.0-alpha - 2024-10-01","text":""},{"location":"docs/changelog.html#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>improve stability of build agent packer scripts, adjust winrm timeout to 15 minutes, remove packer variables that aren't needed (#318)</li> </ul>"},{"location":"docs/changelog.html#chore_4","title":"Chore","text":"<ul> <li>update changelog (#305)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#323)</li> <li>deps: bump mkdocs-material from 9.5.35 to 9.5.37 in /docs (#314)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#324)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#298)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#291)</li> <li>deps: bump the random-provider group across 5 directories with 1 update (#310)</li> <li>deps: bump mkdocs-material from 9.5.34 to 9.5.35 in /docs (#287)</li> </ul>"},{"location":"docs/changelog.html#docs_4","title":"Docs","text":"<ul> <li>add perforce complete example in docs (#333)</li> <li>updates to documentation (#329)</li> </ul>"},{"location":"docs/changelog.html#features_3","title":"Features","text":"<ul> <li>install requirements for (auto)mounting FSx volumes on Jenkins Windows build agents (#319)</li> <li>helix-core: add ARM64 support (#239)</li> </ul>"},{"location":"docs/changelog.html#v101-alpha-2024-09-16","title":"v1.0.1-alpha - 2024-09-16","text":""},{"location":"docs/changelog.html#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>changelog automation (#261)</li> <li>adding branch creation to workflow (#259)</li> <li>dependabot grouping terraform providers (#228)</li> <li>wait for cloud-init to complete prior to installing packages during Perforce Helix Core AMI creation (#193)</li> <li>changelog: GHA bot committer (#255)</li> <li>changelog: Add automated PR creation (#252)</li> <li>fsx_automounter: when FSx automounter can't list tags for an FSx volume, the AccessDenied exception is now treated as a warning (#226)</li> <li>p4_configure: resolve script execution errors and repair broken \u2026 (#232)</li> </ul>"},{"location":"docs/changelog.html#chore_5","title":"Chore","text":"<ul> <li>adjusting changelog automation to leverage GH api (#266)</li> <li>update changelog workflow (#284)</li> <li>update changelog (#285)</li> <li>deps: bump hashicorp/awscc from 1.10.0 to 1.11.0 in /samples/simple-build-pipeline (#220)</li> <li>deps: bump hashicorp/awscc from 1.9.0 to 1.10.0 in /modules/perforce/helix-core (#207)</li> <li>deps: bump mkdocs-material from 9.5.33 to 9.5.34 in /docs (#236)</li> <li>deps: bump actions/upload-artifact from 4.3.6 to 4.4.0 (#235)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#241)</li> <li>deps: bump the awscc-provider group across 3 directories with 1 update (#242)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#233)</li> <li>deps: bump the aws-provider group across 5 directories with 1 update (#231)</li> <li>deps: bump mkdocs-material from 9.5.32 to 9.5.33 in /docs (#229)</li> <li>deps: bump mkdocs-open-in-new-tab from 1.0.3 to 1.0.5 in /docs (#263)</li> <li>deps: bump mkdocs-material from 9.5.31 to 9.5.32 in /docs (#211)</li> <li>deps: bump python from 3.12 to 3.12.6 in /docs (#243)</li> <li>deps: bump hashicorp/awscc from 1.9.0 to 1.10.0 in /modules/perforce/helix-authentication-service (#205)</li> <li>deps: bump hashicorp/aws from 5.62.0 to 5.63.1 in /samples/simple-build-pipeline (#216)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /modules/perforce/helix-authentication-service (#196)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-authentication-service (#197)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /modules/perforce/helix-core (#198)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-core (#199)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/perforce/helix-swarm (#200)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /samples/simple-build-pipeline (#201)</li> <li>deps: bump hashicorp/awscc from 1.6.0 to 1.9.0 in /samples/simple-build-pipeline (#202)</li> <li>deps: bump mike from 2.1.2 to 2.1.3 in /docs (#189)</li> <li>deps: bump hashicorp/aws from 5.59.0 to 5.62.0 in /modules/jenkins (#195)</li> </ul>"},{"location":"docs/changelog.html#docs_5","title":"Docs","text":"<ul> <li>add openssf scorecard badge to readme (#219)</li> <li>link to installation instructions for required tools, fix packer command invocation instructions (#194)</li> <li>Windows Build AMI README (#187)</li> </ul>"},{"location":"docs/changelog.html#v100-alpha-2024-08-07","title":"v1.0.0-alpha - 2024-08-07","text":""},{"location":"docs/changelog.html#staging-2024-08-07","title":"staging - 2024-08-07","text":""},{"location":"docs/changelog.html#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>fix issue where SSH public key was not baked into the Windows Jenkins build agent AMI (#150)</li> <li>bug fixes for FSxZ storage in build farm (#152)</li> <li>allow Jenkins build agents to discover FSx volumes/snapshots and make outbound Internet connections (#147)</li> </ul>"},{"location":"docs/changelog.html#chore_6","title":"Chore","text":"<ul> <li>add CODEOWNERS file (#132)</li> <li>Updates to docs (#63)</li> <li>fix makefile (#65)</li> <li>Modify version handling in Docs (#66)</li> <li>deps: bump mkdocs-material from 9.5.27 to 9.5.28 in /docs (#135)</li> <li>deps: bump mkdocs-material from 9.5.26 to 9.5.27 in /docs (#77)</li> <li>deps: bump aquasecurity/trivy-action from 0.23.0 to 0.24.0 (#137)</li> <li>deps: bump actions/upload-artifact from 4.3.3 to 4.3.4 (#136)</li> <li>deps: bump actions/upload-artifact from 4.3.5 to 4.3.6 (#178)</li> <li>deps: bump mkdocs-material from 9.5.29 to 9.5.30 in /docs (#153)</li> <li>deps: bump mike from 2.1.1 to 2.1.2 in /docs (#110)</li> <li>deps: bump mkdocs-material from 9.5.28 to 9.5.29 in /docs (#144)</li> <li>deps: bump github/codeql-action from 3.25.8 to 3.25.10 (#69)</li> <li>deps: bump ossf/scorecard-action from 2.3.3 to 2.4.0 (#167)</li> <li>deps: bump actions/upload-artifact from 4.3.4 to 4.3.5 (#171)</li> <li>deps: bump mkdocs-material from 9.5.30 to 9.5.31 in /docs (#172)</li> <li>deps: bump github/codeql-action from 3.24.9 to 3.25.8 (#53)</li> <li>deps: bump mkdocs-material from 9.5.25 to 9.5.26 in /docs (#54)</li> </ul>"},{"location":"docs/changelog.html#code-refactoring_1","title":"Code Refactoring","text":"<ul> <li>Perforce Helix Core AMI revamp, simple build pipeline DNS (#73)</li> </ul>"},{"location":"docs/changelog.html#docs_6","title":"Docs","text":"<ul> <li>update changelog (#181)</li> <li>update main docs page (#179)</li> <li>update layout of documentation main page theme (#175)</li> <li>update documentation (#163)</li> <li>update workflow for docs (#129)</li> <li>update workflow (#128)</li> <li>fix workflow to use gh inputs from workflow (#127)</li> <li>update to docs and flip release workflow to manual (#126)</li> <li>fix commit depth (#125)</li> <li>modify the workflow for docs release and update documentation (#124)</li> <li>fix docs ci (#123)</li> <li>modify git fetch-depth for docs ci (#121)</li> <li>update README.md (#119)</li> <li>consolidate Ansible playbooks under assets (#117)</li> <li>fix url to documentation to point to /latest (#80)</li> <li>add GH Pull Request template (#67)</li> <li>updates workflow and adds changelog automation (#61)</li> <li>add issue template for RFCs (#57)</li> <li>add git-chglog for changelog generation (#49)</li> <li>enable workflow dispatch (#36)</li> <li>fix docs release workflow (#34)</li> <li>convert docs releases to use mike (#33)</li> <li>adds markdown docs for assets, modules, playbooks, and samples (#32)</li> <li>adds issue template for submitting maintenance issues (#31)</li> <li>Adds documentation and GH workflow for build/publish of docs (#21)</li> <li>Updates to project README (#20)</li> <li>Adds project docs (#13)</li> </ul>"},{"location":"docs/changelog.html#features_4","title":"Features","text":"<ul> <li>Added getting-started documentation for quickstart with Simple Build Pipeline (#177)</li> <li>Updates to CI configurations for pre-commit and GHA (#154)</li> <li>Helix Authentication Extension (#82)</li> <li>enable web based administration through variables for HAS (#79)</li> <li>complete sample with both Jenkins and Perforce modules (#60)</li> <li>Add packer build agent templates for Linux (Ubuntu Jammy 22.04, Amazon Linux 2023) (#46)</li> <li>devops: Add new DevOps playbook files (#76)</li> <li>packer: switch AMI from Rocky Linux to Amazon Linux 2023 and up\u2026 (#141)</li> </ul>"},{"location":"docs/contributing.html","title":"Table of contents","text":"<ul> <li>Contributing Guidelines</li> <li>Reporting Bugs/Feature Requests</li> <li>Contributing via Pull Requests</li> <li>Conventional Commits</li> <li>Finding contributions to work on</li> <li>Code of Conduct</li> <li>Security issue notifications</li> <li>Licensing</li> </ul>"},{"location":"docs/contributing.html#contributing-guidelines","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"docs/contributing.html#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"docs/contributing.html#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages. See the conventional commits section for more details.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p>"},{"location":"docs/contributing.html#conventional-commits","title":"Conventional Commits","text":"<p>This project uses Conventional Commits following the release of v1.0.0-alpha. These conventions ensure that the commit history of the project remains readable, and supports extensive automation around pull request creation, release cadence, and documentation.</p> <p>We do not enforce conventional commits on contributors. We do require that pull request titles follow convention so that the changelog and release automation work as expected.</p>"},{"location":"docs/contributing.html#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"docs/contributing.html#building-and-testing-project-documetation","title":"Building and Testing Project Documetation","text":"<p>This project uses Material for MkDocs to generate versioned documentation automatically. Content for the documentation is auto-generated by referencing the markdown contained in the project's <code>README.md</code> files.</p> <p>Local development:</p> <ul> <li>To build the project documentation, execute the following command: <code>make docs-build-local VERSION=&lt;VERSION&gt; ALIAS=&lt;ALIAS&gt;</code>, including a semantic release version (i.e. <code>v1.0.0</code>) and an alias (i.e. <code>latest</code>). This will build the project using <code>./docs/Dockerfile</code>.</li> <li>To run the documentation locally, execute the following command: <code>make docs-run VERSION=&lt;VERSION&gt; ALIAS=&lt;ALIAS&gt;</code> which runs the documentation in a local container using <code>mkdocs serve</code> on port <code>8000</code>.</li> </ul> <p>Live documentation (GitHub Pages):</p> <ul> <li>A Github workflow is used to build and deploy the documentation to the gh-pages branch: <code>.github/workflows/docs.yml</code></li> </ul>"},{"location":"docs/contributing.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"docs/contributing.html#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"docs/contributing.html#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"docs/getting-started.html","title":"Getting Started","text":"<p>Welcome to the Cloud Game Development Toolkit . There are a number of ways to use this repository depending on your development needs. This guide will introduce some of the key features of the project, and provide detailed instructions for deploying your game studio on AWS.</p>"},{"location":"docs/getting-started.html#introduction-to-repository-structure","title":"Introduction to Repository Structure","text":""},{"location":"docs/getting-started.html#assets","title":"Assets","text":"<p>An asset is a singular template, script, or automation document that may prove useful in isolation. Currently, the Toolkit contains three types of assets: Ansible playbooks, Jenkins pipelines, and Packer templates. Each of these assets can be used in isolation.</p> <p>For more information about assets, consult the detailed documentation.</p>"},{"location":"docs/getting-started.html#modules","title":"Modules","text":"<p>A module is a reusable Terraform configuration encapsulating all of the resources needed to deploy a particular workload on AWS. These modules are highly configurable through variables, and provide necessary outputs for building interconnected architectures. We recommend reviewing the Terraform module documentation if you are unfamiliar with this concept. Modules are designed for you to depend on in your own Terraform modules, and we don't expect you to have to make any modifications to them; that said, if a module doesn't meet your needs, please raise an issue!</p> <p>For more information about modules, consult the detailed documentation.</p>"},{"location":"docs/getting-started.html#samples","title":"Samples","text":"<p>A sample is a complete reference architecture that stitches together modules and first-party AWS services. A sample is deployed with Terraform, and is the best way to get started with the Cloud Game Development Toolkit . Samples are designed for you to copy from and modify as needed to suit your architecture and needs.</p> <p>Note: Because samples may deploy resources that have unique name constraints, we cannot guarantee that two different samples can be deployed into the same AWS account without modifying either of the samples to integrate shared infrastructure or resolve conflicts. If you're interested in using functionality from multiple samples, we recommend that you use them as reference material to base your own infrastructure off of.</p> <p>For more information about samples, consult the detailed documentation.</p> <p>If you're new to the project, we recommend starting by deploying one of the samples, such as the Simple Build Pipeline.</p>"},{"location":"docs/security.html","title":"Security","text":""},{"location":"docs/security.html#overview","title":"Overview","text":"<p>This project is maintained by members of the AWS for Games technical community within AWS (i.e. Solutions Architects, Technical Account Managers, Software Engineers) who support the gaming industry. Design decisions and tradeoffs made throughout this project reflect our experiences working with game studios to build and maintain their development infrastructure and tools in the cloud. We encourage contributions from the community via Pull Requests, which are manually reviewed and tested by the core maintainers of the project before they are merged.</p> <p>We rely on Open Source Security (OpenSSF) Scorecard assessments to validate our project's security posture against a common set of open source project risks. We also self-certify our security practices against the standards defined by OpenSSF Best Practices Badge Program. The badges above are hyperlinks that you can follow to review the results of each.</p>"},{"location":"docs/security.html#reporting-a-vulnerability","title":"Reporting a vulnerability","text":"<p>If you discover a potential security issue in this project, we ask that you notify AWS/Amazon Security via our vulnerability reporting page or directly via email to aws-security@amazon.com.</p> <p>Please do not create a public GitHub issue.</p>"},{"location":"docs/assets/index.html","title":"Assets","text":"<p>Assets are reusable scripts, pipeline definitions, Dockerfiles, Packer templates, and other resources that might prove useful or are dependencies of any of the modules.</p> <p>Info</p> <p>Don't see an asset listed? Create a feature request for a new asset or learn how to contribute new assets to the project</p> Asset Type Description Packer Templates Packer templates provide an easy way to build machine images for commonly used game dev infrastructure. Currently the project includes Packer templates for UE5 build agents for Linux and Windows, as well as a Packer template for building a Perforce Helix Core version control AMI. Jenkins Pipelines Jenkins Pipelines for common game dev automation workflows Ansible Playbooks Automation scripts for reusable system level configurations. Unlike Packer templates, you can use these to add new functionality to existing EC2 instances. Dockerfiles (Coming Soon!) Dockerfiles for creating Docker images of commonly used game dev infrastructure. These are primarily used in scenarios where there aren't openly available pre-built images that address a use case, or significant customization is needed that warrants building an image"},{"location":"docs/assets/dockerfiles.html","title":"Dockerfiles","text":"<p>Coming soon.</p>"},{"location":"docs/assets/packer/index.html","title":"Packer Templates","text":"<p>Packer is a tool for simplifying and automating Amazon Machine Image (AMI) creation with code. It enables developers to create identical images for multiple platforms. The Packer templates provided in the Cloud Game Development Toolkit can be used to provision EC2 instances with common development tools preinstalled.</p> <p>Info</p> <p>Don't see a Packer template that solves your needs? Create a feature request for a new template or learn how to contribute new assets to the project</p> Template Description Linux Build Agents Provision C++ compilation machines on Amazon Linux 2023 and Ubuntu machines on both x86 and ARM based architectures with useful tools like compiler caches such as Octobuild preinstalled. Windows Build Agents Create Windows 2022 based instances capable of Unreal Engine compilation out of the box. P4 Server (formerly Helix Core) An Amazon Machine Image used for provisioning P4 Server on AWS. This AMI is required for deployment of the Perforce module"},{"location":"docs/modules/index.html","title":"Modules","text":""},{"location":"docs/modules/index.html#introduction","title":"Introduction","text":"<p>A module is an automated deployment of a game development workload (i.e. Jenkins, P4 Server, Unreal Horde) that is implemented as a Terraform module. They are designed to provide flexibility and customization via input variables with defaults based on typical deployment architectures. They are designed to be depended on from other modules (including your own root module), easily integrate with each other, and provide relevant outputs to simplify permissions, networking, and access. Some of the modules have pre-requisites that will be outlined in their respective documentation.</p> <p>Note: While the project focuses on Terraform modules today, this project may expand to provide options for implementations built in other IaC tools such as AWS CDK in the future.</p> <p>Info</p> <p>Don't see a module listed? Create a feature request for a new module. If you'd like to contribute new modules to the project, see the general docs on contributing, as well as the module specific contribution docs below.</p> Module Description Perforce This module allows for deployment of Perforce resources on AWS. These are currently P4 Server (formerly Helix Core), P4Auth (formerly Helix Authentication Service), and P4 Code Review (formerly Helix Swarm). Unreal Horde This module allows for deployment of Unreal Horde on AWS. Unreal Cloud DDC This module allows for deployment of Unreal Cloud DDC (Derived Data Cache) on AWS. TeamCity This module allows for deployment of TeamCity resources on AWS. Jenkins This module allows for deployment of Jenkins on AWS."},{"location":"docs/modules/index.html#how-to-include-these-modules","title":"How to include these modules","text":"<p>We've found that including the CGD Toolkit repository as a git submodule in your own infrastructure repository is a good way of depending on the modules within an (existing) Terraform root module. Forking the CGD Toolkit and submoduling your fork may be a good approach if you intend to make changes to any modules. We recommend starting with the Terraform module documentation for a crash course in the way the CGD Toolkit is designed. Note how you can use the module source argument to declare modules that use the CGD Toolkit's module source code.</p>"},{"location":"docs/modules/index.html#contribution","title":"Contribution","text":"<p>Please follow these guidelines when developing a new module. These are also outlined in the pull-request template for Module additions.</p>"},{"location":"docs/modules/index.html#1-provider-configurations","title":"1. Provider Configurations","text":"<p>A module should not define its own provider configuration. Required provider versions should be outlined in a <code>required_versions</code> block inside of a <code>terraform</code> block:</p> <pre><code>terraform {\n    required_providers {\n        aws = {\n            source  = \"hashicorp/aws\"\n            version = \"&gt;= 5.30.0\"\n        }\n        #...additional required providers\n    }\n}\n</code></pre>"},{"location":"docs/modules/index.html#2-dependency-inversion","title":"2. Dependency Inversion","text":"<p>It is fine if your module needs to declare significant networking or compute resources to run - the Cloud Game Development Toolkit is intended to be highly opinionated. At the same time, we require that modules support a significant level of dependency injection through variables to support diverse use cases. This is a simple consideration that is easier to incorporate from the beginning of module development rather than retroactively.</p> <p>For example, the Jenkins module can provision its own Elastic Container Service cluster, or it can deploy the Jenkins service onto an existing cluster passed via the <code>cluster_name</code> variable.</p>"},{"location":"docs/modules/index.html#3-assumptions-and-guarantees","title":"3. Assumptions and Guarantees","text":"<p>If your module requires certain input formats in order to function Terraform refers to these as \"assumptions.\"</p> <p>If your module provides certain outputs in a consistent format that other configurations should be able to rely on Terraform calls these \"guarantees.\"</p> <p>We recommend outlining your module's assumptions and guarantees prior to implementation by using Terraform custom conditions. These can be used to validate input variables, data blocks, resource attributes, and much more. They are incredibly powerful.</p>"},{"location":"docs/modules/index.html#4-naming-conventions-and-tagging","title":"4. Naming Conventions and Tagging","text":"<p>A module should provide a method for easily tagging the resources that it creates, while following a common naming convention. Currently the modules achieve this with a <code>project_prefix</code> variable that defaults to <code>cgd</code> (for Cloud Game Development Toolkit). This <code>project_prefix</code> is prepended to the beginning of the names of the deployed resources. The names themselves should be descriptive enough, but generally brief. For longer naming, leverage <code>tags</code> for resources that support them, using the <code>Name</code> key.</p> <p>Ensure that tags have default values, but can be overwritten by users. For tags that we want to ensure are always present on resources, we achieve by merging the. For example:</p>"},{"location":"docs/modules/index.html#localstf","title":"locals.tf","text":"<pre><code>locals {\n    tags = merge(\n      {\n        \"environment\" = var.environment\n      },\n      var.tags,\n    )\n}\n</code></pre>"},{"location":"docs/modules/index.html#maintf","title":"main.tf","text":"<p><pre><code>  tags = merge(local.tags, {\n    Name = \"${local.name_prefix}-${var.p4_server_type}-${local.p4_server_az}\"\n  })\n</code></pre> The tags themselves should at the minimum include:</p> <ul> <li> <p><code>RootModuleName</code> - The name of the root module (only relevant if the module is a submodule)</p> </li> <li> <p><code>ModuleName</code> - The name of the module itself</p> </li> <li> <p><code>ModuleSource</code> - The location where the module is hosted</p> </li> </ul> <p>For example: <pre><code>variable \"tags\" {\n  type        = map(any)\n  description = \"Tags to apply to resources.\"\n  default = {\n    \"IaC\"            = \"Terraform\"\n    \"ModuleBy\"       = \"CGD-Toolkit\"\n    \"ParentModuleName\" = \"terraform-aws-perforce\"\n    \"ModuleName\"     = \"p4-server\"\n    \"ModuleSource\"   = \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/perforce\"\n  }\n</code></pre></p>"},{"location":"docs/modules/index.html#5-tests","title":"5. Tests","text":"<p>All modules must include tests. This is to ensure functionality of modules as net new modules are created, or new functionality is added to existing ones. We have standardized with Terraform test. For an example test, see the tests for the Perforce module.</p> <p>To learn how to get started with Terraform tests, see this AWS blog, and this Terraform documentation.</p>"},{"location":"docs/modules/index.html#6-third-party-software","title":"6. Third Party Software","text":"<p>The modules contained in the CGD Toolkit are designed to simplify infrastructure deployments of common game development workload. Naturally, modules may deploy third party applications - in these situations we require that deployments depend on existing licenses and distribution channels.</p> <p>If your module relies on a container or image that is not distributed through the CGD Toolkit we require a disclaimer and the usage of end-user credentials passed as a variable to the module. This repository is not to be used to redistribute software that may be subject to licensing or contractual agreements.</p> <p>If your module relies on a custom Amazon Machine Image (AMI) or container we ask that you provide a Packer template or Dockerfile in the <code>assets/</code> directory and include instructions to create the image prior to infrastructure deployment.</p>"},{"location":"modules/index.html","title":"Modules","text":""},{"location":"modules/index.html#introduction","title":"Introduction","text":"<p>These modules simplify the deployment of common game development workloads on AWS. Some have pre-requisites that will be outlined in their respective documentation. They are designed to be depended on from other modules (including your own root module), easily integrate with each other, and provide relevant outputs to simplify permissions, networking, and access.</p>"},{"location":"modules/index.html#how-to-include-these-modules","title":"How to include these modules","text":"<p>We've found that including the CGD Toolkit repository as a git submodule in your own infrastructure repository is a good way of depending on the modules within an (existing) Terraform root module. Forking the CGD Toolkit and submoduling your fork may be a good approach if you intend to make changes to any modules. We recommend starting with the Terraform module documentation for a crash course in the way the CGD Toolkit is designed. Note how you can use the module source argument to declare modules that use the CGD Toolkit's module source code.</p>"},{"location":"modules/index.html#contribution","title":"Contribution","text":"<p>Please follow these guidelines when developing a new module. These are also outlined in the pull-request template for Module additions.</p>"},{"location":"modules/index.html#1-provider-configurations","title":"1. Provider Configurations","text":"<p>Modules should not define its own provider configurations. Required provider versions should be outlined in a <code>required_versions</code> block inside of a <code>terraform</code> block:</p> <pre><code>terraform {\n    required_providers {\n        aws = {\n            source  = \"hashicorp/aws\"\n            version = \"&gt;= 5.30.0\"\n        }\n    }\n}\n</code></pre>"},{"location":"modules/index.html#2-dependency-inversion","title":"2. Dependency Inversion","text":"<p>It is fine if your module needs to declare significant networking or compute resources to run - the Cloud Game Development Toolkit is intended to be highly opinionated. At the same time, we require that modules support a significant level of dependency injection through variables to support diverse use cases. This is a simple consideration that is easier to incorporate from the beginning of module development rather than retroactively.</p> <p>For example, the Jenkins module can provision its own Elastic Container Service cluster, or it can deploy the Jenkins service onto an existing cluster passed via the <code>cluster_name</code> variable.</p>"},{"location":"modules/index.html#3-assumptions-and-guarantees","title":"3. Assumptions and Guarantees","text":"<p>If your module requires certain input formats in order to function Terraform refers to these as \"assumptions.\"</p> <p>If your module provides certain outputs in a consistent format that other configurations should be able to rely on Terraform calls these \"guarantees.\"</p> <p>We recommend outlining your module's assumptions and guarantees prior to implementation by using Terraform custom conditions. These can be used to validate input variables, data blocks, resource attributes, and much more. They are incredibly powerful.</p>"},{"location":"modules/index.html#4-third-party-software","title":"4. Third Party Software","text":"<p>The modules contained in the CGD Toolkit are designed to simplify infrastructure deployments of common game development workload. Naturally, modules may deploy third party applications - in these situations we require that deployments depend on existing licenses and distribution channels.</p> <p>If your module relies on a container or image that is not distributed through the CGD Toolkit we require a disclaimer and the usage of end-user credentials passed as a variable to the module. This repository is not to be used to redistribute software that may be subject to licensing or contractual agreements.</p> <p>If your module relies on a custom Amazon Machine Image (AMI) or container we ask that you provide a Packer template or Dockerfile in the <code>assets/</code> directory and include instructions to create the image prior to infrastructure deployment.</p>"},{"location":"modules/jenkins/index.html","title":"Jenkins","text":""},{"location":"modules/jenkins/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.97.0 random 3.7.1"},{"location":"modules/jenkins/index.html#providers","title":"Providers","text":"Name Version aws 5.97.0 random 3.7.1"},{"location":"modules/jenkins/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/jenkins/index.html#resources","title":"Resources","text":"Name Type aws_autoscaling_group.jenkins_build_farm_asg resource aws_cloudwatch_log_group.jenkins_service_log_group resource aws_ecs_cluster.jenkins_cluster resource aws_ecs_cluster_capacity_providers.jenkins_cluster_fargate_rpvodiers resource aws_ecs_service.jenkins_service resource aws_ecs_task_definition.jenkins_task_definition resource aws_efs_access_point.jenkins_efs_access_point resource aws_efs_backup_policy.policy resource aws_efs_file_system.jenkins_efs_file_system resource aws_efs_mount_target.jenkins_efs_mount_target resource aws_fsx_openzfs_file_system.jenkins_build_farm_fsxz_file_system resource aws_fsx_openzfs_volume.jenkins_build_farm_fsxz_volume resource aws_iam_instance_profile.build_farm_instance_profile resource aws_iam_policy.build_farm_fsxz_policy resource aws_iam_policy.build_farm_s3_policy resource aws_iam_policy.ec2_fleet_plugin_policy resource aws_iam_policy.jenkins_default_policy resource aws_iam_role.build_farm_role resource aws_iam_role.jenkins_default_role resource aws_iam_role.jenkins_task_execution_role resource aws_iam_role_policy_attachment.build_farm_role_fsxz_attachment resource aws_iam_role_policy_attachment.build_farm_role_s3_attachment resource aws_iam_role_policy_attachment.default_role resource aws_iam_role_policy_attachment.ec2_fleet_plugin_policy_attachment resource aws_iam_role_policy_attachment.task_execution resource aws_launch_template.jenkins_build_farm_launch_template resource aws_lb.jenkins_alb resource aws_lb_listener.jenkins_alb_https_listener resource aws_lb_target_group.jenkins_alb_target_group resource aws_s3_bucket.artifact_buckets resource aws_s3_bucket.jenkins_alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_s3_bucket_public_access_block.artifacts_bucket_public_block resource aws_s3_bucket_versioning.artifact_bucket_versioning resource aws_security_group.jenkins_alb_sg resource aws_security_group.jenkins_build_farm_sg resource aws_security_group.jenkins_build_storage_sg resource aws_security_group.jenkins_efs_security_group resource aws_security_group.jenkins_service_sg resource aws_vpc_security_group_egress_rule.jenkins_alb_outbound_service resource aws_vpc_security_group_egress_rule.jenkins_build_farm_outbound_ipv4 resource aws_vpc_security_group_egress_rule.jenkins_build_farm_outbound_ipv6 resource aws_vpc_security_group_egress_rule.jenkins_service_outbound_ipv4 resource aws_vpc_security_group_egress_rule.jenkins_service_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.jenkins_build_farm_inbound_ssh_service resource aws_vpc_security_group_ingress_rule.jenkins_build_vpc_all_traffic resource aws_vpc_security_group_ingress_rule.jenkins_efs_inbound_service resource aws_vpc_security_group_ingress_rule.jenkins_service_inbound_alb resource random_string.artifact_buckets resource random_string.build_farm resource random_string.fsxz resource random_string.jenkins resource random_string.jenkins_alb_access_logs_bucket_suffix resource aws_caller_identity.current data source aws_ecs_cluster.jenkins_cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.build_farm_fsxz_policy data source aws_iam_policy_document.build_farm_s3_policy data source aws_iam_policy_document.ec2_fleet_plugin_policy data source aws_iam_policy_document.ec2_trust_relationship data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.jenkins_default_policy data source aws_region.current data source aws_vpc.build_farm_vpc data source"},{"location":"modules/jenkins/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required artifact_buckets List of Amazon S3 buckets you wish to create to store build farm artifacts. <pre>map(    object({      name                 = string      enable_force_destroy = optional(bool, true)      enable_versioning    = optional(bool, true)      tags                 = optional(map(string), {})    })  )</pre> <code>null</code> no build_farm_compute Each object in this map corresponds to an ASG used by Jenkins as build agents. <pre>map(object(    {      ami = string      #TODO: Support mixed instances / spot with custom policies      instance_type     = string      ebs_optimized     = optional(bool, true)      enable_monitoring = optional(bool, true)    }  ))</pre> <code>{}</code> no build_farm_fsx_openzfs_storage Each object in this map corresponds to an FSx OpenZFS file system used by the Jenkins build agents. <pre>map(object(    {      storage_capacity    = number      throughput_capacity = number      storage_type        = optional(string, \"SSD\") # \"SSD\", \"HDD\"      deployment_type     = optional(string, \"SINGLE_AZ_1\")      route_table_ids     = optional(list(string), null)      tags                = optional(map(string), null)    }  ))</pre> <code>{}</code> no build_farm_subnets The subnets to deploy the build farms into. <code>list(string)</code> n/a yes certificate_arn The TLS certificate ARN for the Jenkins service load balancer. <code>string</code> <code>null</code> no cluster_name The ARN of the cluster to deploy the Jenkins service into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no container_cpu The CPU allotment for the Jenkins container. <code>number</code> <code>1024</code> no container_memory The memory allotment for the Jenkins container. <code>number</code> <code>4096</code> no container_name The name of the Jenkins service container. <code>string</code> <code>\"jenkins-container\"</code> no container_port The container port used by the Jenkins service container. <code>number</code> <code>8080</code> no create_application_load_balancer Controls creation of an application load balancer within the module. Defaults to true. <code>bool</code> <code>true</code> no create_ec2_fleet_plugin_policy Optional creation of IAM Policy required for Jenkins EC2 Fleet plugin. Default is set to false. <code>bool</code> <code>false</code> no create_jenkins_default_policy Optional creation of Jenkins Default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_jenkins_default_role Optional creation of Jenkins Default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_jenkins_role ARN of the custom IAM Role you wish to use with Jenkins. <code>string</code> <code>null</code> no enable_default_efs_backup_plan This flag controls EFS backups for the Jenkins module. Default is set to true. <code>bool</code> <code>true</code> no enable_jenkins_alb_access_logs Enables access logging for the Jenkins ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_jenkins_alb_deletion_protection Enables deletion protection for the Jenkins ALB. Defaults to true. <code>bool</code> <code>true</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_artifact_buckets List of ARNs of the S3 buckets used to store artifacts created by the build farm. <code>list(string)</code> <code>[]</code> no existing_security_groups A list of existing security group IDs to attach to the Jenkins service load balancer. <code>list(string)</code> <code>null</code> no internal Set this flag to true if you do not want the Jenkins service load balancer to have a public IP. <code>bool</code> <code>false</code> no jenkins_agent_secret_arns A list of secretmanager ARNs (wildcards allowed) that contain any secrets which need to be accessed by the Jenkins service. <code>list(string)</code> <code>null</code> no jenkins_alb_access_logs_bucket ID of the S3 bucket for Jenkins ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no jenkins_alb_access_logs_prefix Log prefix for Jenkins ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no jenkins_alb_subnets A list of subnet ids to deploy the Jenkins load balancer into. Public subnets are recommended. <code>list(string)</code> n/a yes jenkins_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Jenkins. <code>string</code> <code>365</code> no jenkins_efs_performance_mode The performance mode of the EFS file system used by the Jenkins service. Defaults to general purpose. <code>string</code> <code>\"generalPurpose\"</code> no jenkins_efs_throughput_mode The throughput mode of the EFS file system used by the Jenkins service. Defaults to bursting. <code>string</code> <code>\"bursting\"</code> no jenkins_service_desired_container_count The desired number of containers running the Jenkins service. <code>number</code> <code>1</code> no jenkins_service_subnets A list of subnets to deploy the Jenkins service into. Private subnets are recommended. <code>list(string)</code> n/a yes name The name attached to Jenkins module resources. <code>string</code> <code>\"jenkins\"</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"iac-management\": \"CGD-Toolkit\",  \"iac-module\": \"Jenkins\",  \"iac-provider\": \"Terraform\"}</pre> no vpc_id The ID of the existing VPC you would like to deploy the Jenkins service and build farms into. <code>string</code> n/a yes"},{"location":"modules/jenkins/index.html#outputs","title":"Outputs","text":"Name Description alb_security_group_id Security group associated with the Jenkins load balancer build_farm_security_group_id Security group associated with the build farm autoscaling groups jenkins_alb_dns_name The DNS name of the Jenkins application load balancer. jenkins_alb_zone_id The zone ID of the Jenkins ALB. service_security_group_id Security group associated with the ECS service hosting jenkins service_target_group_arn The ARN of the Jenkins service target group"},{"location":"modules/perforce/index.html","title":"Perforce on AWS Terraform Module","text":"<p>For a video walkthrough demonstrating how to use this module, see this YouTube Video:</p> <p></p>"},{"location":"modules/perforce/index.html#features","title":"Features","text":"<ul> <li>Dynamic creation and configuration of P4 Server (formerly Helix Core)</li> <li>Dynamic creation and configuration   of P4 Code Review (formerly Helix Swarm)</li> <li>Dynamic creation and configuration   of P4Auth (formerly Helix Authentication Service)</li> </ul>"},{"location":"modules/perforce/index.html#architecture","title":"Architecture","text":""},{"location":"modules/perforce/index.html#full-example-using-aws-route53-public-hosted-zone","title":"Full example using AWS Route53 Public Hosted Zone","text":""},{"location":"modules/perforce/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Existing DNS Configured<ul> <li>To use this module, you must have an existing domain and related DNS configuration. The example at   <code>/examples/create-resources-complete</code> demonstrates how to provision resources while using Amazon Route53 (   recommended) as the DNS provider. This will make deployment and management easier.</li> <li>You may optionally use a 3rd party DNS provider, however you must create records in your DNS provider to route to   the endpoints that you will create for each component when using the module (e.g. <code>perforce.example.com</code>,   <code>review.perforce.example.com</code>, <code>auth.perforce.example.com</code>). The module has variables that you can use to   customize the subdomains for the services (P4 Server, P4 Code Review, P4Auth), however if not set, the defaults   mentioned above will be used. Ensure you create these records to allow users to connect to the services once   provisioned in AWS.</li> <li>Note: When using either of the two options mentioned above, by default the module will create a Route53   Private Hosted Zone. This is used for internal communication and routing of traffic between P4 Server, P4 Code   Review, and P4Auth.</li> </ul> </li> <li> <p>SSL TLS Certificate</p> <ul> <li>You must have an existing SSL/TLS certificate, or create one during deployment alongside the other resources the   module will create. This is used to provide secure connectivity to the Perforce resources that will be running in   AWS. The certificate will be used by the Application Load Balancer (ALB) that the module will deploy for you. If   using Amazon Route53, see the example at <code>/examples/create-resources-complete</code> to see how to create the related   certificate in Amazon Certificate Manager (ACM). Using a Route53 as the DNS provider makes this process a bit   easier, as ACM can automatically create the required CNAME records needed for DNS validation (a process required   to verify DNS ownership) if you are also using Amazon Route53.</li> <li>If using an 3rd party DNS provider, you must add these CNAME records manually (in addition to the other records   mentioned above for general DNS purposes). If you would prefer to use a 3rd party to create the SSL/TLS   certificate, the module allows you to import this into ACM to be used for the other components that will be   deployed (such as the internal ALB). You may also use Email validation to validate DNS ownership.</li> </ul> </li> <li> <p>Existing Perforce Amazon Machine Image (AMI)</p> <ul> <li>As mentioned in the architecture, an Amazon EC2 instance is used for the P4 Server, and this instance must be be   provisioned using an AMI that is configured for Perforce. To expedite this process, we have   sample HashiCorp Packer templates provided in   the AWS Cloud Game Development Toolkit repository   that you can use to create a Perforce AMI in your AWS Account. Note: You must also reference the   <code>p4_configure.sh</code> and <code>p4_setup.sh</code> files that are in this directory, as these are used to configure the P4 Commit   Server. These are already referenced in the <code>perforce_arm64.pkr.hcl</code> and <code>perforce_x86.pkr.hcl</code> packer templates   that are available for use.</li> </ul> </li> </ul>"},{"location":"modules/perforce/index.html#examples","title":"Examples","text":"<p>For example configurations, please see the examples.</p>"},{"location":"modules/perforce/index.html#deployment-instructions","title":"Deployment Instructions","text":"<ol> <li>Create the Perforce AMI in your AWS account using one of the supplied Packer templates. Ensure you use the Packer    template that aligns with the architecture type (e.g. arm64) of the EC2 instance you wish to create. On the Terraform    side, you may also set this using the <code>instance_architecture</code> variable. Ensure your <code>instance_type</code> is supported for    your desired <code>instance_architecture</code>. For a full list of this mapping, see    the AWS Docs for EC2 Naming Conventions.    You can also use the interactive chart on Instances by Vantage.</li> </ol> <p>IMPORTANT: By default, the module will create compute resources with <code>x86_64</code> architecture. Ensure you use this corresponding Packer template unless you set the <code>instance_architecture</code> variable to <code>arm64</code> or the deployment will fail. Also, unless explicitly set, the Packer templates are configured to build the AMI in whichever AWS region your current credentials are set to (e.g. <code>us-east-1</code>) which will also be the same AWS region your Terraform resources are deployed to unless you explicitly set this. Ensure the AMI is available in the AWS Region you will use the module to deploy resources into.</p> <p>To deploy the template (<code>x86_64</code>) with Packer, do the following (while in the <code>/assets/perforce/p4-server directory</code>)</p> <pre><code>packer init perforce_x86.pkr.hcl\n</code></pre> <pre><code>packer validate perforce_x86.pkr.hcl\n</code></pre> <pre><code>packer build perforce_x86.pkr.hcl\n</code></pre> <ol> <li> <p>Reference your existing fully qualified domain name within each related Perforce service you would like to    provision (e.g. <code>p4_server_config</code>, <code>p4_auth_config</code>, <code>p4_code_review_config</code>) using the    <code>fully_qualified_domain_name</code> variable. We recommend abstracting this t a local value such as    <code>local.fully_qualified_domain_name</code> to ensure this value is consistent across the modules. The module will    automatically configure Perforce using default subdomains of <code>perforce.&lt;your-domain-name&gt;</code> for P4 Server,    <code>auth.perforce.&lt;your-domain-name</code> for P4 Auth, and <code>review.perforce.&lt;your-domain-name</code> for P4 Code Review. You will    also need to create DNS records that will route traffic destined for these domains in the following manner:</p> <ul> <li>Traffic destined for P4 Server will need to route to the Elastic IP (EIP) that is associated with the P4   Server EC2 Instance. By default, this will be using a subdomain named <code>perforce</code>. In your DNS provider, create an   A record named <code>perforce.&lt;your-domain-name&gt;</code> and have it route traffic to the EIP. This value is available as a   Terraform output for your convenience.</li> <li>Traffic destined for <code>*.perforce.&lt;your-domain-name&gt;</code> will need to route to the DNS name of the Network Load   Balancer (NLB) that the module creates. In your DNS provider, create a CNAME record that routes traffic to NLB.   This value is available as a Terraform output for your convenience.</li> <li>Note: If using Amazon Route53 as your DNS provider, the example at  <code>/examples/create-resources-complete</code>   shows you have to leverage Terraform to automatically create these records in an existing Route53 Public Hosted   Zone, as well as how to create the certificate in Amazon Certificate Manager (ACM).</li> </ul> </li> <li> <p>Make any other modifications as desired (such as referencing existing VPC resources) and run <code>terraform init</code> to    initialize Terraform in the current working directory, <code>terraform plan</code> to create and validate the execution plan of    the resources that will be created, and finally <code>terraform apply</code> to create the resources in your AWS Account.</p> </li> <li>Once the resources have finished provisioning successfully, you will need to modify your inbound Security Group Rules    on the P4 Commit Server Instance to allow TCP traffic from your public IP on port 1666 (the perforce default port).    This is necessary to allow your local machine(s) to connect to the P4 Commit Server. Optionally, you can pass in an entire security group to also add to the resource. The complete example demonstrates how to use the <code>existing_security_groups</code> variable to accomplish this.<ul> <li>Note: You may use other means to allow traffic to reach this EC2 Instance (Customer-managed prefix list, VPN   to the VPC that the instance is running in, etc.) but regardless, it is essential that you have the security group   rules set configured correctly to allow access.</li> </ul> </li> <li>Next, modify your inbound Security Group rules for the Perforce Network Load Balancer (NLB) to allow traffic from    HTTPS (port 443) from your public IP address/ This is to provide access to the P4 Code Review and P4Auth services    that are running behind the Application Load Balancer (ALB). Optionally, you can pass in an entire security group to also add to the resource. The complete example demonstrates how to use the <code>existing_security_groups</code> variable to accomplish this.<ul> <li>Note: You may use other means to allow traffic to reach this the Network Load Balancer (Customer-managed   prefix list, VPN to the VPC that the instance is running in, etc.) but regardless, it is essential that you have   the security group rules set configured correctly to allow access.</li> <li>IMPORTANT: Ensure your networking configuration is correct, especially in terms of any public or private   subnets that you reference. This is very important for the internal routing between the P4 resources, as well as   the related Security Groups. Failure to set these correctly may cause a variety of connectivity issues such as web   pages not loading, NLB health checks failing, etc.</li> </ul> </li> <li>Use the provided Terraform outputs to quickly find the URL for P4Auth, P4 Code Review. If you haven't modified the    default values, relevant values for the P4 Server default username/password, and the P4 Code Review default    username/password were created for you and are stored in AWS Secrets Manager.</li> <li>In P4V, use the url of <code>ssl:&lt;your-supplied-root-domain&gt;:1666</code> and the username and password stored in AWS Secrets    Manager to gain access to the commit server.</li> <li>At this point, you should be able to access your P4 Commit Server (P4), and visit the URLs for P4 Code Review (P4    Code Review) and P4Auth (P4Auth).</li> </ol>"},{"location":"modules/perforce/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.97.0 awscc 1.34.0 local 2.5.2 null 3.2.4 random 3.7.1"},{"location":"modules/perforce/index.html#providers","title":"Providers","text":"Name Version aws 5.97.0 null 3.2.4 random 3.7.1"},{"location":"modules/perforce/index.html#modules","title":"Modules","text":"Name Source Version p4_auth ./modules/p4-auth n/a p4_code_review ./modules/p4-code-review n/a p4_server ./modules/p4-server n/a"},{"location":"modules/perforce/index.html#resources","title":"Resources","text":"Name Type aws_ecs_cluster.perforce_web_services_cluster resource aws_ecs_cluster_capacity_providers.providers resource aws_lb.perforce resource aws_lb.perforce_web_services resource aws_lb_listener.perforce resource aws_lb_listener.perforce_web_services resource aws_lb_listener_rule.p4_code_review resource aws_lb_listener_rule.perforce_p4_auth resource aws_lb_target_group.perforce resource aws_lb_target_group_attachment.perforce resource aws_route53_record.internal_p4_server resource aws_route53_record.internal_perforce_web_services resource aws_route53_zone.perforce_private_hosted_zone resource aws_s3_bucket.shared_lb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.shared_access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.shared_lb_access_logs_bucket_policy resource aws_security_group.perforce_network_load_balancer resource aws_security_group.perforce_web_services_alb resource aws_vpc_security_group_egress_rule.p4_code_review_outbound_to_p4_server resource aws_vpc_security_group_egress_rule.perforce_alb_outbound_to_p4_auth resource aws_vpc_security_group_egress_rule.perforce_alb_outbound_to_p4_code_review resource aws_vpc_security_group_egress_rule.perforce_nlb_outbound_to_perforce_web_services_alb resource aws_vpc_security_group_ingress_rule.p4_auth_inbound_from_perforce_web_services_alb resource aws_vpc_security_group_ingress_rule.p4_code_review_inbound_from_perforce_web_services_alb resource aws_vpc_security_group_ingress_rule.p4_server_inbound_from_p4_code_review resource aws_vpc_security_group_ingress_rule.perforce_web_services_inbound_from_p4_server resource aws_vpc_security_group_ingress_rule.perforce_web_services_inbound_from_perforce_nlb resource null_resource.parent_module_certificate resource random_string.shared_lb_access_logs_bucket resource aws_elb_service_account.main data source aws_iam_policy_document.shared_lb_access_logs_bucket_lb_write data source"},{"location":"modules/perforce/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required certificate_arn The ARN of the ACM certificate to be used with the HTTPS listener for the NLB. <code>string</code> <code>null</code> no create_default_sgs Whether to create default security groups for the Perforce resources. <code>bool</code> <code>true</code> no create_route53_private_hosted_zone Whether to create a private Route53 Hosted Zone for the Perforce resources. This private hosted zone is used for internal communication between the P4 Server, P4 Auth Service, and P4 Code Review Service. <code>bool</code> <code>true</code> no create_shared_application_load_balancer Whether to create a shared Application Load Balancer for the Perforce resources. <code>bool</code> <code>true</code> no create_shared_network_load_balancer Whether to create a shared Network Load Balancer for the Perforce resources. <code>bool</code> <code>true</code> no enable_shared_alb_deletion_protection Enables deletion protection for the shared Application Load Balancer for the Perforce resources. <code>bool</code> <code>false</code> no enable_shared_lb_access_logs Enables access logging for both the shared NLB and shared ALB. Defaults to false. <code>bool</code> <code>false</code> no existing_ecs_cluster_name The name of an existing ECS cluster to use for the Perforce server. If omitted a new cluster will be created. <code>string</code> <code>null</code> no existing_security_groups A list of existing security group IDs to attach to the shared network load balancer. <code>list(string)</code> <code>[]</code> no p4_auth_config # General    name: \"The string including in the naming of resources related to P4Auth. Default is 'p4-auth'.\"    project_prefix : \"The project prefix for the P4Auth service. Default is 'cgd'.\"    environment : \"The environment where the P4Auth service will be deployed. Default is 'dev'.\"    enable_web_based_administration: \"Whether to de enable web based administration. Default is 'true'.\"    debug : \"Whether to enable debug mode for the P4Auth service. Default is 'false'.\"    fully_qualified_domain_name : \"The FQDN for the P4Auth Service. This is used for the P4Auth's Perforce configuration.\"    # Compute    cluster_name : \"The name of the ECS cluster where the P4Auth service will be deployed. Cluster is not created if this variable is null.\"    container_name : \"The name of the P4Auth service container. Default is 'p4-auth-container'.\"    container_port : \"The port on which the P4Auth service will be listening. Default is '3000'.\"    container_cpu : \"The number of CPU units to reserve for the P4Auth service container. Default is '1024'.\"    container_memory : \"The number of CPU units to reserve for the P4Auth service container. Default is '4096'.\"    # Storage &amp; Logging    cloudwatch_log_retention_in_days : \"The number of days to retain the P4Auth service logs in CloudWatch. Default is 365 days.\"    # Networking    create_defaults_sgs : \"Whether to create default security groups for the P4Auth service.\"    internal : \"Set this flag to true if you do not want the P4Auth service to have a public IP.\"    create_default_role : \"Whether to create the P4Auth default IAM Role. Default is set to true.\"    custom_role : \"ARN of a custom IAM Role you wish to use with P4Auth.\"    admin_username_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4Auth Administrator username.\"    admin_password_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4Auth Administrator password.\" <pre>object({    # - General -    name                            = optional(string, \"p4-auth\")    project_prefix                  = optional(string, \"cgd\")    environment                     = optional(string, \"dev\")    enable_web_based_administration = optional(bool, true)    debug                           = optional(bool, false)    fully_qualified_domain_name     = string    # - Compute -    container_name   = optional(string, \"p4-auth-container\")    container_port   = optional(number, 3000)    container_cpu    = optional(number, 1024)    container_memory = optional(number, 4096)    # - Storage &amp; Logging -    cloudwatch_log_retention_in_days = optional(number, 365)    # - Networking &amp; Security -    service_subnets          = optional(list(string), null)    create_default_sgs       = optional(bool, true)    existing_security_groups = optional(list(string), [])    internal                 = optional(bool, false)    certificate_arn           = optional(string, null)    create_default_role       = optional(bool, true)    custom_role               = optional(string, null)    admin_username_secret_arn = optional(string, null)    admin_password_secret_arn = optional(string, null)  })</pre> <code>null</code> no p4_code_review_config # General    name: \"The string including in the naming of resources related to P4 Code Review. Default is 'p4-code-review'.\"    project_prefix : \"The project prefix for the P4 Code Review service. Default is 'cgd'.\"    environment : \"The environment where the P4 Code Review service will be deployed. Default is 'dev'.\"    debug : \"Whether to enable debug mode for the P4 Code Review service. Default is 'false'.\"    fully_qualified_domain_name : \"The FQDN for the P4 Code Review Service. This is used for the P4 Code Review's Perforce configuration.\"    # Compute    container_name : \"The name of the P4 Code Review service container. Default is 'p4-code-review-container'.\"    container_port : \"The port on which the P4 Code Review service will be listening. Default is '3000'.\"    container_cpu : \"The number of CPU units to reserve for the P4 Code Review service container. Default is '1024'.\"    container_memory : \"The number of CPU units to reserve for the P4 Code Review service container. Default is '4096'.\"    pd4_port : \"The full URL you will use to access the P4 Depot in clients such P4V and P4Admin. Note, this typically starts with 'ssl:' and ends with the default port of ':1666'.\"    existing_redis_connection : \"The existing Redis connection for the P4 Code Review service.\"    # Storage &amp; Logging    cloudwatch_log_retention_in_days : \"The number of days to retain the P4 Code Review service logs in CloudWatch. Default is 365 days.\"    # Networking &amp; Security    create_default_sgs : \"Whether to create default security groups for the P4 Code Review service.\"    internal : \"Set this flag to true if you do not want the P4 Code Review service to have a public IP.\"    create_default_role : \"Whether to create the P4 Code Review default IAM Role. Default is set to true.\"    custom_role : \"ARN of a custom IAM Role you wish to use with P4 Code Review.\"    super_user_password_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4 Code Review Administrator username.\"    super_user_username_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4 Code Review Administrator password.\"    p4d_p4_code_review_user_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4 Code Review user's username.\"    p4d_p4_code_review_password_secret_arn : \"Optionally provide the ARN of an AWS Secret for the P4 Code Review user's password.\"    p4d_p4_code_review_user_password_arn : \"Optionally provide the ARN of an AWS Secret for the P4 Code Review user's password.\"    enable_sso : \"Whether to enable SSO for the P4 Code Review service. Default is set to false.\"    # Caching    elasticache_node_count : \"The number of Elasticache nodes to create for the P4 Code Review service. Default is '1'.\"    elasticache_node_type : \"The type of Elasticache node to create for the P4 Code Review service. Default is 'cache.t4g.micro'.\" <pre>object({    # General    name                        = optional(string, \"p4-code-review\")    project_prefix              = optional(string, \"cgd\")    environment                 = optional(string, \"dev\")    debug                       = optional(bool, false)    fully_qualified_domain_name = string    # Compute    container_name   = optional(string, \"p4-code-review-container\")    container_port   = optional(number, 80)    container_cpu    = optional(number, 1024)    container_memory = optional(number, 4096)    p4d_port         = optional(string, null)    existing_redis_connection = optional(object({      host = string      port = number    }), null)    # Storage &amp; Logging    cloudwatch_log_retention_in_days = optional(number, 365)    # Networking &amp; Security    create_default_sgs       = optional(bool, true)    existing_security_groups = optional(list(string), [])    internal                 = optional(bool, false)    service_subnets          = optional(list(string), null)    create_default_role = optional(bool, true)    custom_role         = optional(string, null)    super_user_password_secret_arn          = optional(string, null)    super_user_username_secret_arn          = optional(string, null)    p4_code_review_user_password_secret_arn = optional(string, null)    p4_code_review_user_username_secret_arn = optional(string, null)    enable_sso                              = optional(string, true)    # Caching    elasticache_node_count = optional(number, 1)    elasticache_node_type  = optional(string, \"cache.t4g.micro\")  })</pre> <code>null</code> no p4_server_config # - General -    name: \"The string including in the naming of resources related to P4 Server. Default is 'p4-server'\"    project_prefix: \"The project prefix for this workload. This is appended to the beginning of most resource names.\"    environment: \"The current environment (e.g. dev, prod, etc.)\"    auth_service_url: \"The URL for the P4Auth Service.\"    fully_qualified_domain_name = \"The FQDN for the P4Auth Service. This is used for the P4 Server's Perforce configuration.\"    # - Compute -    lookup_existing_ami : \"Whether to lookup the existing Perforce P4 Server AMI.\"    ami_prefix: \"The AMI prefix to use for the AMI that will be created for P4 Server.\"    instance_type: \"The instance type for Perforce P4 Server. Defaults to c6g.large.\"    instance_architecture: \"The architecture of the P4 Server instance. Allowed values are 'arm64' or 'x86_64'.\"    IMPORTANT: \"Ensure the instance family of the instance type you select supports the instance_architecture you select. For example, 'c6in' instance family only works for 'x86_64' architecture, not 'arm64'. For a full list of this mapping, see the AWS Docs for EC2 Naming Conventions: https://docs.aws.amazon.com/ec2/latest/instancetypes/instance-type-names.html\"    p4_server_type: \"The Perforce P4 Server server type. Valid values are 'p4d_commit' or 'p4d_replica'.\"    unicode: \"Whether to enable Unicode configuration for P4 Server the -xi flag for p4d. Set to true to enable Unicode support.\"    selinux: \"Whether to apply SELinux label updates for P4 Server. Don't enable this if SELinux is disabled on your target operating system.\"    case_sensitive: \"Whether or not the server should be case insensitive (Server will run '-C1' mode), or if the server will run with case sensitivity default of the underlying platform. False enables '-C1' mode. Default is set to true.\"    plaintext: \"Whether to enable plaintext authentication for P4 Server. This is not recommended for production environments unless you are using a load balancer for TLS termination. Default is set to false.\"    # - Storage -    storage_type: \"The type of backing store. Valid values are either 'EBS' or 'FSxN'\"    depot_volume_size: \"The size of the depot volume in GiB. Defaults to 128 GiB.\"    metadata_volume_size: \"The size of the metadata volume in GiB. Defaults to 32 GiB.\"    logs_volume_size: \"The size of the logs volume in GiB. Defaults to 32 GiB.\"    # - Networking &amp; Security -    instance_subnet_id: \"The subnet where the P4 Server instance will be deployed.\"    create_default_sg : \"Whether to create a default security group for the P4 Server instance.\"    existing_security_groups: \"A list of existing security group IDs to attach to the P4 Server load balancer.\"    internal: \"Set this flag to true if you do not want the P4 Server instance to have a public IP.\"    super_user_password_secret_arn: \"If you would like to manage your own super user credentials through AWS Secrets Manager provide the ARN for the super user's username here. Otherwise, the default of 'perforce' will be used.\"    super_user_username_secret_arn: \"If you would like to manage your own super user credentials through AWS Secrets Manager provide the ARN for the super user's password here.\"    create_default_role: \"Optional creation of P4 Server default IAM Role with SSM managed instance core policy attached. Default is set to true.\"    custom_role: \"ARN of a custom IAM Role you wish to use with P4 Server.\" <pre>object({    # General    name                        = optional(string, \"p4-server\")    project_prefix              = optional(string, \"cgd\")    environment                 = optional(string, \"dev\")    auth_service_url            = optional(string, null)    fully_qualified_domain_name = string    # Compute    lookup_existing_ami = optional(bool, true)    ami_prefix          = optional(string, \"p4_al2023\")    instance_type         = optional(string, \"c6i.large\")    instance_architecture = optional(string, \"x86_64\")    p4_server_type        = optional(string, null)    unicode        = optional(bool, false)    selinux        = optional(bool, false)    case_sensitive = optional(bool, true)    plaintext      = optional(bool, false)    # Storage    storage_type         = optional(string, \"EBS\")    depot_volume_size    = optional(number, 128)    metadata_volume_size = optional(number, 32)    logs_volume_size     = optional(number, 32)    # Networking &amp; Security    instance_subnet_id       = optional(string, null)    create_default_sg        = optional(bool, true)    existing_security_groups = optional(list(string), [])    internal                 = optional(bool, false)    super_user_password_secret_arn = optional(string, null)    super_user_username_secret_arn = optional(string, null)    create_default_role = optional(bool, true)    custom_role         = optional(string, null)    # FSxN    fsxn_password                     = optional(string, null)    fsxn_filesystem_security_group_id = optional(string, null)    protocol                          = optional(string, null)    fsxn_region                       = optional(string, null)    fsxn_management_ip                = optional(string, null)    fsxn_svm_name                     = optional(string, null)    amazon_fsxn_svm_id                = optional(string, null)    fsxn_aws_profile                  = optional(string, null)  })</pre> <code>null</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no route53_private_hosted_zone_name The name of the private Route53 Hosted Zone for the Perforce resources. <code>string</code> <code>null</code> no s3_enable_force_destroy Enables force destroy for the S3 bucket for both the shared NLB and shared ALB access log storage. Defaults to true. <code>bool</code> <code>true</code> no shared_alb_access_logs_prefix Log prefix for shared ALB access logs. <code>string</code> <code>\"perforce-alb-\"</code> no shared_alb_subnets A list of subnets to attach to the shared application load balancer. <code>list(string)</code> <code>null</code> no shared_application_load_balancer_name The name of the shared Application Load Balancer for the Perforce resources. <code>string</code> <code>\"p4alb\"</code> no shared_ecs_cluster_name The name of the ECS cluster to use for the shared ECS Cluster. <code>string</code> <code>\"perforce-cluster\"</code> no shared_lb_access_logs_bucket ID of the S3 bucket for both the shared NLB and shared ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no shared_network_load_balancer_name The name of the shared Network Load Balancer for the Perforce resources. <code>string</code> <code>\"p4nlb\"</code> no shared_nlb_access_logs_prefix Log prefix for shared NLB access logs. <code>string</code> <code>\"perforce-nlb-\"</code> no shared_nlb_subnets A list of subnets to attach to the shared network load balancer. <code>list(string)</code> <code>null</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"terraform-aws-perforce\",  \"ModuleSource\": \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/perforce/terraform-aws-perforce\",  \"RootModuleName\": \"-\"}</pre> no vpc_id The VPC ID where the Perforce resources will be deployed. <code>string</code> n/a yes"},{"location":"modules/perforce/index.html#outputs","title":"Outputs","text":"Name Description p4_auth_alb_dns_name The DNS name of the P4Auth ALB. p4_auth_alb_security_group_id Security group associated with the P4Auth load balancer. p4_auth_alb_zone_id The hosted zone ID of the P4Auth ALB. p4_auth_perforce_cluster_name Name of the ECS cluster hosting P4Auth. p4_auth_service_security_group_id Security group associated with the ECS service running P4Auth. p4_auth_target_group_arn The service target group for the P4Auth. p4_code_review_alb_dns_name The DNS name of the P4 Code Review ALB. p4_code_review_alb_security_group_id Security group associated with the P4 Code Review load balancer. p4_code_review_alb_zone_id The hosted zone ID of the P4 Code Review ALB. p4_code_review_perforce_cluster_name Name of the ECS cluster hosting P4 Code Review. p4_code_review_service_security_group_id Security group associated with the ECS service running P4 Code Review. p4_code_review_target_group_arn The service target group for the P4 Code Review. p4_server_eip_id The ID of the Elastic IP associated with your P4 Server instance. p4_server_eip_public_ip The public IP of your P4 Server instance. p4_server_instance_id Instance ID for the P4 Server instance p4_server_lambda_link_name The name of the Lambda link for the P4 Server instance to use with FSxN. p4_server_private_ip Private IP for the P4 Server instance p4_server_security_group_id The default security group of your P4 Server instance. p4_server_super_user_password_secret_arn The ARN of the AWS Secrets Manager secret holding your P4 Server super user's username. p4_server_super_user_username_secret_arn The ARN of the AWS Secrets Manager secret holding your P4 Server super user's password. shared_application_load_balancer_arn The ARN of the shared application load balancer. shared_network_load_balancer_arn The ARN of the shared network load balancer. END_TF_DOCS"},{"location":"modules/perforce/examples/create-resources-complete/index.html","title":"Create Resources Complete (with Route53)","text":"<p>This example deploys P4 Server (formerly Helix Core), P4 Code Review (formerly Helix Swarm), and the P4 Auth Service (formerly the Helix Auth Service) using Amazon Route53 as the DNS provider.</p>"},{"location":"modules/perforce/examples/create-resources-complete/index.html#architecture","title":"Architecture","text":""},{"location":"modules/perforce/examples/create-resources-complete/index.html#important","title":"Important","text":"<p>This example creates DNS records in an existing Rout53 Public Hosted Zone and as well as an ACM Certificate. This certificate needs to be validated, which is not a fixed amount of time. During a standard deployment this is a non-issue since multiple dependent resources take longer to deploy than the certificate takes to validate. However, if you change the name of your domain where referenced in the ACM certificate after the initial apply, you may encounter the following error:</p> <pre><code>\u2502 Error : modifying ELBv2 Listener (arn : aws :elasticloadbalancing : us-east-1 : xx : listener/app/cgd-perforce-shared-alb/xx) : operation error Elastic Load Balancing v2 : ModifyListener, https response error StatusCode : 400, RequestID : xx-xx-xx-xx-xx, api error UnsupportedCertificate : The certificate 'arn:aws:acm:us-east-1:x:certificate/xx-xx-xx-xx-xx' must have a fully-qualified domain name, a supported signature, and a supported key size.\n\u2502\n\u2502   with module.terraform-aws-perforce.aws_lb_listener.perforce_web_services[0\n],\n\u2502   on../../lb.tf line 161, in resource \"aws_lb_listener\" \"perforce_web_services\" :\n\u2502  161 : resource \"aws_lb_listener\" \"perforce_web_services\" {\n</code></pre> <p>If this occurs, it is because Terraform is attempting to attach the certificate to the ALB listener before it has finished validation. Wait a few minutes and retry <code>terraform apply</code>.</p>"},{"location":"modules/perforce/examples/create-resources-complete/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.97.0 awscc 1.34.0 http 3.5.0 netapp-ontap 2.1.0 random 3.7.1"},{"location":"modules/perforce/examples/create-resources-complete/index.html#providers","title":"Providers","text":"Name Version aws 5.97.0 http 3.5.0"},{"location":"modules/perforce/examples/create-resources-complete/index.html#modules","title":"Modules","text":"Name Source Version terraform-aws-perforce ../../ n/a"},{"location":"modules/perforce/examples/create-resources-complete/index.html#resources","title":"Resources","text":"Name Type aws_acm_certificate.perforce resource aws_acm_certificate_validation.perforce resource aws_default_security_group.default resource aws_eip.nat_gateway_eip resource aws_internet_gateway.igw resource aws_nat_gateway.nat_gateway resource aws_route.private_rt_nat_gateway resource aws_route53_record.external_perforce_p4_server resource aws_route53_record.external_perforce_web_services resource aws_route53_record.perforce_cert resource aws_route_table.private_rt resource aws_route_table.public_rt resource aws_route_table_association.private_rt_asso resource aws_route_table_association.public_rt_asso resource aws_security_group.allow_my_ip resource aws_subnet.private_subnets resource aws_subnet.public_subnets resource aws_vpc.perforce_vpc resource aws_vpc_security_group_ingress_rule.allow_http resource aws_vpc_security_group_ingress_rule.allow_https resource aws_vpc_security_group_ingress_rule.allow_icmp resource aws_vpc_security_group_ingress_rule.allow_perforce resource aws_availability_zones.available data source aws_lb.shared_services_nlb data source aws_route53_zone.root data source http_http.my_ip data source"},{"location":"modules/perforce/examples/create-resources-complete/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required route53_public_hosted_zone_name The name of your existing Route53 Public Hosted Zone. This is required to create the ACM certificate and Route53 records. <code>string</code> n/a yes"},{"location":"modules/perforce/examples/create-resources-complete/index.html#outputs","title":"Outputs","text":"Name Description p4_auth_admin_url The URL for the P4Auth service admin page. p4_code_review_url The URL for the P4 Code Review service. p4_server_connection_string The connection string for the P4 Server. Set your P4PORT environment variable to this value."},{"location":"modules/perforce/modules/p4-auth/index.html","title":"P4Auth Submodule","text":"<p>P4Auth enables you to integrate certain Perforce products with your organization's Identity Provider (IdP).</p> <p>This module creates the following resources:</p> <ul> <li>An Elastic Container Service (ECS) cluster backed by AWS Fargate. This can also be created externally and passed in via the <code>cluster_name</code> variable.</li> <li>An ECS service running the latest P4Auth container (perforce/helix-auth-svc) available.</li> <li>AWS Secrets Manager secrets for an administrative user that has access to the Helix Authentication Service's web UI. These credentials are needed to configure external identity providers through the UI.</li> <li>Supporting resources such as Cloudwatch log groups, IAM roles, and security groups.</li> </ul>"},{"location":"modules/perforce/modules/p4-auth/index.html#architecture","title":"Architecture","text":""},{"location":"modules/perforce/modules/p4-auth/index.html#prerequisites","title":"Prerequisites","text":"<p>P4Admin can be configured at deployment time or through the web UI following deployment. If you opt to configure P4Admin through the web-based UI you will need to create an administrative user for initial login. You can either create and upload these credentials to AWS Secrets Manager yourself, or you opt to have the module create these credentials for you. The parent module does this by default.</p> <p>Should you choose to create this administrative user yourself you will need to specify the ARN for the username and password as module variables. You can create the secret using the AWS CLI:</p> <pre><code>aws secretsmanager create-secret \\\n    --name P4AuthAdmin \\\n    --description \"P4Auth Admin\" \\\n    --secret-string \"{\\\"username\\\":\\\"admin\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\"\n</code></pre> <p>And then provide the relevant ARNs as variables when you define the Helix Authentication module in your Terraform configurations:</p> <pre><code>module \"p4_auth\" {\n    source = \"modules/perforce/modules/p4-auth\"\n    ...\n    admin_username_secret_arn = \"arn:aws:secretsmanager:&lt;your-aws-region&gt;:&lt;your-aws-account-id&gt;:secret:P4AuthAdmin-a1b2c3:username::\"\n    admin_password_secret_arn = \"arn:aws:secretsmanager:&lt;your-aws-region&gt;:&lt;your-aws-account-id&gt;:secret:P4AuthAdmin-a1b2c3:password::\"\n}\n</code></pre> <p>If you do not provide these the module will create a random Super User and create the secret for you. The ARN of this secret is then available as an output to be referenced elsewhere, and can be accessed from the AWS Secrets Manager console.</p>"},{"location":"modules/perforce/modules/p4-auth/index.html#enabling-system-for-cross-domain-identity-management-scim","title":"Enabling System for Cross-domain Identity Management (SCIM)","text":"<p>P4Auth supports System for Cross-domain Identity Management (SCIM) for provisioning users and groups from an identity management system.</p> <p>To enable SCIM in the Terraform module, you need to:</p> <ol> <li>Set up a secret containing your SCIM Bearer Token in AWS Secrets Manager.</li> <li>Provide the appropriate <code>scim_bearer_token_arn</code>, <code>p4d_super_user_arn</code>, <code>p4d_super_user_password_arn</code> and <code>p4d_port</code> variables to the module.</li> <li>Set up connectivity between P4 Server and P4Auth. The parent module does this for you.</li> </ol> <p>Once this is set up, you can verify that SCIM works by making the following call to create a user:</p> <pre><code>curl -X POST -H 'Authorization: Bearer &lt;base64-encoded bearer token&gt;' \\\n  -H \"Content-Type: application/scim+json\" \\\n  -d '{\n    \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"],\n    \"userName\": \"example1\",\n    \"externalId\": \"example1\",\n    \"name\": {\n      \"formatted\": \"Example 1\",\n      \"familyName\": \"Example\",\n      \"givenName\": \"One\"\n    }\n  }' \\ -v -v -v https://&lt;p4auth domain name&gt;/scim/v2/Users\n</code></pre>"},{"location":"modules/perforce/modules/p4-auth/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.97.0 awscc 1.34.0 random 3.7.1"},{"location":"modules/perforce/modules/p4-auth/index.html#providers","title":"Providers","text":"Name Version aws 5.97.0 awscc 1.34.0 random 3.7.1"},{"location":"modules/perforce/modules/p4-auth/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/modules/p4-auth/index.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.log_group resource aws_ecs_cluster.cluster resource aws_ecs_cluster_capacity_providers.cluster_fargate_providers resource aws_ecs_service.service resource aws_ecs_task_definition.task_definition resource aws_iam_policy.default_policy resource aws_iam_policy.scim_secrets_manager_policy resource aws_iam_policy.secrets_manager_policy resource aws_iam_role.default_role resource aws_iam_role.task_execution_role resource aws_iam_role_policy_attachment.default_role resource aws_iam_role_policy_attachment.task_execution_role_ecs resource aws_iam_role_policy_attachment.task_execution_role_scim_secrets_manager resource aws_iam_role_policy_attachment.task_execution_role_secrets_manager resource aws_lb.alb resource aws_lb_listener.alb_https_listener resource aws_lb_target_group.alb_target_group resource aws_s3_bucket.alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_security_group.alb resource aws_security_group.ecs_service resource aws_vpc_security_group_egress_rule.alb_outbound_to_ecs_service resource aws_vpc_security_group_egress_rule.ecs_service_outbound_to_internet_ipv4 resource aws_vpc_security_group_egress_rule.ecs_service_outbound_to_internet_ipv6 resource aws_vpc_security_group_ingress_rule.ecs_service_inbound_from_alb resource awscc_secretsmanager_secret.admin_password resource awscc_secretsmanager_secret.admin_username resource random_string.alb_access_logs_bucket_suffix resource random_string.p4_auth resource aws_ecs_cluster.cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.default_policy data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.helix_authentication_service_scim_secrets_manager_policy data source aws_iam_policy_document.secrets_manager_policy data source aws_region.current data source"},{"location":"modules/perforce/modules/p4-auth/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required admin_password_secret_arn Optionally provide the ARN of an AWS Secret for the P4Auth Administrator password. <code>string</code> <code>null</code> no admin_username_secret_arn Optionally provide the ARN of an AWS Secret for the P4Auth Administrator username. <code>string</code> <code>null</code> no alb_access_logs_bucket ID of the S3 bucket for P4Auth ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no alb_access_logs_prefix Log prefix for P4Auth ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no alb_subnets A list of subnets to deploy the load balancer into. Public subnets are recommended. <code>list(string)</code> <code>[]</code> no application_load_balancer_name The name of the P4Auth ALB. Defaults to the project prefix and module name. <code>string</code> <code>null</code> no certificate_arn The TLS certificate ARN for the P4Auth load balancer. <code>string</code> <code>null</code> no cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for P4Auth. <code>string</code> <code>365</code> no cluster_name The name of the ECS cluster to deploy the P4Auth into. Cluster is not created if this variable is null. <code>string</code> <code>null</code> no container_cpu The CPU allotment for the P4Auth container. <code>number</code> <code>1024</code> no container_memory The memory allotment for the P4Auth container. <code>number</code> <code>4096</code> no container_name The name of the P4Auth container. <code>string</code> <code>\"p4-auth-container\"</code> no container_port The container port that P4Auth runs on. <code>number</code> <code>3000</code> no create_application_load_balancer This flag controls the creation of an application load balancer as part of the module. <code>bool</code> <code>true</code> no create_default_role Optional creation of P4Auth default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_role ARN of the custom IAM Role you wish to use with P4Auth. <code>string</code> <code>null</code> no debug Set this flag to enable execute command on service containers and force redeploys. <code>bool</code> <code>false</code> no deregistration_delay The amount of time to wait for in-flight requests to complete while deregistering a target. The range is 0-3600 seconds. <code>number</code> <code>30</code> no enable_alb_access_logs Enables access logging for the P4Auth ALB. Defaults to false. <code>bool</code> <code>false</code> no enable_alb_deletion_protection Enables deletion protection for the P4Auth ALB. Defaults to true. <code>bool</code> <code>false</code> no enable_web_based_administration Flag for enabling web based administration of P4Auth. <code>bool</code> <code>false</code> no existing_security_groups A list of existing security group IDs to attach to the P4Auth load balancer. <code>list(string)</code> <code>[]</code> no fully_qualified_domain_name The fully qualified domain name where P4Auth will be available. <code>string</code> <code>null</code> no internal Set this flag to true if you do not want the P4Auth load balancer to have a public IP. <code>bool</code> <code>false</code> no name The name attached to P4Auth module resources. <code>string</code> <code>\"p4-auth\"</code> no p4d_port The P4D_PORT environment variable where Helix Authentication Service should look for Helix Core. Required if you want to use SCIM to provision users and groups. Defaults to 'ssl:perforce:1666' <code>string</code> <code>\"ssl:perforce:1666\"</code> no p4d_super_user_arn If you would like to use SCIM to provision users and groups, you need to set this variable to the ARN of an AWS Secrets Manager secret containing the super user username for p4d. <code>string</code> <code>null</code> no p4d_super_user_password_arn If you would like to use SCIM to provision users and groups, you need to set this variable to the ARN of an AWS Secrets Manager secret containing the super user password for p4d. <code>string</code> <code>null</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no s3_enable_force_destroy Enables force destroy for the S3 bucket for P4Auth access log storage. Defaults to true. <code>bool</code> <code>true</code> no scim_bearer_token_arn If you would like to use SCIM to provision users and groups, you need to set this variable to the ARN of an AWS Secrets Manager secret containing the bearer token. <code>string</code> <code>null</code> no subnets A list of subnets to deploy the P4Auth ECS Service into. Private subnets are recommended. <code>list(string)</code> n/a yes tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"p4-auth\",  \"ModuleSource\": \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/perforce/terraform-aws-perforce\",  \"RootModuleName\": \"terraform-aws-perforce\"}</pre> no vpc_id The ID of the existing VPC you would like to deploy P4Auth into. <code>string</code> n/a yes"},{"location":"modules/perforce/modules/p4-auth/index.html#outputs","title":"Outputs","text":"Name Description alb_dns_name The DNS name of the P4Auth ALB alb_security_group_id Security group associated with the P4Auth load balancer alb_zone_id The hosted zone ID of the P4Auth ALB cluster_name Name of the ECS cluster hosting P4Auth service_security_group_id Security group associated with the ECS service running P4Auth target_group_arn The service target group for P4Auth"},{"location":"modules/perforce/modules/p4-code-review/index.html","title":"P4 Code Review Submodule","text":"<p>P4 Code Review is a free code review tool for projects hosted in P4 Server. This module deploys P4 Code Review as a service on AWS Elastic Container Service using the publicly available image from Dockerhub.</p> <p>P4 Code Review also relies on a Redis cache. The module provisions a single node AWS Elasticache Redis OSS cluster and configures connectivity for the P4 Code Review service.</p> <p>This module deploys the following resources:</p> <ul> <li>An Elastic Container Service (ECS) cluster backed by AWS Fargate. This can also be created externally and passed in via the <code>cluster_name</code> variable.</li> <li>An ECS service running the latest P4 Code Review container (perforce/helix-swarm) available.</li> <li>An Application Load Balancer for TLS termination of the P4 Code Review service.</li> <li>A single node AWS Elasticache Redis OSS cluster.</li> <li>Supporting resources such as Cloudwatch log groups, IAM roles, and security groups.</li> </ul>"},{"location":"modules/perforce/modules/p4-code-review/index.html#architecture","title":"Architecture","text":""},{"location":"modules/perforce/modules/p4-code-review/index.html#prerequisites","title":"Prerequisites","text":"<p>P4 Code Review needs to be able to connect to a P4 Server. P4 Code Review leverages the same authentication mechanism as P4 Server, and needs to install required plugins on the upstream P4 Server instance during setup. This happens automatically, but P4 Code Review requires an administrative user's credentials to be able to initially connect. These credentials are provided to the module through variables specifying AWS Secrets Manager secrets, and then pulled into the P4 Code Review container during startup. See the <code>p4d_super_user_arn</code>, <code>p4d_super_user_password_arn</code>, <code>p4d_swarm_user_arn</code>, and <code>p4d_swarm_password_arn</code> variables below for more details.</p> <p>The P4 Server submodule creates an administrative user on initial deployment, and stores the credentials in AWS Secrets manager. The ARN of the credentials secret is then made available as a Terraform output from the module, and can be referenced elsewhere. The is done by default by the parent Perforce module.</p> <p>Should you need to manually create the administrative user secret the following AWS CLI command may prove useful:</p> <pre><code>aws secretsmanager create-secret \\\n    --name P4CodeReviewSuperUser \\\n    --description \"P4 Code Review Super User\" \\\n    --secret-string \"{\\\"username\\\":\\\"swarm\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\"\n</code></pre> <p>You can then provide these credentials as variables when you define the P4 Code Review module in your Terraform configurations (the parent Perforce module does this for you):</p> <pre><code>module \"p4_code_review\" {\n    source = \"modules/perforce/modules/p4-code-review\"\n    ...\n    p4d_super_user_arn = \"arn:aws:secretsmanager:&lt;your-aws-region&gt;:&lt;your-aws-account-id&gt;:secret:P4CodeReviewSuperUser-a1b2c3:username::\"\n    p4d_super_user_password_arn = \"arn:aws:secretsmanager:&lt;your-aws-region&gt;:&lt;your-aws-account-id&gt;:secret:P4CodeReviewSuperUser-a1b2c3:password::\"\n}\n</code></pre>"},{"location":"modules/perforce/modules/p4-code-review/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.97.0 random 3.7.1"},{"location":"modules/perforce/modules/p4-code-review/index.html#providers","title":"Providers","text":"Name Version aws 5.97.0 random 3.7.1"},{"location":"modules/perforce/modules/p4-code-review/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/modules/p4-code-review/index.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.log_group resource aws_cloudwatch_log_group.redis_service_log_group resource aws_ecs_cluster.cluster resource aws_ecs_cluster_capacity_providers.cluster_fargate_providers resource aws_ecs_service.service resource aws_ecs_task_definition.task_definition resource aws_elasticache_cluster.cluster resource aws_elasticache_subnet_group.subnet_group resource aws_iam_policy.default_policy resource aws_iam_policy.secrets_manager_policy resource aws_iam_role.default_role resource aws_iam_role.task_execution_role resource aws_iam_role_policy_attachment.default_role resource aws_iam_role_policy_attachment.p4_auth_task_execution_role_ecs resource aws_iam_role_policy_attachment.p4_auth_task_execution_role_secrets_manager resource aws_lb.alb resource aws_lb_listener.alb_https_listener resource aws_lb_target_group.alb_target_group resource aws_s3_bucket.alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_security_group.alb resource aws_security_group.ecs_service resource aws_security_group.elasticache resource aws_vpc_security_group_egress_rule.alb_outbound_to_ecs_service resource aws_vpc_security_group_egress_rule.ecs_service_outbound_to_internet_ipv4 resource aws_vpc_security_group_egress_rule.ecs_service_outbound_to_internet_ipv6 resource aws_vpc_security_group_ingress_rule.ecs_service_inbound_alb resource aws_vpc_security_group_ingress_rule.elasticache_inbound_from_ecs_service resource random_string.alb_access_logs_bucket_suffix resource random_string.p4_code_review resource aws_ecs_cluster.cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.default_policy data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.secrets_manager_policy data source aws_region.current data source"},{"location":"modules/perforce/modules/p4-code-review/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required alb_access_logs_bucket ID of the S3 bucket for P4 Code Review ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no alb_access_logs_prefix Log prefix for P4 Code Review ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no alb_subnets A list of subnets to deploy the load balancer into. Public subnets are recommended. <code>list(string)</code> <code>[]</code> no application_load_balancer_name The name of the P4 Code Review ALB. Defaults to the project prefix and module name. <code>string</code> <code>null</code> no certificate_arn The TLS certificate ARN for the P4 Code Review service load balancer. <code>string</code> <code>null</code> no cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for P4 Code Review. <code>string</code> <code>365</code> no cluster_name The name of the cluster to deploy the P4 Code Review service into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no container_cpu The CPU allotment for the P4 Code Review container. <code>number</code> <code>1024</code> no container_memory The memory allotment for the P4 Code Review container. <code>number</code> <code>2048</code> no container_name The name of the P4 Code Review container. <code>string</code> <code>\"p4-code-review-container\"</code> no container_port The container port that P4 Code Review runs on. <code>number</code> <code>80</code> no create_application_load_balancer This flag controls the creation of an application load balancer as part of the module. <code>bool</code> <code>true</code> no create_default_role Optional creation of P4 Code Review Default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_role ARN of the custom IAM Role you wish to use with P4 Code Review. <code>string</code> <code>null</code> no debug Debug flag to enable execute command on service for container access. <code>bool</code> <code>false</code> no deregistration_delay The amount of time to wait for in-flight requests to complete while deregistering a target. The range is 0-3600 seconds. <code>number</code> <code>30</code> no elasticache_node_count Number of cache nodes to provision in the Elasticache cluster. <code>number</code> <code>1</code> no elasticache_node_type The type of nodes provisioned in the Elasticache cluster. <code>string</code> <code>\"cache.t4g.micro\"</code> no enable_alb_access_logs Enables access logging for the P4 Code Review ALB. Defaults to false. <code>bool</code> <code>false</code> no enable_alb_deletion_protection Enables deletion protection for the P4 Code Review ALB. Defaults to true. <code>bool</code> <code>false</code> no enable_sso Set this to true if using SSO for P4 Code Review authentication. <code>bool</code> <code>false</code> no existing_redis_connection The connection specifications to use for an existing Redis deployment. <pre>object({    host = string    port = number  })</pre> <code>null</code> no existing_security_groups A list of existing security group IDs to attach to the P4 Code Review load balancer. <code>list(string)</code> <code>[]</code> no fully_qualified_domain_name The fully qualified domain name that P4 Code Review should use for internal URLs. <code>string</code> <code>null</code> no internal Set this flag to true if you do not want the P4 Code Review service load balancer to have a public IP. <code>bool</code> <code>false</code> no name The name attached to P4 Code Review module resources. <code>string</code> <code>\"p4-code-review\"</code> no p4_code_review_user_password_secret_arn Optionally provide the ARN of an AWS Secret for the p4d P4 Code Review password. <code>string</code> n/a yes p4_code_review_user_username_secret_arn Optionally provide the ARN of an AWS Secret for the p4d P4 Code Review username. <code>string</code> n/a yes p4d_port The P4D_PORT environment variable where P4 Code Review should look for P4 Code Review. Defaults to 'ssl:perforce:1666' <code>string</code> <code>\"ssl:perforce:1666\"</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no s3_enable_force_destroy Enables force destroy for the S3 bucket for P4 Code Review access log storage. Defaults to true. <code>bool</code> <code>true</code> no subnets A list of subnets to deploy the P4 Code Review ECS Service into. Private subnets are recommended. <code>list(string)</code> n/a yes super_user_password_secret_arn Optionally provide the ARN of an AWS Secret for the p4d super user password. <code>string</code> n/a yes super_user_username_secret_arn Optionally provide the ARN of an AWS Secret for the p4d super user username. <code>string</code> n/a yes tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"p4-code-review\",  \"ModuleSource\": \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/perforce/terraform-aws-perforce\",  \"RootModuleName\": \"terraform-aws-perforce\"}</pre> no vpc_id The ID of the existing VPC you would like to deploy P4 Code Review into. <code>string</code> n/a yes"},{"location":"modules/perforce/modules/p4-code-review/index.html#outputs","title":"Outputs","text":"Name Description alb_dns_name The DNS name of the P4 Code Review ALB alb_security_group_id Security group associated with the P4 Code Review load balancer alb_zone_id The hosted zone ID of the P4 Code Review ALB cluster_name Name of the ECS cluster hosting P4 Code Review service_security_group_id Security group associated with the ECS service running P4 Code Review target_group_arn The service target group for P4 Code Review"},{"location":"modules/perforce/modules/p4-server/index.html","title":"P4 Server Submodule","text":"<p>P4 Server is a scalable version control system that helps teams manage code alongside large, digital assets and collaborate more effectively in one central, secure location. With AWS, teams can quickly deploy Helix Core and accelerate innovation.</p> <p>This module provisions P4 Server on an EC2 Instance with three dedicated EBS volumes for depots, metadata, and logs. It can also be configured to automatically install the required plugins to integrate with P4Auth. This is the default option if using the parent module. This allows end users to quickly set up single-sign-on for their Perforce Helix Core server.</p>"},{"location":"modules/perforce/modules/p4-server/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/perforce/modules/p4-server/index.html#prerequisites","title":"Prerequisites","text":"<p>This module deploys P4 Server on AWS using an Amazon Machine Image (AMI) that is included in the Cloud Game Development Toolkit. You must provision this AMI using Hashicorp Packer prior to deploying this module. To get started consult the documentation for the P4 Server AMI.</p>"},{"location":"modules/perforce/modules/p4-server/index.html#optional","title":"Optional","text":"<p>You can optionally define the Helix Core super user's credentials prior to deployment. To do so, create a secret for the Helix Core super user's username and password:</p> <pre><code>aws secretsmanager create-secret \\\n    --name HelixCoreSuperUser \\\n    --description \"Helix Core Super User\" \\\n    --secret-string \"{\\\"username\\\":\\\"admin\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\"\n</code></pre> <p>You can then provide the relevant ARN as variables when you define the Helix Core module in your Terraform configurations:</p> <pre><code>module \"perforce_helix_core\" {\n    source = \"modules/perforce/helix-core\"\n    ...\n    helix_core_super_user_username_arn = \"arn:aws:secretsmanager:us-west-2:123456789012:secret:HelixCoreSuperUser-a1b2c3:username::\"\n    helix_core_super_user_password_arn = \"arn:aws:secretsmanager:us-west-2:123456789012:secret:HelixCoreSuperUser-a1b2c3:password::\"\n}\n</code></pre> <p>If you do not provide these the module will create a random Super User and create the secret for you. The ARN of this secret is then available as an output to be referenced elsewhere.</p>"},{"location":"modules/perforce/modules/p4-server/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.97.0 awscc 1.34.0 local 2.5.2 netapp-ontap 2.1.0 null 3.2.4 random 3.7.1"},{"location":"modules/perforce/modules/p4-server/index.html#providers","title":"Providers","text":"Name Version aws 5.89.0 awscc 1.34.0 netapp-ontap 2.1.0 random 3.7.1"},{"location":"modules/perforce/modules/p4-server/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/modules/p4-server/index.html#resources","title":"Resources","text":"Name Type aws_ebs_volume.depot resource aws_ebs_volume.logs resource aws_ebs_volume.metadata resource aws_eip.server_eip resource aws_fsx_ontap_volume.depot resource aws_fsx_ontap_volume.logs resource aws_fsx_ontap_volume.metadata resource aws_iam_instance_profile.instance_profile resource aws_iam_policy.default_policy resource aws_iam_role.default_role resource aws_iam_role.lambda_role resource aws_iam_role_policy_attachment.default_role_default_policy resource aws_iam_role_policy_attachment.default_role_ssm_managed_instance_core resource aws_iam_role_policy_attachment.lambda_service_basic_execution_role resource aws_iam_role_policy_attachment.lambda_service_role resource aws_iam_role_policy_attachment.lambda_vpc_access_role resource aws_instance.server_instance resource aws_lambda_function.lambda_function resource aws_security_group.default_security_group resource aws_security_group.fsxn_lambda_link_security_group resource aws_volume_attachment.depot_attachment resource aws_volume_attachment.logs_attachment resource aws_volume_attachment.metadata_attachment resource aws_vpc_security_group_egress_rule.link_outbound_fsxn resource aws_vpc_security_group_egress_rule.server_internet resource aws_vpc_security_group_ingress_rule.fsxn_inbound_link resource awscc_secretsmanager_secret.super_user_password resource awscc_secretsmanager_secret.super_user_username resource netapp-ontap_lun.depots_volume_lun resource netapp-ontap_lun.logs_volume_lun resource netapp-ontap_lun.metadata_volume_lun resource netapp-ontap_san_igroup.perforce_igroup resource netapp-ontap_san_lun-map.depots_lun_map resource netapp-ontap_san_lun-map.logs_lun_map resource netapp-ontap_san_lun-map.metadata_lun_map resource random_string.p4_server resource aws_ami.existing_server_ami data source aws_iam_policy_document.default_policy data source aws_iam_policy_document.ec2_trust_relationship data source aws_subnet.instance_subnet data source"},{"location":"modules/perforce/modules/p4-server/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required amazon_fsxn_filesystem_id The ID of the existing FSx ONTAP file system to use if storage type is FSxN. <code>string</code> <code>null</code> no amazon_fsxn_svm_id The ID of the Storage Virtual Machine (SVM) for the FSx ONTAP filesystem. <code>string</code> <code>null</code> no auth_service_url The URL for the P4Auth Service. <code>string</code> <code>null</code> no case_sensitive Whether or not the server should be case insensitive (Server will run '-C1' mode), or if the server will run with case sensitivity default of the underlying platform. False enables '-C1' mode <code>bool</code> <code>true</code> no create_default_role Optional creation of P4 Server default IAM Role with SSM managed instance core policy attached. Default is set to true. <code>bool</code> <code>true</code> no create_default_sg Whether to create a default security group for the P4 Server instance. <code>bool</code> <code>true</code> no custom_role ARN of the custom IAM Role you wish to use with P4 Server. <code>string</code> <code>null</code> no depot_volume_size The size of the depot volume in GiB. Defaults to 128 GiB. <code>number</code> <code>128</code> no environment The environment attached to P4 Server module resources. <code>string</code> <code>\"dev\"</code> no existing_security_groups A list of existing security group IDs to attach to the P4 Server load balancer. <code>list(string)</code> <code>[]</code> no fsxn_filesystem_security_group_id The ID of the security group for the FSx ONTAP filesystem. <code>string</code> <code>null</code> no fsxn_management_ip FSxN management ip address <code>string</code> <code>null</code> no fsxn_password FSxN admin user password AWS secret manager arn <code>string</code> <code>null</code> no fsxn_region The ID of the Storage Virtual Machine (SVM) for the FSx ONTAP filesystem. <code>string</code> <code>null</code> no fsxn_svm_name FSxN storage virtual machine name <code>string</code> <code>null</code> no fully_qualified_domain_name The fully qualified domain name where P4 Server will be available. This is used to generate self-signed certificates on the P4 Server. <code>string</code> <code>null</code> no instance_architecture The architecture of the P4 Server instance. Allowed values are 'arm64' or 'x86_64'. <code>string</code> <code>\"x86_64\"</code> no instance_subnet_id The subnet where the P4 Server instance will be deployed. <code>string</code> n/a yes instance_type The instance type for Perforce P4 Server. Defaults to c6g.large. <code>string</code> <code>\"c6i.large\"</code> no internal Set this flag to true if you do not want the P4 Server instance to have a public IP. <code>bool</code> <code>false</code> no logs_volume_size The size of the logs volume in GiB. Defaults to 32 GiB. <code>number</code> <code>32</code> no metadata_volume_size The size of the metadata volume in GiB. Defaults to 32 GiB. <code>number</code> <code>32</code> no name The name attached to P4 Server module resources. <code>string</code> <code>\"p4-server\"</code> no p4_server_type The Perforce P4 Server type. <code>string</code> n/a yes plaintext Whether to enable plaintext authentication for P4 Server. This is not recommended for production environments unless you are using a load balancer for TLS termination. <code>bool</code> <code>false</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no protocol Specify the protocol (NFS or ISCSI) <code>string</code> <code>null</code> no selinux Whether to apply SELinux label updates for P4 Server. Don't enable this if SELinux is disabled on your target operating system. <code>bool</code> <code>false</code> no storage_type The type of backing store [EBS, FSxN] <code>string</code> n/a yes super_user_password_secret_arn If you would like to manage your own super user credentials through AWS Secrets Manager provide the ARN for the super user's username here. Otherwise, the default of 'perforce' will be used. <code>string</code> <code>null</code> no super_user_username_secret_arn If you would like to manage your own super user credentials through AWS Secrets Manager provide the ARN for the super user's password here. <code>string</code> <code>null</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"p4-server\",  \"ModuleSource\": \"https://github.com/aws-games/cloud-game-development-toolkit/tree/main/modules/perforce/terraform-aws-perforce\",  \"RootModuleName\": \"terraform-aws-perforce\"}</pre> no unicode Whether to enable Unicode configuration for P4 Server the -xi flag for p4d. Set to true to enable Unicode support. <code>bool</code> <code>false</code> no vpc_id The VPC where P4 Server should be deployed <code>string</code> n/a yes"},{"location":"modules/perforce/modules/p4-server/index.html#outputs","title":"Outputs","text":"Name Description eip_id The ID of the Elastic IP associated with your P4 Server instance. eip_public_ip The public IP of your P4 Server instance. instance_id Instance ID for the P4 Server instance lambda_link_name Lambda function name for the FSxN Link private_ip Private IP for the P4 Server instance security_group_id The default security group of your P4 Server instance. super_user_password_secret_arn The ARN of the AWS Secrets Manager secret holding your P4 Server super user's username. super_user_username_secret_arn The ARN of the AWS Secrets Manager secret holding your P4 Server super user's password."},{"location":"modules/teamcity/index.html","title":"TeamCity Module","text":"<p>TeamCity is a user-friendly continuous integration (CI) server for developers and build engineers created by JetBrains. This module deploys a TeamCity server on AWS Elastic Container Service.</p> <p>The TeamCity server relies on shared file system for persistent storage of configuration, build results, and current operation files as well as a SQL database to store build history, user data, build results, and runtime data. This module provides these services by provisioning an Amazon Elastic File System and an Amazon Amazon Aurora Serverless V2 cluster running the PostgreSQL engine.</p>"},{"location":"modules/teamcity/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/teamcity/index.html#examples","title":"Examples","text":"<p>For example configurations, please see the examples.</p>"},{"location":"modules/teamcity/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.9 aws 5.89.0 random 3.5.1"},{"location":"modules/teamcity/index.html#providers","title":"Providers","text":"Name Version aws 5.89.0 random 3.5.1"},{"location":"modules/teamcity/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/teamcity/index.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.teamcity_log_group resource aws_db_subnet_group.teamcity_db_subnet_group resource aws_ecs_cluster.teamcity_cluster resource aws_ecs_service.teamcity resource aws_ecs_task_definition.teamcity_task_definition resource aws_efs_access_point.teamcity_efs_data_access_point resource aws_efs_file_system.teamcity_efs_file_system resource aws_efs_mount_target.teamcity_efs_mount_target resource aws_iam_policy.teamcity_default_policy resource aws_iam_role.teamcity_default_role resource aws_iam_role.teamcity_task_execution_role resource aws_lb.teamcity_external_lb resource aws_lb_listener.teamcity_listener resource aws_lb_target_group.teamcity_target_group resource aws_rds_cluster.teamcity_db_cluster resource aws_rds_cluster_instance.teamcity_db_cluster_instance resource aws_s3_bucket.teamcity_alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_security_group.teamcity_alb_sg resource aws_security_group.teamcity_db_sg resource aws_security_group.teamcity_efs_sg resource aws_security_group.teamcity_service_sg resource aws_vpc_security_group_egress_rule.alb_outbound_service resource aws_vpc_security_group_egress_rule.service_outbound_internet resource aws_vpc_security_group_ingress_rule.service_db resource aws_vpc_security_group_ingress_rule.service_efs resource aws_vpc_security_group_ingress_rule.service_inbound_alb resource random_string.teamcity_alb_access_logs_bucket_suffix resource aws_ecs_cluster.teamcity_cluster data source aws_efs_file_system.efs_file_system data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.teamcity_default_policy data source aws_region.current data source"},{"location":"modules/teamcity/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required alb_certificate_arn The ARN of the SSL certificate to use for the ALB <code>string</code> <code>null</code> no alb_subnets The subnets in which the ALB will be deployed <code>list(string)</code> <code>[]</code> no aurora_instance_count Number of instances to provision for the Aurora cluster <code>number</code> <code>2</code> no aurora_skip_final_snapshot Flag for whether a final snapshot should be created when the cluster is destroyed. <code>bool</code> <code>true</code> no cluster_name The name of the ECS cluster to deploy TeamCity to. <code>string</code> <code>null</code> no container_cpu The number of CPU units to allocate to the TeamCity server container <code>number</code> <code>1024</code> no container_memory The number of MB of memory to allocate to the TeamCity server container <code>number</code> <code>4096</code> no container_name The name of the TeamCity server container <code>string</code> <code>\"teamcity\"</code> no container_port The port on which the TeamCity server container listens <code>number</code> <code>8111</code> no create_external_alb Set this flag to true to create an external load balancer for TeamCity. <code>bool</code> <code>true</code> no database_connection_string The database connection string for TeamCity <code>string</code> <code>null</code> no database_master_password The master password for the database <code>string</code> <code>null</code> no database_master_username The master username for the database <code>string</code> <code>null</code> no debug Set this flag to enable ECS execute permissions on the TeamCity server container and force new service deployments on Terraform apply. <code>bool</code> <code>false</code> no desired_container_count The desired number of containers running TeamCity server. <code>number</code> <code>1</code> no efs_access_point_id The ID of the EFS access point to use for the TeamCity data volume. <code>string</code> <code>null</code> no efs_encryption_enabled Set this flag to true to enable EFS encryption. <code>bool</code> <code>true</code> no efs_id The ID of the EFS file system to use for the TeamCity service. <code>string</code> <code>null</code> no enable_teamcity_alb_access_logs Enables access logging for the TeamCity ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_teamcity_alb_deletion_protection Enables deletion protection for the TeamCity ALB. Defaults to true. <code>bool</code> <code>false</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no name The name applied to resources in the TeamCity module <code>string</code> <code>\"teamcity\"</code> no service_subnets The subnets in which the TeamCity server service will be deployed <code>list(string)</code> n/a yes tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"iac-management\": \"CGD-Toolkit\",  \"iac-module\": \"TeamCity\",  \"iac-provider\": \"Terraform\"}</pre> no teamcity_alb_access_logs_bucket ID of the S3 bucket for TeamCity ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no teamcity_alb_access_logs_prefix Log prefix for TeamCity ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no teamcity_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for TeamCity. <code>string</code> <code>365</code> no teamcity_efs_performance_mode The performance mode of the EFS file system used by the TeamCity service. Defaults to general purpose. <code>string</code> <code>\"generalPurpose\"</code> no teamcity_efs_throughput_mode The throughput mode of the EFS file system used by the TeamCity service. Defaults to bursting. <code>string</code> <code>\"bursting\"</code> no vpc_id The ID of the VPC in which the service will be deployed <code>string</code> n/a yes"},{"location":"modules/teamcity/index.html#outputs","title":"Outputs","text":"Name Description external_alb_dns_name DNS endpoint of Application Load Balancer (ALB) external_alb_zone_id Zone ID for internet facing load balancer"},{"location":"modules/unreal/horde/index.html","title":"Unreal Horde","text":"<p>Unreal Engine Horde is a set of services supporting workflows Epic uses to develop Fortnite, Unreal Engine, and other titles. This module deploys the Unreal Engine Horde server on AWS Elastic Container Service using the image available from the Epic Games Github organization.. Unreal Engine Horde relies on a Redis cache and a MongoDB compatible database. This module provides these services by provisioning an Amazon Elasticache with Redis OSS Compatibility cluster and an Amazon DocumentDB cluster.</p> <p>Check out this video from Unreal Fest 2024 to learn more about the Unreal Horde module:</p> <p></p>"},{"location":"modules/unreal/horde/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/unreal/horde/index.html#prerequisites","title":"Prerequisites","text":"<p>Unreal Engine Horde is only available through the Epic Games Github organization's package registry or the Unreal Engine source code. In order to get access to this software you will need to join the Epic Games organization on Github and accept the Unreal Engine EULA.</p>"},{"location":"modules/unreal/horde/index.html#examples","title":"Examples","text":"<p>For example configurations, please see the examples.</p>"},{"location":"modules/unreal/horde/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.69.0 random 3.6.3"},{"location":"modules/unreal/horde/index.html#providers","title":"Providers","text":"Name Version aws 5.69.0 random 3.6.3"},{"location":"modules/unreal/horde/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/unreal/horde/index.html#resources","title":"Resources","text":"Name Type aws_autoscaling_group.unreal_horde_agent_asg resource aws_cloudwatch_log_group.unreal_horde_log_group resource aws_docdb_cluster.horde resource aws_docdb_cluster_instance.horde resource aws_docdb_cluster_parameter_group.horde resource aws_docdb_subnet_group.horde resource aws_ecs_cluster.unreal_horde_cluster resource aws_ecs_service.unreal_horde resource aws_ecs_task_definition.unreal_horde_task_definition resource aws_elasticache_cluster.horde resource aws_elasticache_subnet_group.horde resource aws_iam_instance_profile.unreal_horde_agent_instance_profile resource aws_iam_policy.horde_agents_s3_policy resource aws_iam_policy.unreal_horde_default_policy resource aws_iam_policy.unreal_horde_secrets_manager_policy resource aws_iam_role.unreal_horde_agent_default_role resource aws_iam_role.unreal_horde_default_role resource aws_iam_role.unreal_horde_task_execution_role resource aws_launch_template.unreal_horde_agent_template resource aws_lb.unreal_horde_external_alb resource aws_lb.unreal_horde_internal_alb resource aws_lb_listener.unreal_horde_external_alb_https_listener resource aws_lb_listener.unreal_horde_internal_alb_https_listener resource aws_lb_listener_rule.unreal_horde_external_alb_grpc_rule resource aws_lb_listener_rule.unreal_horde_internal_alb_grpc_rule resource aws_lb_target_group.unreal_horde_api_target_group_external resource aws_lb_target_group.unreal_horde_api_target_group_internal resource aws_lb_target_group.unreal_horde_grpc_target_group_external resource aws_lb_target_group.unreal_horde_grpc_target_group_internal resource aws_s3_bucket.ansible_playbooks resource aws_s3_bucket.unreal_horde_alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_s3_bucket_public_access_block.ansible_playbooks_bucket_public_block resource aws_s3_bucket_versioning.ansible_playbooks_versioning resource aws_s3_object.unreal_horde_agent_playbook resource aws_s3_object.unreal_horde_agent_service resource aws_security_group.unreal_horde_agent_sg resource aws_security_group.unreal_horde_docdb_sg resource aws_security_group.unreal_horde_elasticache_sg resource aws_security_group.unreal_horde_external_alb_sg resource aws_security_group.unreal_horde_internal_alb_sg resource aws_security_group.unreal_horde_sg resource aws_ssm_association.configure_unreal_horde_agent resource aws_ssm_document.ansible_run_document resource aws_vpc_security_group_egress_rule.unreal_horde_agents_outbound_ipv4 resource aws_vpc_security_group_egress_rule.unreal_horde_agents_outbound_ipv6 resource aws_vpc_security_group_egress_rule.unreal_horde_external_alb_outbound_service_api resource aws_vpc_security_group_egress_rule.unreal_horde_external_alb_outbound_service_grpc resource aws_vpc_security_group_egress_rule.unreal_horde_internal_alb_outbound_service_api resource aws_vpc_security_group_egress_rule.unreal_horde_internal_alb_outbound_service_grpc resource aws_vpc_security_group_egress_rule.unreal_horde_outbound_ipv4 resource aws_vpc_security_group_egress_rule.unreal_horde_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.unreal_horde_agents_inbound_agents resource aws_vpc_security_group_ingress_rule.unreal_horde_docdb_ingress resource aws_vpc_security_group_ingress_rule.unreal_horde_elasticache_ingress resource aws_vpc_security_group_ingress_rule.unreal_horde_inbound_external_alb_api resource aws_vpc_security_group_ingress_rule.unreal_horde_inbound_external_alb_grpc resource aws_vpc_security_group_ingress_rule.unreal_horde_inbound_internal_alb_api resource aws_vpc_security_group_ingress_rule.unreal_horde_inbound_internal_alb_grpc resource aws_vpc_security_group_ingress_rule.unreal_horde_service_inbound_agents resource random_string.unreal_horde resource random_string.unreal_horde_alb_access_logs_bucket_suffix resource random_string.unreal_horde_ansible_playbooks_bucket_suffix resource aws_ecs_cluster.unreal_horde_cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.ec2_trust_relationship data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.horde_agents_s3_policy data source aws_iam_policy_document.unreal_horde_default_policy data source aws_iam_policy_document.unreal_horde_secrets_manager_policy data source aws_region.current data source"},{"location":"modules/unreal/horde/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required admin_claim_type The claim type for administrators. <code>string</code> <code>null</code> no admin_claim_value The claim value for administrators. <code>string</code> <code>null</code> no agents Configures autoscaling groups to be used as build agents by Unreal Engine Horde. <pre>map(object({    ami           = string    instance_type = string    block_device_mappings = list(      object({        device_name = string        ebs = object({          volume_size = number        })      })    )    min_size = optional(number, 0)    max_size = optional(number, 1)  }))</pre> <code>{}</code> no auth_method The authentication method for the Horde server. <code>string</code> <code>\"Anonymous\"</code> no certificate_arn The TLS certificate ARN for the Unreal Horde load balancer. <code>string</code> n/a yes cluster_name The name of the cluster to deploy the Unreal Horde into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no container_api_port The container port for the Unreal Horde web server. <code>number</code> <code>5000</code> no container_cpu The CPU allotment for the Unreal Horde container. <code>number</code> <code>1024</code> no container_grpc_port The container port for the Unreal Horde GRPC channel. <code>number</code> <code>5002</code> no container_memory The memory allotment for the Unreal Horde container. <code>number</code> <code>4096</code> no container_name The name of the Unreal Horde container. <code>string</code> <code>\"unreal-horde-container\"</code> no create_external_alb Set this flag to true to create an external load balancer for Unreal Horde. <code>bool</code> <code>true</code> no create_internal_alb Set this flag to true to create an internal load balancer for Unreal Horde. <code>bool</code> <code>true</code> no create_unreal_horde_default_policy Optional creation of Unreal Horde default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_unreal_horde_default_role Optional creation of Unreal Horde default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_unreal_horde_role ARN of the custom IAM Role you wish to use with Unreal Horde. <code>string</code> <code>null</code> no database_connection_string The database connection string that Horde should use. <code>string</code> <code>null</code> no debug Set this flag to enable ECS execute permissions on the Unreal Horde container and force new service deployments on Terraform apply. <code>bool</code> <code>false</code> no desired_container_count The desired number of containers running Unreal Horde. <code>number</code> <code>1</code> no docdb_backup_retention_period Number of days to retain backups for DocumentDB cluster. <code>number</code> <code>7</code> no docdb_instance_class The instance class for the Horde DocumentDB cluster. <code>string</code> <code>\"db.t4g.medium\"</code> no docdb_instance_count The number of instances to provision for the Horde DocumentDB cluster. <code>number</code> <code>2</code> no docdb_master_password Master password created for DocumentDB cluster. <code>string</code> <code>\"mustbeeightchars\"</code> no docdb_master_username Master username created for DocumentDB cluster. <code>string</code> <code>\"horde\"</code> no docdb_preferred_backup_window The preferred window for DocumentDB backups to be created. <code>string</code> <code>\"07:00-09:00\"</code> no docdb_skip_final_snapshot Flag for whether a final snapshot should be created when the cluster is destroyed. <code>bool</code> <code>true</code> no docdb_storage_encrypted Configure DocumentDB storage at rest. <code>bool</code> <code>true</code> no elasticache_node_count Number of cache nodes to provision in the Elasticache cluster. <code>number</code> <code>1</code> no elasticache_node_type The type of nodes provisioned in the Elasticache cluster. <code>string</code> <code>\"cache.t4g.micro\"</code> no elasticache_snapshot_retention_limit The number of Elasticache snapshots to retain. <code>number</code> <code>5</code> no enable_new_agents_by_default Set this flag to automatically enable new agents that enroll with the Horde Server. <code>bool</code> <code>false</code> no enable_unreal_horde_alb_access_logs Enables access logging for the Unreal Horde ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_unreal_horde_alb_deletion_protection Enables deletion protection for the Unreal Horde ALB. Defaults to true. <code>bool</code> <code>false</code> no environment The current environment (e.g. Development, Staging, Production, etc.). This will tag ressources and set ASPNETCORE_ENVIRONMENT variable. <code>string</code> <code>\"Development\"</code> no existing_security_groups A list of existing security group IDs to attach to the Unreal Horde load balancer. <code>list(string)</code> <code>[]</code> no fully_qualified_domain_name The fully qualified domain name where your Unreal Engine Horde server will be available. This agents will use this to enroll. <code>string</code> n/a yes github_credentials_secret_arn A secret containing the Github username and password with permissions to the EpicGames organization. <code>string</code> n/a yes name The name attached to Unreal Engine Horde module resources. <code>string</code> <code>\"unreal-horde\"</code> no oidc_audience The audience used for validating externally issued tokens. <code>string</code> <code>null</code> no oidc_authority The authority for the OIDC authentication provider used. <code>string</code> <code>null</code> no oidc_client_id The client ID used for authenticating with the OIDC provider. <code>string</code> <code>null</code> no oidc_client_secret The client secret used for authenticating with the OIDC provider. <code>string</code> <code>null</code> no oidc_signin_redirect The sign-in redirect URL for the OIDC provider. <code>string</code> <code>null</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no redis_connection_config The redis connection configuration that Horde should use. <code>string</code> <code>null</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"iac-management\": \"CGD-Toolkit\",  \"iac-module\": \"unreal-horde\",  \"iac-provider\": \"Terraform\"}</pre> no unreal_horde_alb_access_logs_bucket ID of the S3 bucket for Unreal Horde ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no unreal_horde_alb_access_logs_prefix Log prefix for Unreal Horde ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no unreal_horde_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Unreal Horde. <code>string</code> <code>365</code> no unreal_horde_external_alb_subnets A list of subnets to deploy the Unreal Horde load balancer into. Public subnets are recommended. <code>list(string)</code> <code>[]</code> no unreal_horde_internal_alb_subnets A list of subnets to deploy the Unreal Horde internal load balancer into. Private subnets are recommended. <code>list(string)</code> <code>[]</code> no unreal_horde_service_subnets A list of subnets to deploy the Unreal Horde service into. Private subnets are recommended. <code>list(string)</code> n/a yes vpc_id The ID of the existing VPC you would like to deploy Unreal Horde into. <code>string</code> n/a yes"},{"location":"modules/unreal/horde/index.html#outputs","title":"Outputs","text":"Name Description agent_security_group_id n/a external_alb_dns_name n/a external_alb_sg_id n/a external_alb_zone_id n/a internal_alb_dns_name n/a internal_alb_sg_id n/a internal_alb_zone_id n/a service_security_group_id n/a"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html","title":"Unreal Cloud DDC Infra Module","text":"<p>Unreal Cloud Derived Data Cache (source code) is a caching system that stores additional data required to use assets, such as compiled shaders. This allows the engine to quickly retrieve this data instead of having to regenerate it, saving time and disk space for the development team. For distributed teams, a cloud-hosted DDC enables efficient collaboration by ensuring all team members have access to the same cached data regardless of their location. This module deploys the core infrastructure for Unreal Engine's Cloud Derived Data Cache (DDC) on AWS. It creates a scalable, secure, and high-performance environment that optimizes asset processing and distribution throughout your game development pipeline, reducing build times and improving team collaboration.</p> <p>The Unreal Cloud Derived Data Cache (DDC) infrastructure module implements Epic's recommended architecture using ScyllaDB, a high-performance Cassandra-compatible database. This module provisions the following AWS resources:</p> <ol> <li> <p>ScyllaDB Database Layer:</p> <ul> <li>Deployed on EC2 instances</li> <li>Supports both single-node and multi-node cluster configurations</li> <li>Optimized for high-throughput DDC operations</li> <li>Configured with AWS Systems Manager Session Manager to provide secure shell access without requiring SSH or bastion hosts</li> </ul> </li> <li> <p>ScyllaDB Monitoring Stack:</p> <ul> <li>Deployed on an EC2 instance</li> <li>Uses Prometheus for metrics collection, Alertmanager for handling alerts, and Grafana for visualization</li> <li>Creates a Application Load Balancer for accessing the Grafana UI for real-time insights into ScyllaDB node performance</li> </ul> </li> <li> <p>Amazon EKS Cluster with specialized node groups:</p> <ul> <li>System node group: Handles core Kubernetes components and system workloads</li> <li>NVME node group: Optimized for high-performance storage operations</li> <li>Worker node group: Manages regional data replication and distribution</li> <li>Configured with AWS Systems Manager Session Manager to provide secure shell access without requiring SSH or bastion hosts</li> </ul> </li> <li> <p>S3 Bucket:</p> <ul> <li>Provides durable storage for cached assets</li> <li>Enables cross-region asset availability</li> <li>Serves as a persistent backup layer</li> </ul> </li> </ol>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#prerequisites","title":"Prerequisites","text":""},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#network-infrastructure-requirements","title":"Network Infrastructure Requirements","text":"<p>At a minimum, the Cloud DDC Module requires a Virtual Private Cloud (VPC) with a specific subnet configuration. The suggested configuration includes:</p> <ul> <li>2 public subnets</li> <li>2 private subnets</li> <li>Coverage across 2 Availability Zones</li> <li>An S3 interface endpoint</li> </ul> <p>This architecture ensures high availability and secure communication patterns for your DDC infrastructure.</p> <p></p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#configuring-node-groups-and-scylladb-deployment","title":"Configuring Node Groups and ScyllaDB Deployment","text":"<p>The footprint of your Cloud DDC deployment can be configured through 2 variables:</p> <p></p> <p>EKS Node Group Configuration: <code>eks_node_group_subnets</code></p> <p>The <code>eks_node_group_subnets</code> variable defines the subnet distribution for your EKS node groups. Each specified subnet serves as a potential target for node placement, providing granular control over the geographical distribution of your EKS infrastructure. Adding more subnets to this configuration increases deployment flexibility and enables broader availability zone coverage for your workloads at the cost of increased network complexity and potential inter-AZ data transfer charges.</p> <p></p> <p>ScyllaDB Instance Distribution: <code>scylla_subnets</code></p> <p>The <code>scylla_subnets</code> variable determines the deployment topology of your ScyllaDB instances. Each specified subnet receives a dedicated ScyllaDB instance, with multiple subnet configurations automatically establishing a distributed cluster architecture. Configurations of two or more subnets enable high availability and data resilience through native ScyllaDB clustering at the cost of increased infrastructure complexity and proportionally higher operational expenses.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.5 aws &gt;= 5.38 tls &gt;= 4.0.5"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#providers","title":"Providers","text":"Name Version aws &gt;= 5.38 tls &gt;= 4.0.5"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.unreal_cluster_cloudwatch resource aws_eks_cluster.unreal_cloud_ddc_eks_cluster resource aws_eks_identity_provider_config.eks_cluster_oidc_association resource aws_eks_node_group.nvme_node_group resource aws_eks_node_group.system_node_group resource aws_eks_node_group.worker_node_group resource aws_iam_instance_profile.scylla_instance_profile resource aws_iam_openid_connect_provider.unreal_cloud_ddc_oidc_provider resource aws_iam_role.eks_cluster_role resource aws_iam_role.monitoring_node_group_role resource aws_iam_role.nvme_node_group_role resource aws_iam_role.scylla_role resource aws_iam_role.worker_node_group_role resource aws_instance.scylla_ec2_instance resource aws_launch_template.nvme_launch_template resource aws_launch_template.system_launch_template resource aws_launch_template.worker_launch_template resource aws_route53_record.scylla_records resource aws_route53_zone.scylla_zone resource aws_s3_bucket.unreal_ddc_logging_s3_bucket resource aws_s3_bucket.unreal_ddc_s3_bucket resource aws_s3_bucket_logging.unreal-log-s3-log resource aws_s3_bucket_logging.unreal-s3-log resource aws_s3_bucket_public_access_block.unreal_ddc_log_s3_acls resource aws_s3_bucket_public_access_block.unreal_ddc_s3_acls resource aws_s3_bucket_server_side_encryption_configuration.unreal-s3-bucket resource aws_s3_bucket_server_side_encryption_configuration.unreal-s3-logging-bucket resource aws_security_group.nvme_security_group resource aws_security_group.scylla_security_group resource aws_security_group.system_security_group resource aws_security_group.worker_security_group resource aws_security_group_rule.peer_cidr_blocks_ingress_sg_rules resource aws_security_group_rule.peer_cidr_blocks_scylla_egress_sg_rules resource aws_security_group_rule.scylla_to_nvme_group_egress_sg_rules resource aws_security_group_rule.scylla_to_nvme_group_ingress_sg_rules resource aws_security_group_rule.scylla_to_worker_group_egress_sg_rules resource aws_security_group_rule.scylla_to_worker_group_ingress_sg_rules resource aws_security_group_rule.self_ingress_sg_rules resource aws_security_group_rule.self_scylla_egress_sg_rules resource aws_security_group_rule.ssm_egress_sg_rules resource aws_ssm_association.scylla_config_association resource aws_ssm_document.config_scylla resource aws_ami.scylla_ami data source aws_ssm_parameter.eks_ami_latest_release data source tls_certificate.eks_tls_certificate data source"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required eks_cluster_access_cidr List of the CIDR Ranges you want to grant public access to the EKS Cluster. <code>list(string)</code> n/a yes name Unreal Cloud DDC Workload Name <code>string</code> <code>\"unreal-cloud-ddc\"</code> no nvme_managed_node_desired_size Desired number of nvme managed node group instances <code>number</code> <code>2</code> no nvme_managed_node_instance_type Nvme managed node group instance type <code>string</code> <code>\"i3en.xlarge\"</code> no nvme_managed_node_max_size Max number of nvme managed node group instances <code>number</code> <code>2</code> no peer_cidr_blocks The peered cidr blocks you want your vpc to communicate with if you have a multi region ddc. <code>list(string)</code> <code>[]</code> no private_subnets Private subnets you want scylla and the worker nodes to be installed into. <code>list(string)</code> <code>[]</code> no scylla_ami_name Name of the Scylla AMI to be used to get the AMI ID <code>string</code> <code>\"ScyllaDB 6.0.1\"</code> no scylla_architecture The chip architecture to use when finding the scylla image. Valid <code>string</code> <code>\"x86_64\"</code> no scylla_db_storage Size of gp3 ebs volumes attached to Scylla DBs <code>number</code> <code>100</code> no scylla_db_throughput Throughput of gp3 ebs volumes attached to Scylla DBs <code>number</code> <code>200</code> no scylla_dns The local private dns name that you want Scylla to be queryable on. <code>string</code> <code>null</code> no scylla_instance_type The type and size of the Scylla instance. <code>string</code> <code>\"i4i.2xlarge\"</code> no scylla_private_subnets The subnets you want Scylla to be installed into. Can repeat subnet ids to install into the same subnet/az. This will also determine how many Scylla instances are deployed. <code>list(string)</code> <code>[]</code> no system_managed_node_desired_size Desired number of monitoring managed node group instances. <code>number</code> <code>1</code> no system_managed_node_instance_type Monitoring managed node group instance type. <code>string</code> <code>\"m5.large\"</code> no system_managed_node_max_size Max number of monitoring managed node group instances. <code>number</code> <code>2</code> no vpc_id String for VPC ID <code>string</code> n/a yes worker_managed_node_desired_size Desired number of worker managed node group instances. <code>number</code> <code>1</code> no worker_managed_node_instance_type Worker managed node group instance type. <code>string</code> <code>\"c5.xlarge\"</code> no worker_managed_node_max_size Max number of worker managed node group instances. <code>number</code> <code>1</code> no"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#outputs","title":"Outputs","text":"Name Description cluster_arn n/a cluster_certificate_authority_data n/a cluster_endpoint n/a cluster_name n/a oidc_provider_arn n/a oidc_provider_identity n/a s3_bucket_id n/a"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.10.3 aws &gt;=5.89.0 random 3.5.1 tls &gt;= 4.0.6"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#providers_1","title":"Providers","text":"Name Version aws 5.99.1 random 3.5.1 tls 4.1.0"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#modules_1","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#resources_1","title":"Resources","text":"Name Type aws_cloudwatch_log_group.unreal_cluster_cloudwatch resource aws_eks_cluster.unreal_cloud_ddc_eks_cluster resource aws_eks_node_group.nvme_node_group resource aws_eks_node_group.system_node_group resource aws_eks_node_group.worker_node_group resource aws_iam_instance_profile.scylla_instance_profile resource aws_iam_instance_profile.scylla_monitoring_profile resource aws_iam_openid_connect_provider.unreal_cloud_ddc_oidc_provider resource aws_iam_role.eks_cluster_role resource aws_iam_role.nvme_node_group_role resource aws_iam_role.scylla_monitoring_role resource aws_iam_role.scylla_role resource aws_iam_role.system_node_group_role resource aws_iam_role.worker_node_group_role resource aws_iam_role_policy.scylla_monitoring_policy resource aws_iam_role_policy_attachments_exclusive.eks_cluster_policy_attachement resource aws_iam_role_policy_attachments_exclusive.nvme_policy_attachement resource aws_iam_role_policy_attachments_exclusive.scylla_policy_attachement resource aws_iam_role_policy_attachments_exclusive.system_policy_attachement resource aws_iam_role_policy_attachments_exclusive.worker_policy_attachement resource aws_instance.scylla_ec2_instance_other_nodes resource aws_instance.scylla_ec2_instance_seed resource aws_instance.scylla_monitoring resource aws_launch_template.nvme_launch_template resource aws_launch_template.system_launch_template resource aws_launch_template.worker_launch_template resource aws_lb.scylla_monitoring_alb resource aws_lb_listener.scylla_monitoring_listener resource aws_lb_target_group.scylla_monitoring_alb_target_group resource aws_lb_target_group_attachment.scylla_monitoring resource aws_s3_bucket.scylla_monitoring_lb_access_logs_bucket resource aws_s3_bucket.unreal_ddc_s3_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_s3_bucket_public_access_block.unreal_ddc_s3_acls resource aws_security_group.cluster_security_group resource aws_security_group.nvme_security_group resource aws_security_group.scylla_monitoring_lb_sg resource aws_security_group.scylla_monitoring_sg resource aws_security_group.scylla_security_group resource aws_security_group.system_security_group resource aws_security_group.worker_security_group resource aws_vpc_security_group_egress_rule.cluster_egress_sg_rule resource aws_vpc_security_group_egress_rule.nvme_egress_sg_rules resource aws_vpc_security_group_egress_rule.scylla_monitoring_lb_sg_egress_rule resource aws_vpc_security_group_egress_rule.scylla_monitoring_sg_egress_rule resource aws_vpc_security_group_egress_rule.self_scylla_egress_sg_rules resource aws_vpc_security_group_egress_rule.ssm_egress_sg_rules resource aws_vpc_security_group_egress_rule.system_egress_sg_rules resource aws_vpc_security_group_egress_rule.worker_egress_sg_rules resource aws_vpc_security_group_ingress_rule.cluster_lb_ingress_sg_rule resource aws_vpc_security_group_ingress_rule.scylla_monitoring_ingress_node_exporter resource aws_vpc_security_group_ingress_rule.scylla_monitoring_ingress_prometheus resource aws_vpc_security_group_ingress_rule.scylla_monitoring_lb_monitoring resource aws_vpc_security_group_ingress_rule.self_ingress_cluster_sg_rule resource aws_vpc_security_group_ingress_rule.self_ingress_sg_rules resource random_string.scylla_monitoring_lb_access_logs_bucket_suffix resource aws_ami.amazon_linux data source aws_ami.scylla_ami data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.scylla_monitoring_assume_role data source aws_iam_policy_document.scylla_monitoring_policy_doc data source tls_certificate.eks_tls_certificate data source"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#inputs_1","title":"Inputs","text":"Name Description Type Default Required alb_certificate_arn The ARN of the certificate to use on the ALB <code>string</code> <code>null</code> no create_application_load_balancer Whether to create an application load balancer for the Scylla monitoring dashboard. <code>bool</code> <code>true</code> no create_scylla_monitoring_stack Whether to create the Scylla monitoring stack <code>bool</code> <code>true</code> no debug Enable debug mode <code>bool</code> <code>false</code> no eks_cluster_cloudwatch_log_group_prefix Prefix to be used for the EKS cluster CloudWatch log group. <code>string</code> <code>\"/aws/eks/unreal-cloud-ddc/cluster\"</code> no eks_cluster_logging_types List of EKS cluster log types to be enabled. <code>list(string)</code> <pre>[  \"api\",  \"audit\",  \"authenticator\",  \"controllerManager\",  \"scheduler\"]</pre> no eks_cluster_private_access Allows private access of the EKS Control Plane from subnets attached to EKS Cluster <code>bool</code> <code>true</code> no eks_cluster_public_access Allows public access of EKS Control Plane should be used with <code>bool</code> <code>false</code> no eks_cluster_public_endpoint_access_cidr List of the CIDR Ranges you want to grant public access to the EKS Cluster's public endpoint. <code>list(string)</code> <code>[]</code> no eks_node_group_subnets A list of subnets ids you want the EKS nodes to be installed into. Private subnets are strongly recommended. <code>list(string)</code> <code>[]</code> no enable_scylla_monitoring_lb_access_logs Whether to enable access logs for the Scylla monitoring load balancer. <code>bool</code> <code>false</code> no enable_scylla_monitoring_lb_deletion_protection Whether to enable deletion protection for the Scylla monitoring load balancer. <code>bool</code> <code>false</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_security_groups List of existing security groups to add to the monitoring and Unreal DDC load balancers <code>list(string)</code> <code>[]</code> no internal_facing_application_load_balancer Whether the application load balancer should be internal-facing. <code>bool</code> <code>false</code> no kubernetes_version Kubernetes version to be used by the EKS cluster. <code>string</code> <code>\"1.31\"</code> no monitoring_application_load_balancer_subnets The subnets in which the ALB will be deployed <code>list(string)</code> <code>null</code> no name Unreal Cloud DDC Workload Name <code>string</code> <code>\"unreal-cloud-ddc\"</code> no nvme_managed_node_desired_size Desired number of nvme managed node group instances <code>number</code> <code>2</code> no nvme_managed_node_instance_type Nvme managed node group instance type <code>string</code> <code>\"i3en.large\"</code> no nvme_managed_node_max_size Max number of nvme managed node group instances <code>number</code> <code>2</code> no nvme_managed_node_min_size Min number of nvme managed node group instances <code>number</code> <code>1</code> no nvme_node_group_label Label applied to nvme node group. These will need to be matched in values for taints and tolerations for the worker pod definition. <code>map(string)</code> <pre>{  \"unreal-cloud-ddc/node-type\": \"nvme\"}</pre> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no region The AWS region to deploy to <code>string</code> <code>\"us-west-2\"</code> no scylla_ami_name Name of the Scylla AMI to be used to get the AMI ID <code>string</code> <code>\"ScyllaDB 6.0.1\"</code> no scylla_architecture The chip architecture to use when finding the scylla image. Valid <code>string</code> <code>\"x86_64\"</code> no scylla_db_storage Size of gp3 ebs volumes attached to Scylla DBs <code>number</code> <code>100</code> no scylla_db_throughput Throughput of gp3 ebs volumes attached to Scylla DBs <code>number</code> <code>200</code> no scylla_instance_type The type and size of the Scylla instance. <code>string</code> <code>\"i4i.2xlarge\"</code> no scylla_monitoring_instance_storage Size of gp3 ebs volumes in GB attached to Scylla monitoring instance <code>number</code> <code>20</code> no scylla_monitoring_instance_type The type and size of the Scylla monitoring instance. <code>string</code> <code>\"t3.xlarge\"</code> no scylla_monitoring_lb_access_logs_bucket Name of the S3 bucket to store the access logs for the Scylla monitoring load balancer. <code>string</code> <code>null</code> no scylla_monitoring_lb_access_logs_prefix Prefix to use for the access logs for the Scylla monitoring load balancer. <code>string</code> <code>null</code> no scylla_subnets A list of subnet IDs where Scylla will be deployed. Private subnets are strongly recommended. <code>list(string)</code> <code>[]</code> no system_managed_node_desired_size Desired number of system managed node group instances. <code>number</code> <code>1</code> no system_managed_node_instance_type Monitoring managed node group instance type. <code>string</code> <code>\"m5.large\"</code> no system_managed_node_max_size Max number of system managed node group instances. <code>number</code> <code>2</code> no system_managed_node_min_size Min number of system managed node group instances. <code>number</code> <code>1</code> no system_node_group_label Label applied to system node group <code>map(string)</code> <pre>{  \"pool\": \"system-pool\"}</pre> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"Unreal DDC\"}</pre> no vpc_id String for VPC ID <code>string</code> n/a yes worker_managed_node_desired_size Desired number of worker managed node group instances. <code>number</code> <code>1</code> no worker_managed_node_instance_type Worker managed node group instance type. <code>string</code> <code>\"c5.large\"</code> no worker_managed_node_max_size Max number of worker managed node group instances. <code>number</code> <code>1</code> no worker_managed_node_min_size Min number of worker managed node group instances. <code>number</code> <code>0</code> no worker_node_group_label Label applied to worker node group. These will need to be matched in values for taints and tolerations for the worker pod definition. <code>map(string)</code> <pre>{  \"unreal-cloud-ddc/node-type\": \"worker\"}</pre> no"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra/index.html#outputs_1","title":"Outputs","text":"Name Description cluster_arn ARN of the EKS Cluster cluster_certificate_authority_data Public key for the EKS Cluster cluster_endpoint EKS Cluster Endpoint cluster_name Name of the EKS Cluster external_alb_dns_name DNS endpoint of Application Load Balancer (ALB) external_alb_zone_id Zone ID for internet facing load balancer nvme_node_group_label Label for the NVME node group oidc_provider_arn OIDC provider for the EKS Cluster peer_security_group_id ID of the Peer Security Group s3_bucket_id Bucket to be used for the Unreal Cloud DDC assets scylla_ips IPs of the Scylla EC2 instances system_node_group_label Label for the System node group worker_node_group_label Label for the Worker node group"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html","title":"Unreal Engine Cloud DDC Intra Cluster Module","text":"<p>Warning</p> <p>Many of the links in this document lead back to the Unreal Engine source code hosted on GitHub. Access to the Unreal Engine source code requires that you connect your existing GitHub account to your Epic account. If you are seeing 404 errors when opening certain links, follow the instructions here to connect your accounts.</p> <p>Unreal Cloud Derived Data Cache (source code) is a caching system that stores additional data required to use assets, such as compiled shaders. This allows the engine to quickly retrieve this data instead of having to regenerate it, saving time and disk space for the development team. For distributed teams, a cloud-hosted DDC enables efficient collaboration by ensuring all team members have access to the same cached data regardless of their location. This Terraform module deploys the Unreal Cloud DDC container image provided by the Epic Games GitHub organization. It also configures the necessary service accounts and IAM roles required to run the Unreal Cloud DDC service on AWS.</p> <p>This module currently utilizes the Terraform EKS Blueprints Addons repository to install the following addons to the Kubernetes cluster, with the required IAM roles and service accounts:</p> <ul> <li>CoreDNS: Provides DNS services for the Kubernetes cluster, enabling reliable name resolution for the Unreal Cloud DDC service.     Kube-Proxy: Manages network traffic routing within the cluster, ensuring seamless communication between the Unreal Cloud DDC service and other components.</li> <li>VPC-CNI: Implements the Kubernetes networking model within the AWS VPC, allowing the Unreal Cloud DDC service to be properly integrated with the network infrastructure.</li> <li>EBS CSI Driver: Provides persistent storage capabilities using Amazon Elastic Block Store (EBS), enabling the Unreal Cloud DDC service to store and retrieve cached data.</li> </ul>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>This module is designed to be used in conjunction with the Unreal Cloud DDC Infra Module which deploys the required infrastructure to host the Cloud DDC service.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#github-secret","title":"GitHub Secret","text":"<p>Next, for the module to be able to access the Unreal Cloud DDC container image, there are 2 things you must do. First, if you have not done so, you must connect your GitHub account to your Epic account, thereby granting you access to the container images in the Unreal Engine repository. Next, you will need to create a <code>github_credentials</code> secret which includes a <code>username</code> and <code>access-token</code> field.</p> <p>Note</p> <p>Instructions on creating a new access token can be found here. You will need to provide the <code>read:package</code> and <code>repo</code> permissions to the access token you create.</p> <p>You can then upload the secret to AWS Secret Manager using the following AWS CLI command:</p> <pre><code>aws secretsmanager create-secret --name \"ecr-pullthroughcache/github-credentials\" --secret-string '{\"username\":\"USERNAME-PLACEHOLDER\",\"access-token\":\"ACCESS-TOKEN-PLACEHOLDER\"}'\n</code></pre> <p>Note</p> <p>Make sure to replace the <code>GITHUB-USERNAME-PLACEHOLDER</code> and <code>GITHUB-ACCESS-TOKEN-PLACEHOLDER</code> with the appropriate values from your GitHub account prior to running the command.</p> <p>Warning</p> <p>Note that the name of the secret must be prefixed with <code>ecr-pullthroughcache/</code> and the fields must be called <code>username</code> and <code>access-token</code> for ECR to properly detect the secrets. If making changes to the above command, you must adhere to these rules.</p> <p>Once the secret is created, pass the newly uploaded secret's ARN into the <code>ghcr_credentials_secret_manager_arn</code> variable.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#customizing-your-deployment","title":"Customizing Your Deployment","text":""},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#oidc-secret","title":"OIDC Secret","text":"<p>To use client secrets for OIDC authentication, a new secret must be uploaded to AWS Secrets Manager. You can upload the new secret to AWS Secret Manager using the following AWS CLI command:</p> <p>Note</p> <p>Make sure to replace the <code>CLIENT-SECRET-PLACEHOLDER</code> and <code>CLIENT-ID-PLACEHOLDER</code> with the appropriate values from your IDP prior to running the command.</p> <pre><code>aws secretsmanager create-secret --name \"external-idp-oidc-credentials\" --secret-string '{\"client_secret\":\"CLIENT-SECRET-PLACEHOLDER\",\"client_id\":\"CLIENT-ID-PLACEHOLDER\"}'\n</code></pre> <p>The ARN for the newly created secret must then be passed to the <code>oidc_credentials_secret_manager_arn</code> variable. The secret is referenced using the following format and should be passed into the variable using the same format:</p> <pre><code>aws!arn:aws:secretsmanager:&lt;region&gt;:&lt;aws-account-number&gt;:secret:&lt;secret-name&gt;|&lt;json-field&gt;\n</code></pre> <p>Note</p> <p>Note the prefix <code>aws!</code> and the postfix <code>|&lt;json-field&gt;</code> are added to the ARN of the newly created secret.</p> <p>Note</p> <p>While we highly encourage the use of OIDC tokens for production environments, users can use a bearer token in its place by providing the token to the <code>unreal_cloud_ddc_helm_values</code> variable. See DDC sample for an example implementation.</p> <pre><code>    unreal_cloud_ddc_helm_values = [\n        templatefile(\"${path.module}/assets/unreal_cloud_ddc_single_region.yaml\", {\n            token = &lt;bearer-token&gt;\n            # Other templatefile parameters...\n        })\n    ]\n</code></pre>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#chart-values-helm-configurations","title":"Chart Values (Helm Configurations)","text":"<p>The <code>unreal_cloud_ddc_helm_values</code> variable provides an open-ended way to configure the Unreal Cloud DDC deployment through the use of YAML files. We generally recommend you to use a template file. An example of a template file configuration can be found in the <code>unreal-cloud-ddc-single-region</code> sample located here. You can also find additional example templates provided by Epic here.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.5 aws &gt;= 5.38 helm &gt;=2.9.0 kubernetes &gt;=2.24.0"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#providers","title":"Providers","text":"Name Version aws &gt;= 5.38 helm &gt;=2.9.0 kubernetes &gt;=2.24.0"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#modules","title":"Modules","text":"Name Source Version aws_load_balancer_controller git::https://github.com/aws-ia/terraform-aws-eks-blueprints-addon.git 327207ad17f3069fdd0a76c14d3e07936eff4582 cert_manager git::https://github.com/aws-ia/terraform-aws-eks-blueprints-addon.git 327207ad17f3069fdd0a76c14d3e07936eff4582 ebs_csi_irsa_role git::https://github.com/terraform-aws-modules/terraform-aws-iam.git//modules/iam-role-for-service-accounts-eks ccb4f252cc340d85fd70a8a1fb1cae496a698c1f eks_blueprints_all_other_addons git::https://github.com/aws-ia/terraform-aws-eks-blueprints-addons.git a9963f4a0e168f73adb033be594ac35868696a91 eks_service_account_iam_role git::https://github.com/terraform-aws-modules/terraform-aws-iam.git//modules/iam-assumable-role-with-oidc ccb4f252cc340d85fd70a8a1fb1cae496a698c1f s3_iam_policy git::https://github.com/terraform-aws-modules/terraform-aws-iam.git//modules/iam-policy ccb4f252cc340d85fd70a8a1fb1cae496a698c1f"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#resources","title":"Resources","text":"Name Type helm_release.unreal_cloud_ddc resource kubernetes_namespace.unreal_cloud_ddc resource kubernetes_service_account.unreal_cloud_ddc_service_account resource aws_eks_cluster.unreal_cloud_ddc_cluster data source aws_iam_policy_document.aws_load_balancer_controller data source aws_iam_policy_document.cert_manager data source aws_partition.current data source aws_s3_bucket.unreal_cloud_ddc_bucket data source"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required cert_manager_hosted_zone_arns List of ARNs to be passed to Certificate Manager Addon <code>list(string)</code> n/a yes cluster_name Name of the EKS Cluster <code>string</code> n/a yes external_secrets_secret_manager_arn_list List of ARNS for Secret Manager Secrets to use in Unreal Cloud DDC <code>list(string)</code> <code>[]</code> no oidc_provider_arn ARN of the OIDC Provider from EKS Cluster <code>string</code> n/a yes s3_bucket_id ID of the S3 Bucket for Unreal Cloud DDC to use <code>string</code> n/a yes unreal_cloud_ddc_helm_values List of YAML files for Unreal Cloud DDC <code>list(string)</code> <code>[]</code> no unreal_cloud_ddc_namespace Namespace for Unreal Cloud DDC <code>string</code> <code>\"unreal-cloud-ddc\"</code> no"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.10.3 aws &gt;=5.73.0 helm &gt;=2.16.0 kubernetes &gt;=2.33.0"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#providers_1","title":"Providers","text":"Name Version aws 5.99.1 helm 2.17.0 kubernetes 2.37.1"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#modules_1","title":"Modules","text":"Name Source Version eks_blueprints_all_other_addons git::https://github.com/aws-ia/terraform-aws-eks-blueprints-addons.git a9963f4a0e168f73adb033be594ac35868696a91"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#resources_1","title":"Resources","text":"Name Type aws_ecr_pull_through_cache_rule.unreal_cloud_ddc_ecr_pull_through_cache_rule resource aws_iam_policy.s3_secrets_manager_iam_policy resource aws_iam_role.ebs_csi_iam_role resource aws_iam_role.unreal_cloud_ddc_sa_iam_role resource aws_iam_role_policy_attachment.ebs_csi_policy_attacment resource aws_iam_role_policy_attachment.unreal_cloud_ddc_sa_iam_role_s3_secrets_policy_attachment resource helm_release.unreal_cloud_ddc resource kubernetes_namespace.unreal_cloud_ddc resource kubernetes_service_account.unreal_cloud_ddc_service_account resource aws_caller_identity.current data source aws_eks_cluster.unreal_cloud_ddc_cluster data source aws_iam_openid_connect_provider.oidc_provider data source aws_iam_policy_document.unreal_cloud_ddc_policy data source aws_lb.unreal_cloud_ddc_load_balancer data source aws_region.current data source aws_s3_bucket.unreal_cloud_ddc_bucket data source"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#inputs_1","title":"Inputs","text":"Name Description Type Default Required certificate_manager_hosted_zone_arn ARN of the Certificate Manager for Ingress. <code>list(string)</code> <code>[]</code> no cluster_name Name of the EKS Cluster <code>string</code> n/a yes cluster_oidc_provider_arn ARN of the OIDC Provider from EKS Cluster <code>string</code> n/a yes enable_certificate_manager Enable Certificate Manager for Ingress. Required for TLS termination. <code>bool</code> <code>false</code> no ghcr_credentials_secret_manager_arn Arn for credentials stored in secret manager. Needs to be prefixed with 'ecr-pullthroughcache/' to be compatible with ECR pull through cache. <code>string</code> n/a yes name Unreal Cloud DDC Workload Name <code>string</code> <code>\"unreal-cloud-ddc\"</code> no oidc_credentials_secret_manager_arn Arn for oidc credentials stored in secret manager. <code>string</code> <code>null</code> no project_prefix The project prefix for this workload. This is appended to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no s3_bucket_id ID of the S3 Bucket for Unreal Cloud DDC to use <code>string</code> n/a yes tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IaC\": \"Terraform\",  \"ModuleBy\": \"CGD-Toolkit\",  \"ModuleName\": \"Unreal DDC\"}</pre> no unreal_cloud_ddc_helm_values List of YAML files for Unreal Cloud DDC <code>list(string)</code> <code>[]</code> no unreal_cloud_ddc_namespace Namespace for Unreal Cloud DDC <code>string</code> <code>\"unreal-cloud-ddc\"</code> no unreal_cloud_ddc_service_account_name Name of Unreal Cloud DDC service account. <code>string</code> <code>\"unreal-cloud-ddc-sa\"</code> no unreal_cloud_ddc_version Version of the Unreal Cloud DDC Helm chart. <code>string</code> <code>\"1.2.0\"</code> no"},{"location":"modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster/index.html#outputs_1","title":"Outputs","text":"Name Description unreal_cloud_ddc_load_balancer_name n/a unreal_cloud_ddc_load_balancer_zone_id n/a"},{"location":"samples/index.html","title":"Overview","text":"<p>Samples represent a reference implementation that can be copied, modified and deployed to solve for a specific use-case or workload. These are Terraform configurations and integrations with other common AWS workloads and services. Each sample will provide its own documentation and instructions that follows the template below:</p>"},{"location":"samples/index.html#1-predeployment","title":"1) Predeployment","text":"<p>In the predeployment phase the user is instructed to provision or take note of any necessary pre-existing resources. Creating SSL certificates or keypairs, provisioning Amazon Machine Images (AMIs) with Packer, or documenting existing resource IDs and names all fall into this phase.</p>"},{"location":"samples/index.html#2-deployment","title":"2) Deployment","text":"<p>In the deployment phase the user is instructed to run <code>terraform apply</code> on one or more Terraform configurations with the appropriate variables.</p>"},{"location":"samples/index.html#3-postdeployment","title":"3) Postdeployment","text":"<p>Finally, the postdeployment phase includes any Ansible playbooks or remote execution instructions for configuring the applications that have been deployed. These may be automated or manual steps.</p>"},{"location":"samples/simple-build-pipeline/index.html","title":"Simple Build Pipeline","text":"<p>This sample demonstrates how to use the Cloud Game Development Toolkit to deploy a simple build pipeline on AWS.</p>"},{"location":"samples/simple-build-pipeline/index.html#features","title":"Features","text":"<ul> <li>Deployment of version control with P4 Server, P4 Auth, and Code Review</li> <li>Deployment of CI/CD with Jenkins</li> </ul>"},{"location":"samples/simple-build-pipeline/index.html#step-1-install-prerequisites","title":"Step 1. Install Prerequisites","text":"<p>You will need the following tools to complete this tutorial:</p> <ol> <li>Terraform CLI</li> <li>Packer CLI</li> <li>AWS CLI v2</li> </ol>"},{"location":"samples/simple-build-pipeline/index.html#step-2-create-perforce-p4-server-formerly-helix-core-amazon-machine-image","title":"Step 2. Create Perforce P4 Server (formerly Helix Core) Amazon Machine Image","text":"<p>Prior to deploying the infrastructure for running P4 Server, we need to create an Amazon Machine Image containing the necessary software and tools. The Cloud Game Development Toolkit contains a Packer template for doing just this.</p> <p>IMPORTANT: Uploading shell scripts from Windows to Unix-based systems using Packer can sometimes fail due to line ending differences. Windows uses CRLF (<code>\\r\\n</code>) while Unix systems use LF ( <code>\\n</code>). This discrepancy can cause issues when the script is executed on the target system. In this case, this issue can occur with the shell scripts that are used to configure Perforce on the EC2 Instance.</p> <p>To avoid this, ensure you use WSL or something else similar to allow you to execute Unix commands, or use a Unix-based machine.</p> <ol> <li>From your terminal, run the following commands from the root of the repository (this example assumes usage of <code>x86_64</code> architecture as this is the default instance architecture used in the module):</li> </ol> <pre><code>packer init ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\npacker build ./assets/packer/perforce/p4-server/perforce_x86.pkr.hcl\n</code></pre> <p>This will use your AWS credentials to provision an EC2 instance in your Default VPC. This instance is only used to create the AMI and will be terminated once the AMI is successfully created. The Region, VPC, and Subnet where this instance is provisioned and the AMI is created are configurable - please consult the  <code>example.pkrvars.hcl</code> file and the Packer documentation on assigning variables for more details.</p> <p>Note: The P4 Server template will default to the user's CLI configured region, if a region is not provided.</p> <p>Note: The AWS Region where this AMI is created must be the same Region where you intend to deploy the Simple Build Pipeline.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-3-create-build-agent-amazon-machine-images","title":"Step 3. Create Build Agent Amazon Machine Images","text":"<p>This section covers the creation of Amazon Machine Images used to provision Jenkins build agents. Different studios have different needs at this stage, so we'll cover the creation of three different build agent AMIs.</p> <p>Note: The Build Agent templates will default to the user's CLI configured region, if a region is not provided.</p>"},{"location":"samples/simple-build-pipeline/index.html#amazon-linux-2023-amazon-machine-image","title":"Amazon Linux 2023 Amazon Machine Image","text":"<p>This Amazon Machine Image is provisioned using the Amazon Linux 2023 base operating system. It is highly configurable through variables, but there is only one variable that is required: A public SSH key. This public SSH key is used by the Jenkins orchestration service to establish an initial connection to the agent.</p> <p>This variable can be passed to Packer using the <code>-var-file</code> or <code>-var</code> command line flag. If you are using a variable file, please consult the  <code>example.pkrvars.hcl</code> for overridable fields. You can also pass the SSH key directly at the command line:</p>"},{"location":"samples/simple-build-pipeline/index.html#there-are-separate-arm-and-x86-based-packer-scripts-available-for-amazon-linux-2023","title":"There are separate ARM and x86 based Packer scripts available for Amazon Linux 2023","text":""},{"location":"samples/simple-build-pipeline/index.html#arm-based-image","title":"ARM Based Image","text":"<pre><code>packer build -var \"public_key=&lt;include public key here&gt;\" amazon-linux-2023-arm64.pkr.hcl\n</code></pre>"},{"location":"samples/simple-build-pipeline/index.html#x86-based-image","title":"x86 Based Image","text":"<pre><code>packer build -var \"public_key=&lt;include public key here&gt;\" amazon-linux-2023-x86_64.pkr.hcl\n</code></pre> <p>Note: The above commands assume you are running <code>packer</code> from the <code>/assets/packer/build-agents/linux</code> directory.</p> <p>Then securely store the private key value as a secret in AWS Secrets Manager.</p> <pre><code>aws secretsmanager create-secret \\\n    --name JenkinsPrivateSSHKey \\\n    --description \"Private SSH key for Jenkins build agent access.\" \\\n    --secret-string \"&lt;insert private SSH key here&gt;\" \\\n    --tags 'Key=jenkins:credentials:type,Value=sshUserPrivateKey' 'Key=jenkins:credentials:username,Value=ec2-user'\n</code></pre> <p>Take note of the output of this CLI command. You will need the ARN later.</p>"},{"location":"samples/simple-build-pipeline/index.html#ubuntu-jammy-2204-amazon-machine-images","title":"Ubuntu Jammy 22.04 Amazon Machine Images","text":"<p>These Amazon Machine Images are provisioned using the Ubuntu Jammy 22.04 base operating system. Just like the Amazon Linux 2023 AMI above, the only required variable is a public SSH key. All Linux Packer templates use the same variables file, so if you would like to share a public key across all build nodes we recommend using a variables file. In the case you do choose to use a variable file, please consult the  <code>example.pkrvars.hcl</code> for overridable fields.</p>"},{"location":"samples/simple-build-pipeline/index.html#there-are-separate-amd64-and-arm-based-packer-scripts-available-for-ubuntu-jammy-2204","title":"There are separate AMD64 and ARM based Packer scripts available for Ubuntu Jammy 22.04","text":""},{"location":"samples/simple-build-pipeline/index.html#amd64-based-image","title":"AMD64 Based Image","text":"<pre><code>packer build -var \"public_key=&lt;include public key here&gt;\" ubuntu-jammy-22.04-amd64-server.pkr.hcl\n</code></pre>"},{"location":"samples/simple-build-pipeline/index.html#arm-based-image_1","title":"ARM Based Image","text":"<pre><code>packer build -var \"public_key=&lt;include public key here&gt;\" ubuntu-jammy-22.04-arm64-server.pkr.hcl\n</code></pre> <p>Note: The above commands assume you are running <code>packer</code> from the <code>/assets/packer/build-agents/linux</code> directory.</p> <p>Finally, you'll want to upload the private SSH key to AWS Secrets Manager so that the Jenkins orchestration service can use it to connect to this build agent.</p> <pre><code>aws secretsmanager create-secret \\\n    --name JenkinsPrivateSSHKey \\\n    --description \"Private SSH key for Jenkins build agent access.\" \\\n    --secret-string \"&lt;insert private SSH key here&gt;\" \\\n    --tags 'Key=jenkins:credentials:type,Value=sshUserPrivateKey' 'Key=jenkins:credentials:username,Value=ubuntu'\n</code></pre> <p>Note: If you have already created a secret for any of the previous Amazon Linux images, please remember to change the name of this secret to avoid a naming conflict.</p> <p>Take note of the output of this CLI command. You will need the ARN later.</p>"},{"location":"samples/simple-build-pipeline/index.html#windows-2022-x86-based-amazon-machine-image","title":"Windows 2022 X86 based Amazon Machine Image","text":"<p>This Amazon Machine Image is provisioned using the Windows Server 2022 base operating system. It installs all required tooling for Unreal Engine 5 compilation by default. Please consult the release notes for Unreal Engine 5.4 for details on what tools are used for compiling this version of the engine.</p> <p>Again, the only required variable for building this Amazon Machine Image is a public SSH key.</p> <pre><code>packer build -var \"public_key=&lt;include public ssh key here&gt;\" windows.pkr.hcl\n</code></pre> <p>Note: The above command assumes you are running <code>packer</code> from the <code>/assets/packer/build-agents/windows</code> directory.</p> <p>Finally, you'll want to upload the private SSH key to AWS Secrets Manager so that the Jenkins orchestration service can use it to connect to this build agent.</p> <pre><code>aws secretsmanager create-secret \\\n    --name JenkinsPrivateSSHKey \\\n    --description \"Private SSH key for Jenkins build agent access.\" \\\n    --secret-string \"&lt;insert private SSH key here&gt;\" \\\n    --tags 'Key=jenkins:credentials:type,Value=sshUserPrivateKey' 'Key=jenkins:credentials:username,Value=jenkins'\n</code></pre> <p>Note: If you have already created a secret for any of the previous Amazon Linux or Ubuntu Jammy based images, please remember to change the name of this secret to avoid a naming conflict.</p> <p>Take note of the output of this CLI command. You will need the ARN later.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-4-create-route53-hosted-zone","title":"Step 4. Create Route53 Hosted Zone","text":"<p>Now that all of the required Amazon Machine Images exist we are almost ready to move on to provisioning infrastructure. However, the Simple Build Pipeline requires that we create one resource ahead of time: A Route53 Hosted Zone. The Simple Build Pipeline creates DNS records and SSL certificates for all the applications it deploys to support secure communication over the internet. However, these certificates and DNS records rely on the existence of a public hosted zone associated with your company's route domain. Since different studios may use different DNS registrars or DNS providers, the Simple Build Pipeline requires this first step to be completed manually. Everything else will be deployed automatically in the next step.</p> <p>If you do not already have a domain you can register one with Route53. When you register a domain with Route53 a public hosted zone is automatically created.</p> <p>If you already have a domain that you would like to use for the Simple Build Pipeline please consult the documentation for making Amazon Route 53 the DNS service for an existing domain.</p> <p>Once your hosted zone exists you can proceed to the next step.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-5-configure-simple-build-pipeline-variables","title":"Step 5. Configure Simple Build Pipeline Variables","text":"<p>Configurations for the Simple Build Pipeline are split between 2 files:  <code>local.tf</code> and  <code>variables.tf</code>. Variables in <code>local.tf</code> are typically static and can be modified within the file itself. Variables in <code>variables.tf</code> tend to be more dynamic and are passed in through the <code>terraform apply</code> command either directly through a <code>-var</code> flag or as file using the <code>-var-file</code> flag.</p> <p>We'll start by walking through the required configurations in <code>local.tf</code>.</p> <p>1. <code>jenkins_agent_secret_arns</code> is a list of AWS Secrets Manager ARNs that the Jenkins orchestration service will be granted access to. This is primarily used for providing private SSH keys to Jenkins so that the orchestration service can connect to your build agents. When you created build agent AMIs earlier you also uploaded private SSH keys to AWS Secrets Manager. The ARNs of those secrets should be added to the <code>jenkins_agent_secret_arns</code> list so that Jenkins can connect to the provisioned build agents.</p> <ol> <li> <p>The    <code>build_farm_compute</code> map contains all of the information needed to provision your Jenkins build farms. Each entry in this map corresponds to an EC2 Auto Scaling group, and requires two fields to be specified:    <code>ami</code> and <code>instance_type</code>. The    <code>local.tf</code> file contains an example configuration that has been commented out. Using the AMI IDs from Step 3, please specify the build farms you would like to provision. Selecting the right instance type for your build farm is highly dependent on your build process. Larger instances are more expensive, but provide improved performance. For example, large Unreal Engine compilation jobs will perform significantly better on Compute Optimized instances, while cook jobs tend to benefit from the increased RAM available from Memory Optimized instances. It can be a good practice to provision an EC2 instance using your custom AMI, and run your build process locally to determine the right instance size for your build farm. Once you have settled on an instance type, complete the    <code>build_farm_compute</code> map to configure your build farms.</p> </li> <li> <p>Finally, the    <code>build_farm_fsx_openzfs_storage</code> field configures file systems used by your build agents for mounting P4 Server workspaces and shared caches. Again, an example configuration is provided but commented out. Depending on the number of builds you expect to be performing and the size of your project, you may want to adjust the size of the suggested file systems.</p> </li> </ol> <p>The variables in <code>variables.tf</code> are as follows:</p> <p>.<code>route53_public_hosted_zone_name</code> must be set to the public hosted zone you created in Step 4. Your applications will be deployed at subdomains. For example, if <code>route53_public_hosted_zone_name=example.com</code> then Jenkins will be available at <code>jenkins.example.com</code> and P4 Server e will be available at <code>perforce.example.com</code>. These subdomains are configurable via <code>locals.tf</code>.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-6-deploy-simple-build-pipeline","title":"Step 6. Deploy Simple Build Pipeline","text":"<p>Now we are ready to deploy your Simple Build Pipeline! Navigate to the <code>/samples/simple-build-pipeline</code> directory and run the following commands:</p> <pre><code>terraform init\n</code></pre> <p>This will install the modules and required Terraform providers.</p> <pre><code>terraform apply -var \"route53_public_hosted_zone_name=&lt;insert your root domain&gt;\"\n</code></pre> <p>This will create a Terraform plan, and wait for manual approval to deploy the proposed resources. Once approval is given the entire deployment process takes roughly 10 minutes.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-7-configure-jenkins","title":"Step 7. Configure Jenkins","text":"<p>Now that everything is deployed, its time to configure the applications included in the Simple Build Pipeline. First, we will setup Jenkins.</p>"},{"location":"samples/simple-build-pipeline/index.html#initial-access","title":"Initial Access","text":"<p>When accessing Jenkins for the first time, an administrator's password is required. This password is auto-generated and available through the service logs.</p> <ol> <li>Open the AWS console and navigate to the Elastic Container Service (ECS) console.</li> <li>In the <code>Clusters</code> tab, select the <code>build-pipeline-cluster</code></li> <li>Select the <code>cgd-jenkins-service</code></li> <li>Select the <code>Logs</code> tab</li> <li>Scroll through the logs until you find the password, below is an example of what the password section looks like. Note that each line is shown as its own log entry in the console.</li> </ol> <p></p> <p>Open the Jenkins console in your preferred browser by navigating to <code>jenkins.&lt;your fully qualified domain name&gt;</code>, and log in using the administrator's password you just located. Install the suggested plugins and create your first admin user. For the Jenkins URL accept the default value. This URL is provided as an output on the terminal where you ran <code>terraform apply</code>.</p>"},{"location":"samples/simple-build-pipeline/index.html#useful-plugins","title":"Useful Plugins","text":"<p>There are 2 plugins recommended for the solutions: The EC2 Fleet Plugin and the AWS Secrets Manager Credentials Provider Plugin. The <code>EC2 Fleet</code> Plugin is used to integrate Jenkins with AWS and allows EC2 instances to be used as build nodes through an autoscaling group. The <code>AWS Secrets Manager Credentials Provider</code> Plugin will allow users to store their credentials in AWS Secrets Manager and seamlessly access them in Jenkins.</p> <ol> <li>Open the Jenkins console.</li> <li>On the left-hand side, select the <code>Manage Jenkins</code> tab.</li> <li>Then, under the <code>System Configuration</code> section, select <code>Plugins</code>.</li> <li>On the left-hand side, select <code>Available plugins</code>.</li> <li>Using the search bar at the top of the page, search for <code>EC2 Fleet</code>.</li> <li>Select the <code>EC2 Fleet</code> plugin.</li> <li>Using the search bar at the top of the page, search for <code>AWS Secrets Manager Credentials Provider</code>.</li> <li>Select the <code>AWS Secrets Manager Credentials Provider</code> plugin.</li> <li>Click <code>install</code> on the top-right corner of the page.</li> <li>Once the installation is complete, Select <code>Go back to the top page</code> at the bottom of the page</li> </ol>"},{"location":"samples/simple-build-pipeline/index.html#jenkins-cloud-configuration","title":"Jenkins Cloud Configuration","text":"<p>We now need to setup our Auto Scaling groups as Jenkins build agents. To do this, we will create multiple Jenkins \"Cloud\" resources; one for each of the Auto Scaling groups we deployed in the previous step.</p> <ol> <li>From the Jenkins homepage, on the left-hand side, choose <code>Manage Jenkins</code>.</li> <li>Under the <code>System Configuration</code> section, choose <code>Clouds</code></li> <li>Select <code>New Cloud</code></li> <li>Enter a name for your cloud configuration</li> <li>Select <code>Amazon EC2 Fleet</code></li> <li>Click <code>Create</code></li> <li>On the <code>New Cloud</code> configuration page, change the following settings.<ol> <li>Region - Select the region in which you deployed the Simple Build Pipeline</li> <li>EC2 Fleet - Select the autoscaling group you would like to use</li> <li>Launcher - Select <code>Launch agents via SSH</code></li> <li>Launcher -&gt; Credentials - Select the credentials associated with that particular autoscaling group</li> <li>Launcher -&gt; Host Key Verification Strategy - Select <code>Non verifying Verification Strategy</code></li> <li>Connect to instaces via private IP instead of public IP - Select the <code>Private IP</code> check box</li> <li>Max Idle Minutes Before Scaledown - Set this variable to <code>5</code> (minutes). Feel free to change this based on your needs.</li> </ol> </li> </ol> <p>Repeat the process above for each of the Auto Scaling groups you specified in your <code>build_farm_compute</code> configuration. You should now be able to reference these \"Cloud\" agents in your Jenkins pipeline definitions.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-8-configure-p4auth-formerly-helix-authentication-service","title":"Step 8. Configure P4Auth (formerly Helix Authentication Service)","text":"<p>The P4Auth provides integrations with common identity providers so that end-users of P4 Server (formerly Helix Core) and P4 Code Review (formerly Helix Swarm) can use their existing credentials to access version control and code review tools.</p> <p>The Simple Build Pipeline deploys the P4Auth with the administrator web-based UI enabled. You should be able to navigate to <code>auth.perforce.&lt;your fully qualified domain name&gt;/admin</code> to configure your external IDP. This URL is provided as an output on the terminal where you ran <code>terraform apply</code>.</p> <p>With the default configuration, the deployment of the Perforce module as part of the Simple Build Pipeline creates a random administrator password and stores it in AWS Secrets Manager. You can find this password by navigating to the AWS Secrets Manager console and viewing the secret ending in <code>-AdminUserPassword</code> The username is also available within the secret ending in <code>-AdminUsername</code>. Use these credentials to access the web UI and configure your identity provider.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-9-test-p4-server-and-p4-code-review","title":"Step 9. Test P4 Server and P4 Code Review","text":"<p>Like P4Auth, an administrator's password is created for P4 Server. The username and password are available in AWS Secrets Manager under the secrets ending in <code>-SuperUserPassword</code> and <code>-SuperUserUsername</code>. Use these credentials to access P4 Server for the first time. The P4 server connection string and P4 Code Review URLs are provided as outputds on the terminal where you ran <code>terraform apply</code>.</p> <p>Once you have access to P4 Server you should be able to provision new users. You can do this through the P4Admin GUI or from the command line. For more information please consult the P4 Server documentation. Users provisioned with an email address that corresponds with the identity provider configured in P4Auth will be able to use their existing credentials to log in to P4 Server and P4 Code Review.</p>"},{"location":"samples/simple-build-pipeline/index.html#step-10-cleanup","title":"Step 10. Cleanup","text":"<p>Tearing down the resources created by the Simple Build Pipeline is as easy as running <code>terraform destroy</code> in the <code>/samples/simple-build-pipeline</code> directory. However, this will not delete the secrets you've uploaded, the AMIs created with Packer, or the the Route53 hosted zone you set up initially. Those resources will need to be explicitly destroyed using the AWS console or relevant CLI commands.</p>"},{"location":"samples/simple-build-pipeline/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.97.0 http 3.5.0 netapp-ontap 2.1.0"},{"location":"samples/simple-build-pipeline/index.html#providers","title":"Providers","text":"Name Version aws 5.97.0 http 3.5.0"},{"location":"samples/simple-build-pipeline/index.html#modules","title":"Modules","text":"Name Source Version jenkins ../../modules/jenkins n/a terraform-aws-perforce ../../modules/perforce n/a"},{"location":"samples/simple-build-pipeline/index.html#resources","title":"Resources","text":"Name Type aws_acm_certificate.shared resource aws_acm_certificate_validation.shared_certificate resource aws_default_security_group.default resource aws_ecs_cluster.build_pipeline_cluster resource aws_ecs_cluster_capacity_providers.providers resource aws_eip.nat_gateway_eip resource aws_internet_gateway.igw resource aws_lb.service_nlb resource aws_lb.web_alb resource aws_lb_listener.internal_https resource aws_lb_listener.public_https resource aws_lb_listener_rule.jenkins resource aws_lb_listener_rule.perforce_auth resource aws_lb_listener_rule.perforce_code_review resource aws_lb_target_group.alb_target resource aws_lb_target_group_attachment.alb_attachment resource aws_nat_gateway.nat_gateway resource aws_route.private_rt_nat_gateway resource aws_route53_record.jenkins_private resource aws_route53_record.jenkins_public resource aws_route53_record.p4_server_private resource aws_route53_record.p4_server_public resource aws_route53_record.p4_web_services_private resource aws_route53_record.p4_web_services_public resource aws_route53_record.shared_certificate resource aws_route53_zone.private_zone resource aws_route_table.private_rt resource aws_route_table.public_rt resource aws_route_table_association.private_rt_asso resource aws_route_table_association.public_rt_asso resource aws_security_group.allow_my_ip resource aws_security_group.internal_shared_application_load_balancer resource aws_security_group.public_network_load_balancer resource aws_subnet.private_subnets resource aws_subnet.public_subnets resource aws_vpc.build_pipeline_vpc resource aws_vpc_security_group_egress_rule.internal_alb_http_to_jenkins resource aws_vpc_security_group_egress_rule.internal_alb_http_to_p4_auth resource aws_vpc_security_group_egress_rule.internal_alb_http_to_p4_code_review resource aws_vpc_security_group_egress_rule.public_nlb_https_to_internal_alb resource aws_vpc_security_group_ingress_rule.allow_https resource aws_vpc_security_group_ingress_rule.allow_perforce resource aws_vpc_security_group_ingress_rule.internal_alb_https_from_p4_server resource aws_vpc_security_group_ingress_rule.internal_alb_https_from_public_nlb resource aws_vpc_security_group_ingress_rule.jenkins_http_from_internal_alb resource aws_vpc_security_group_ingress_rule.p4_auth_http_from_internal_alb resource aws_vpc_security_group_ingress_rule.p4_code_review_http_from_internal_alb resource aws_vpc_security_group_ingress_rule.p4_server_from_jenkins_build_farm resource aws_vpc_security_group_ingress_rule.p4_server_from_jenkins_service resource aws_ami.ubuntu_noble_amd data source aws_availability_zones.available data source aws_route53_zone.root data source http_http.my_ip data source"},{"location":"samples/simple-build-pipeline/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required route53_public_hosted_zone_name The fully qualified domain name of your existing Route53 Hosted Zone. <code>string</code> n/a yes"},{"location":"samples/simple-build-pipeline/index.html#outputs","title":"Outputs","text":"Name Description jenkins_url The URL for the Jenkins service. p4_auth_admin_url The URL for the P4Auth service admin page. p4_code_review_url The URL for the P4 Code Review service. p4_server_connection_string The connection string for the P4 Server. Set your P4PORT environment variable to this value."},{"location":"samples/unreal-cloud-ddc-single-region/index.html","title":"Unreal Cloud DDC Single Region","text":"<p>The Unreal Cloud DDC Single Region is a comprehensive solution that leverages several AWS services to create a robust and efficient data caching system. It uses a well-designed Virtual Private Cloud (VPC) to ensure network isolation and security. The solution employs an Amazon Elastic Kubernetes Service (EKS) Cluster with Node Groups to manage and orchestrate containerized applications.</p> <p>At the heart of the system is an instance of ScyllaDB, a high-performance NoSQL database, running on specially optimized Amazon EC2 instances. The Unreal Cloud Derived Data Cache Container is managed by Helm, a package manager for Kubernetes, and uses Amazon S3 for durable storage.</p>"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#predeployment-set-up-github-content-repository-credentials","title":"Predeployment - Set Up Github Content Repository Credentials","text":"<p>The Unreal Cloud DDC Inter Cluster module utilizes a pull through cache to access the Unreal Cloud DDC image. This requires a secret in Secrets Manager. The secret needs to be prefixed with <code>ecr-pullthroughcache/</code>. Additionally, the secret is required to be in the following format: <pre><code>{\n  \"username\":\"GITHUB-USER-NAME-PLACEHOLDER\",\n  \"accessToken\":\"GITHUB-ACCESS-TOKEN-PLACEHOLDER\"\n}\n</code></pre></p>"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#deployment","title":"Deployment","text":"<p>Once you've completed the prerequisites and set your variables, you can deploy the solution by running:</p> <pre><code>terraform apply\n</code></pre> <p>The deployment can take close to 30 minutes. Creating the EKS Node Groups and EKS Cluster take around 20 minutes to fully deploy.</p>"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#postdeployment","title":"Postdeployment","text":"<p>The sample deploys a Route53 dns record that you can use to access your Unreal DDC cluster. This record points to an NLB which may take more time to become fully available when the deployment is complete. You can view the provisioning status of this NLB on the EC2 load balncing screen.</p> <p>The Unreal Cloud DDC module creates a Service Account and valid bearer token for testing. This bearer token is stored in AWS Secrets Manager. The ARN of this secret is provided as a Terraform output (<code>\"unreal_cloud_ddc_bearer_token_arn\"</code>) on the console following deployment. To fetch the bearer token you can use the aws CLI: <pre><code>aws secretsmanager get-secret-value --secret-id &lt;\"unreal_cloud_ddc_bearer_token_arn\"&gt;\n</code></pre></p> <p>To validate you can put an object you can run: <pre><code>curl http://&lt;unreal_ddc_url&gt;/api/v1/refs/ddc/default/00000000000000000000000000000000000000aa -X PUT --data 'test' -H 'content-type: application/octet-stream' -H 'X-Jupiter-IoHash: 4878CA0425C739FA427F7EDA20FE845F6B2E46BA' -i -H 'Authorization: ServiceAccount &lt;secret-manager-token&gt;'\n</code></pre> After running this you should get a response that looks as the following: <pre><code>HTTP/1.1 200 OK\nServer: nginx\nDate: Wed, 29 Jan 2025 19:15:05 GMT\nContent-Type: application/json; charset=utf-8\nTransfer-Encoding: chunked\nConnection: keep-alive\nServer-Timing: blob.put.FileSystemStore;dur=0.1451;desc=\"PUT to store: 'FileSystemStore'\",blob.put.AmazonS3Store;dur=267.0449;desc=\"PUT to store: 'AmazonS3Store'\",blob.get-metadata.FileSystemStore;dur=0.0406;desc=\"Blob GET Metadata from: 'FileSystemStore'\",ref.finalize;dur=7.1407;desc=\"Finalizing the ref\",ref.put;dur=25.2064;desc=\"Inserting ref\"\n\n{\"needs\":[]}%\n</code></pre></p> <p>You can then access the same chunk with the following command: <pre><code>curl http://&lt;unreal_ddc_url&gt;/api/v1/refs/ddc/default/00000000000000000000000000000000000000aa.json -i -H 'Authorization: ServiceAccount &lt;unreal-cloud-ddc-bearer-token&gt;'\n</code></pre></p> <p>The response should look like the following: <pre><code>HTTP/1.1 200 OK\nServer: nginx\nDate: Wed, 29 Jan 2025 19:16:46 GMT\nContent-Type: application/json\nContent-Length: 66\nConnection: keep-alive\nX-Jupiter-IoHash: 7D873DCC262F62FBAA871FE61B2B52D715A1171E\nX-Jupiter-LastAccess: 01/29/2025 19:16:46\nServer-Timing: ref.get;dur=0.0299;desc=\"Fetching Ref from DB\"\n\n{\"RawHash\":\"4878ca0425c739fa427f7eda20fe845f6b2e46ba\",\"RawSize\":4}%\n</code></pre> For a more comprehensive test of your deployment, we recommend using the bench marking tools. To do so we used a x2idn.32xlarge as it matched Epic's benchmarking instance to test their configuration.</p> <p>With the benchmarking tools we ran the following command after compiling the docker image: <pre><code>docker run --network host jupiter_benchmark --seed --seed-remote --host http://&lt;unreal_ddc_url&gt; --namespace ddc \\\n--header=\"Authorization: ServiceAccount &lt;unreal-cloud-ddc-bearer-token&gt;\" all\n</code></pre> Just a note here, you will have to specify the namespace to be DDC as the token only has access to that namespace.</p> <p>It is recommended that if you are using this in a production capacity you change the authentication mode from Service Account to Bearer and use an IDP to authenticate and TLS termination.</p> <p>This sample also deploys a ScyllaDB monitoring stack, enabling real-time insights into the status and performance of your ScyllaDB nodes. The monitoring stack includes Prometheus for metrics collection, Alertmanager for handling alerts, and Grafana for visualization. You can access the Grafana dashboard by using the <code>\"monitoring_url\"</code> provided in the sample outputs. To learn more about the ScyllaDB monitoring stack, refer to the ScyllaDB Monitoring Stack Documentation.</p>"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws &gt;= 5.38 helm &gt;= 2.9.0 kubernetes &gt;= 2.24.0"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#providers","title":"Providers","text":"Name Version aws &gt;= 5.38"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#modules","title":"Modules","text":"Name Source Version unreal_cloud_ddc_infra ../../modules/unreal/unreal-cloud-ddc-infra n/a unreal_cloud_ddc_intra_cluster ../../modules/unreal/unreal-cloud-ddc-intra-cluster n/a unreal_cloud_ddc_vpc git::https://github.com/terraform-aws-modules/terraform-aws-vpc.git 25322b6b6be69db6cca7f167d7b0e5327156a595"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#resources","title":"Resources","text":"Name Type aws_region.current data source"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required caller_ip IPs that will be allow listed to access cluster over internet <code>list(string)</code> <code>[]</code> no ghcr_password GHCR password <code>string</code> n/a yes ghcr_username GHCR username <code>string</code> n/a yes jwt_audience JWT Audience <code>string</code> n/a yes jwt_authority JWT Authority <code>string</code> n/a yes okta_auth_server_id Okta Auth Server ID <code>string</code> n/a yes okta_domain Okta Domain <code>string</code> n/a yes profile AWS Profile name <code>string</code> <code>\"default\"</code> no region AWS Region <code>string</code> <code>\"us-west-2\"</code> no"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.10.3 aws &gt;= 5.89.0 awscc &gt;= 1.26.0 helm &gt;= 2.9.0 http &gt;= 3.4.5 kubernetes &gt;= 2.24.0 random 3.5.1"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#providers_1","title":"Providers","text":"Name Version aws 5.99.1 awscc 1.43.0 http 3.5.0"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#modules_1","title":"Modules","text":"Name Source Version unreal_cloud_ddc_infra ../../modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-infra n/a unreal_cloud_ddc_intra_cluster ../../modules/unreal/unreal-cloud-ddc/unreal-cloud-ddc-intra-cluster n/a unreal_cloud_ddc_vpc ./vpc n/a"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#resources_1","title":"Resources","text":"Name Type aws_acm_certificate.scylla_monitoring resource aws_acm_certificate_validation.scylla_monitoring resource aws_route53_record.scylla_monitoring resource aws_route53_record.scylla_monitoring_cert resource aws_route53_record.unreal_cloud_ddc resource aws_security_group.unreal_ddc_load_balancer_access_security_group resource aws_vpc_security_group_egress_rule.unreal_ddc_load_balancer_egress_sg_rules resource aws_vpc_security_group_ingress_rule.unreal_ddc_load_balancer_http2_ingress_rule resource aws_vpc_security_group_ingress_rule.unreal_ddc_load_balancer_http_ingress_rule resource aws_vpc_security_group_ingress_rule.unreal_ddc_load_balancer_https_ingress_rule resource awscc_secretsmanager_secret.unreal_cloud_ddc_token resource aws_availability_zones.available data source aws_caller_identity.current data source aws_ecr_authorization_token.token data source aws_region.current data source aws_route53_zone.root data source aws_secretsmanager_secret_version.unreal_cloud_ddc_token data source http_http.public_ip data source"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#inputs_1","title":"Inputs","text":"Name Description Type Default Required allow_my_ip Automatically add your IP to the security groups allowing access to the Unreal DDC and SycllaDB Monitoring load balancers <code>bool</code> <code>true</code> no github_credential_arn Github Credential ARN <code>string</code> n/a yes route53_public_hosted_zone_name The root domain name for the Hosted Zone where the ScyllaDB monitoring record should be created. <code>string</code> n/a yes"},{"location":"samples/unreal-cloud-ddc-single-region/index.html#outputs_1","title":"Outputs","text":"Name Description monitoring_url n/a unreal_cloud_ddc_bearer_token_arn n/a unreal_ddc_url n/a"}]}