{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>The Cloud Game Development Toolkit (a.k.a. CGD Toolkit) is a collection of templates and configurations for deploying game development infrastructure and tools on AWS.</p> <p>Info</p> <p>This project is under active development and we have yet to solve for many developer needs. If you would like to see something in this repository please create a feature request in the Issues tab, or raise a pull request. You'll find our contribution guidelines here.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The CGD Toolkit consists of four key components:</p> <ul> <li> <p>Assets</p> <p>Reusable Amazon Machine Image (AMI) templates written in Packer for common game development workloads.</p> <p> Learn about Assets</p> </li> <li> <p>Modules</p> <p>Configurable Terraform modules for simplified cloud deployment with best-practices by default.</p> <p> Learn about Modules</p> </li> <li> <p>Playbooks</p> <p>Automation scripts written with Ansible to configure workloads after deployment.</p> <p> Learn about Playbooks</p> </li> <li> <p>Samples</p> <p>Complete Terraform configurations for expedited studio setup that demonstrate module usage and integration with other AWS services.</p> <p> Learn about Samples</p> </li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check the Getting Started.</p>"},{"location":"assets/","title":"Assets","text":"<p>Assets are reusable scripts, pipeline definitions, Dockerfiles, Packer templates, and other resources that might prove useful or are dependencies of any of the modules. Each asset contains its own documentation as a <code>README.md</code> file. </p> <p>Info</p> <p>Don't see an asset listed? Create a feature request for a new asset or learn how to contribute new assets to the project below</p>"},{"location":"assets/#packer-templates","title":"Packer Templates","text":"<p>Packer templates provide an easy way to build machine images for commonly used game dev infrastructure. Currently the project includes Packer templates for UE5 build agents for Linux and Windows, as well as a Packer template for building a Perforce Helix Core version control AMI.</p> <ul> <li>Unreal Engine Build Agents: Packer templates for building Unreal Engine on Windows and Linux, including support for x86 and ARM (i.e. Graviton).</li> <li>Perforce Helix Core AMI: Packer templates for creating an Amazon Machine Image (AMI) of Perforce Helix Core on Linux and configuration with Perforce Server Deployment Package (SDP)</li> </ul>"},{"location":"assets/#unreal-engine-build-agents","title":"Unreal Engine Build Agents","text":"<p>Packer templates to create machine images for Unreal Engine build agents.</p> Asset Description Type Docs Unreal Engine Build Agent (Linux) Packer templates for building Unreal Engine on Linux, with support for x86 and ARM (i.e. Graviton). Packer Template Docs Unreal Engine Build Agent (Windows) Packer templates for building Unreal Engine on Windows Packer Template Docs"},{"location":"assets/#packer-templates-for-unreal-engine-linux-build-agents","title":"Packer templates for Unreal Engine Linux build agents","text":"<p>The following templates provide Unreal Engine Linux build agents:</p> Operating system CPU architecture file location Ubuntu Jammy 22.04 x86_64 (a.k.a. amd64) <code>x86_64/ubuntu-jammy-22.04-amd64-server.pkr.hcl</code> Ubuntu Jammy 22.04 aarch64 (a.k.a. arm64) <code>aarch64/ubuntu-jammy-22.04-arm64-server.pkr.hcl</code> Amazon Linux 2023 x86_64 (a.k.a. amd64) <code>x86_64/amazon-linux-2023-x86_64.pkr.hcl</code> Amazon Linux 2023 aarch64 (a.k.a. arm64) <code>aarch64/amazon-linux-2023-arm64.pkr.hcl</code>"},{"location":"assets/#usage","title":"Usage","text":"<ol> <li>Make a copy of <code>example.pkrvars.hcl</code> and adjust the input variables as needed</li> <li>Ensure you have active AWS credentials</li> <li>Invoke <code>packer build --var-file=&lt;your .pkrvars.hcl file&gt; &lt;path to .pkr.hcl file&gt;</code>, then wait for the build to complete.</li> </ol>"},{"location":"assets/#software-packages-included","title":"Software packages included","text":"<p>The templates install various software packages:</p> <p>common tools</p> <p>Some common tools are installed to enable installing other software, performing maintenance tasks, and compile some C++ software:</p> <ul> <li>git</li> <li>curl</li> <li>jq</li> <li>unzip</li> <li>dos2unix</li> <li>AWS CLI v2</li> <li>AWS Systems Manager Agent</li> <li>Amazon Corretto</li> <li>mount.nfs, to be able to mount FSx volumes over NFS</li> <li>python3</li> <li>python3 packages: 'pip', 'requests', 'boto3' and 'botocore'</li> <li>clang</li> <li>cmake3</li> <li>scons</li> <li>Development libraries for compiling the Amazon GameLift Server SDK for C++</li> <li>Development libraries for compiling the Godot 4 game engine (if available in the OS's package manager)</li> </ul> <p>mold</p> <p>The 'mold' linker is installed to enable faster linking.</p> <p>FSx automounter service</p> <p>The FSx automounter systemd service is a service written in Python that automatically mounts FSx for OpenZFS volumes on instance bootup. The service uses resource tags on FSx volumes to determine if and where to mount volumes on.</p> <p>You can use the following tags on FSx volumes: * 'automount-fsx-volume-name' tag: specifies the name of the local mount point. The mount point specified will be prefixed with 'fsx_' by the service. * 'automount-fsx-volume-on' tag: This tag contains a space-delimited list of EC2 instance names on which the volume will be automatically mounted by this service (if it is running on that instance).</p> <p>For example, if the FSx automounter service is running on an EC2 instance with Name tag 'ubuntu-builder', and an FSx volume has tag <code>automount-fsx-volume-on</code>=<code>al2023-builder ubuntu-builder</code> and tag <code>automount-fsx-volume-name</code>=<code>workspace</code>, then the automounter will automatically mount that volume on <code>/mnt/fsx_workspace</code>.</p> <p>Note that the automounter service makes use of the ListTagsForResource FSx API call, which is rate-limited. If you intend to scale up hundreds of EC2 instances that are running this service, then we recommend automatically mounting FSx volumes using <code>/etc/fstab</code>.</p> <p>mount_ephemeral service</p> <p>The mount_ephemeral service is a systemd service written as a simple bash script that mounts NVMe attached instance storage volume automatically as temporary storage. It does this by formatting <code>/dev/nvme1n1</code> as xfs and then mounting it on <code>/tmp</code>. This service runs on instance bootup.</p> <p>create_swap service</p> <p>The create_swap service is a systemd service written as a simple bash script that creates a 1GB swap file on <code>/swapfile</code>. This service runs on instance bootup.</p> <p>sccache</p> <p>'sccache' is installed to cache c/c++ compilation artefacts, which can speed up builds by avoiding unneeded work.</p> <p>sccache is installed as a systemd service, and configured to use <code>/mnt/fsx_cache/sccache</code> as its cache folder. The service expects this folder to be available or set up by another service.</p> <p>octobuild</p> <p>'Octobuild' is installed to act as a compilation cache for Unreal Engine.</p> <p>Octobuild is configured (in octobuild.conf) to use <code>/mnt/fsx_cache/octobuild_cache</code> as its cache folder, and expects this folder to be available or set up by another service.</p> <p>NOTE: Octobuild is not supported on aarch64, and therefore not installed there.</p>"},{"location":"assets/#processor-architectures-and-naming-conventions","title":"Processor architectures and naming conventions","text":"<p>Within this folder, the processor architecture naming conventions as reported by <code>uname -m</code> are used, hence why there are scripts here with names containing \"x86_64\" or \"aarch64\". The packer template <code>.hcl</code> files are named following the naming conventions of the operating system that they are based on. Unfortunately, because some operating systems don't use the same terminology in their naming conventions throughout, this means that you'll see this lack of consistency here has well.</p>"},{"location":"assets/#unreal-engine-build-agent-windows","title":"Unreal Engine Build Agent (Windows)","text":"<p>Documentation for UE Build Agents for Windows</p>"},{"location":"assets/#perforce-helix-core-packer-template","title":"Perforce Helix Core Packer Template","text":""},{"location":"assets/#overview","title":"Overview","text":"<p>This Packer template automates the deployment and configuration of a Helix Core Server (P4D) on a Linux environment, specifically tailored for use with SELinux and systemd. It performs various tasks such as checking and setting up the necessary user and group, handling SELinux context, installing and configuring Perforce Software's Server Deployment Package (SDP), and setting up the Helix Core service with systemd.</p> <p>Steps in the Packer template automation include: </p> <ol> <li>Pre-Flight Checks: Ensures the script is run with root privileges.</li> <li>Environment Setup: Defines paths and necessary constants for the installation.</li> <li>SELinux Handling: Checks if SELinux is enabled and installs required packages.</li> <li>User and Group Verification: Ensures the 'perforce' user and group exist.</li> <li>Directory Creation and Ownership: Ensures necessary directories exist and have correct ownership.</li> <li>Helix Binaries and SDP Installation: Downloads and extracts SDP, checks for Helix binaries, and downloads them if missing.</li> <li>Systemd Service Configuration: Sets up a systemd service for the p4d server.</li> <li>SSL Configuration: Updates SSL certificate configuration with the EC2 instance DNS name.</li> <li>SELinux Context Management: Updates SELinux context for p4d.</li> <li>Crontab Initialization: Sets up crontab for the 'perforce' user.</li> <li>SDP Verification: Runs a script to verify the SDP installation.</li> </ol>"},{"location":"assets/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Linux system with DNF package manager (e.g., Fedora, RHEL, CentOS).</li> <li>Root access to the system.</li> <li>SELinux in Enforcing or Permissive mode (optional but recommended).</li> <li>Access to the internet for downloading necessary packages and binaries.</li> </ul>"},{"location":"assets/#how-to-use","title":"How to Use","text":"<ol> <li>Download the Script: Clone or download this repository to your system.</li> <li>Provide Execution Permission: Give execute permission to the script using <code>chmod +x &lt;script_name&gt;.sh</code>.</li> <li>Run the Script: Execute the script as root:</li> </ol> <p><code>sudo ./&lt;script_name&gt;.sh</code></p> <ol> <li>Follow the On-Screen Instructions: The script is mostly automated, but monitor the output for any errors or required manual inputs.</li> </ol>"},{"location":"assets/#important-notes","title":"Important Notes","text":"<ul> <li>This script is designed for a specific use-case and might require modifications for different environments or requirements.</li> <li>Ensure you have a backup of your system before running the script, as it makes significant changes to users, groups, and services.</li> <li>The script assumes an internet connection for downloading packages and binaries.</li> </ul>"},{"location":"assets/#dockerfiles","title":"Dockerfiles","text":"<p>Dockerfiles for creating Docker images of commonly used game dev infrastructure. </p>"},{"location":"assets/#contribute-new-assets-to-the-cloud-game-development-toolkit","title":"Contribute new Assets to the Cloud Game Development Toolkit","text":"<p>This section will contain documentation about how to create new assets, how they should be tested and documented, and the process for submitting them as PRs to the project.</p>"},{"location":"modules/","title":"Cloud Game Development Terraform Modules","text":""},{"location":"modules/#introduction","title":"Introduction","text":"<p>These modules simplify the deployment of common game development workloads on AWS. Some have pre-requisites that will be outlined in their respective documentation. They are designed to easily integrate with each other, and provide relevant outputs to simplify permissions, networking, and access.</p>"},{"location":"modules/#contribution","title":"Contribution","text":"<p>We recommend starting with the Terraform module documentation for a crash course in the way the CGD Toolkit is designed.</p> <p>Please follow these guidelines when developing a new module. These are also outlined in the pull-request template for Module additions.</p>"},{"location":"modules/#1-provider-configurations","title":"1. Provider Configurations","text":"<p>Modules should not define its own provider configurations. Required provider versions should be outlined in a <code>required_versions</code> block inside of a <code>terraform</code> block:</p> <pre><code>terraform {  \n    required_providers {    \n        aws = {      \n            source  = \"hashicorp/aws\"      \n            version = \"&gt;= 5.30.0\"    \n        }  \n    }\n}\n</code></pre>"},{"location":"modules/#2-dependency-inversion","title":"2. Dependency Inversion","text":"<p>It is fine if your module needs to declare significant networking or compute resources to run - the Cloud Game Development Toolkit is intended to be highly opinionated. At the same time, we require that modules support a significant level of dependency injection through variables to support diverse use cases. This is a simple consideration that is easier to incorporate from the beginning of module development rather than retroactively.</p> <p>For example, the Jenkins module can provision its own Elastic Container Service cluster, or it can deploy the Jenkins service onto an existing cluster passed via the <code>cluster_name</code> variable.</p>"},{"location":"modules/#3-assumptions-and-guarantees","title":"3. Assumptions and Guarantees","text":"<p>If your module requires certain input formats in order to function Terraform refers to these as \"assumptions.\"</p> <p>If your module provides certain outputs in a consistent format that other configurations should be able to rely on Terraform calls these \"guarantees.\"</p> <p>We recommend outlining your module's assumptions and guarantees prior to implementation by using Terraform custom conditions. These can be used to validate input variables, data blocks, resource attributes, and much more. They are incredibly powerful.</p>"},{"location":"modules/#4-third-party-software","title":"4. Third Party Software","text":"<p>The modules contained in the CGD Toolkit are designed to simplify infrastructure deployments of common game development workload. Naturally, modules may deploy third party applications - in these situations we require that deployments depend on existing licenses and distribution channels.</p> <p>If your module relies on a container or image that is not distributed through the CGD Toolkit we require a disclaimer and the usage of end-user credentials passed as a variable to the module. This repository is not to be used to redistribute software that may be subject to licensing or contractual agreements. </p> <p>If your module relies on a custom Amazon Machine Image (AMI) or container we ask that you provide a Packer template or Dockerfile in the assets directory and include instructions to create the image prior to infrastructure deployment.</p>"},{"location":"playbooks/","title":"Cloud Game Development Toolkit Playbooks","text":"<p>This directory contains automation scripts for configuring workloads after deployment. We've selected Ansible as our IT automation platform, but we envision this directory will contain SSM run documents, Chef recipes, and potentially just plain, old bash scripts in the long term.</p>"},{"location":"samples/","title":"Cloud Game Development Toolkit Samples","text":"<p>Samples represent a reference implementation that can be deployed to solve for a specific use-case or workload. These are Terraform configurations and integrations with other common AWS workloads and services. Each sample will provide its own documentation and instructions that follows the template below:</p>"},{"location":"samples/#1-predeployment","title":"1) Predeployment","text":"<p>In the predeployment phase the user is instructed to provision or take note of any necessary pre-existing resources. Creating SSL certificates or keypairs, provisioning Amazon Machine Images (AMIs) with Packer, or documenting existing resource IDs and names all fall into this phase.</p>"},{"location":"samples/#2-deployment","title":"2) Deployment","text":"<p>In the deployment phase the user is instructed to run <code>terraform apply</code> on one or more Terraform configurations with the appropriate variables.</p>"},{"location":"samples/#3-postdeployment","title":"3) Postdeployment","text":"<p>Finally, the postdeployment phase includes any Ansible playbooks or remote execution instructions for configuring the applications that have been deployed. These may be automated or manual steps.</p>"}]}