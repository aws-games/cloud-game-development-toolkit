{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to the Cloud Game Development Toolkit","text":"<p>The Cloud Game Development Toolkit (a.k.a. CGD Toolkit) is a collection of templates and configurations for deploying game development infrastructure and tools on AWS.</p> <p>Info</p> <p>This project is under active development and community contributions are welcomed!. If you would like to see something in this repository please create a feature request in the Issues tab. If you'd like to contribute, raise a pull request. You'll find our contribution guidelines here.</p>"},{"location":"index.html#introduction","title":"Introduction","text":"<p>The CGD Toolkit consists of three key components:</p> <ul> <li> <p> Assets</p> <p>Foundational resources such as image templates, configurations scripts, and CI/CD pipeline definitions for game development.</p> <p> Assets</p> </li> <li> <p> Modules</p> <p>Infrastructure as code (we use Terraform) for deploying common game dev workloads with best-practices by default. These typically consume assets.</p> <p> Modules</p> </li> <li> <p> Samples</p> <p>Opinionated implementations to address common use cases for expedited game studio setup and battle-tested scenarios from the community.</p> <p> Samples</p> </li> </ul>"},{"location":"contributing.html","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"contributing.html#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"contributing.html#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p>"},{"location":"contributing.html#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"contributing.html#building-and-testing-project-documetation","title":"Building and Testing Project Documetation","text":"<p>This project uses Material for MkDocs to generate versioned documentation automatically. Content for the documentation is auto-generated by referencing the markdown contained in the project's <code>README.md</code> files. </p> <p>Local development: - To build the project documentation, run: <code>make docs-local-docker VERSION=&lt;Semantic Release Version&gt;</code>. This will build the project using <code>./docs/Dockerfile</code> and host the documentation using <code>mkdocs serve</code> on port <code>8000</code>. </p> <p>Live documentation (GitHub Pages): - A Github workflow is used to build and deploy the documentation to the gh-pages branch: <code>.github/workflows/docs.yml</code></p>"},{"location":"contributing.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"contributing.html#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"contributing.html#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"assets/index.html","title":"Assets","text":"<p>Assets are reusable scripts, pipeline definitions, Dockerfiles, Packer templates, and other resources that might prove useful or are dependencies of any of the modules. Each asset contains its own documentation as a <code>README.md</code> file.</p> <p>Info</p> <p>Don't see an asset listed? Create a feature request for a new asset or learn how to contribute new assets to the project below</p> Asset Type Description Packer Templates Packer templates provide an easy way to build machine images for commonly used game dev infrastructure. Currently the project includes Packer templates for UE5 build agents for Linux and Windows, as well as a Packer template for building a Perforce Helix Core version control AMI. Jenkins Pipelines Jenkins Pipelines for common game dev automation workflows Ansible Playbooks Automation scripts for reusable system level configurations. Dockerfiles Dockerfiles for creating Docker images of commonly used game dev infrastructure. These are primarily used in scenarios where there aren't openly available pre-built images that address a use case, or significant customization is needed that warrants building an image"},{"location":"assets/dockerfiles.html","title":"Dockerfiles","text":""},{"location":"assets/jenkins-pipelines.html","title":"Jenkins Pipelines","text":""},{"location":"assets/packer.html","title":"Packer Templates","text":"<ul> <li>Unreal Engine Build Agents: Packer templates for building Unreal Engine on Windows and Linux, including support for x86 and ARM (i.e. Graviton).</li> <li>Perforce Helix Core AMI: Packer templates for creating an Amazon Machine Image (AMI) of Perforce Helix Core on Linux and configuration with Perforce Server Deployment Package (SDP)</li> </ul>"},{"location":"assets/packer.html#unreal-engine-build-agents","title":"Unreal Engine Build Agents","text":"<p>Packer templates to create machine images for Unreal Engine build agents.</p> Asset Description Type Docs Unreal Engine Build Agent (Linux) Packer templates for building Unreal Engine on Linux, with support for x86 and ARM (i.e. Graviton). Packer Template Docs Unreal Engine Build Agent (Windows) Packer templates for building Unreal Engine on Windows Packer Template Docs"},{"location":"assets/packer.html#packer-templates-for-unreal-engine-linux-build-agents","title":"Packer templates for Unreal Engine Linux build agents","text":"<p>The following templates provide Unreal Engine Linux build agents:</p> Operating system CPU architecture file location Ubuntu Jammy 22.04 x86_64 (a.k.a. amd64) <code>x86_64/ubuntu-jammy-22.04-amd64-server.pkr.hcl</code> Ubuntu Jammy 22.04 aarch64 (a.k.a. arm64) <code>aarch64/ubuntu-jammy-22.04-arm64-server.pkr.hcl</code> Amazon Linux 2023 x86_64 (a.k.a. amd64) <code>x86_64/amazon-linux-2023-x86_64.pkr.hcl</code> Amazon Linux 2023 aarch64 (a.k.a. arm64) <code>aarch64/amazon-linux-2023-arm64.pkr.hcl</code>"},{"location":"assets/packer.html#usage","title":"Usage","text":"<ol> <li>Make a copy of <code>example.pkrvars.hcl</code> and adjust the input variables as needed</li> <li>Ensure you have active AWS credentials</li> <li>Invoke <code>packer build --var-file=&lt;your .pkrvars.hcl file&gt; &lt;path to .pkr.hcl file&gt;</code>, then wait for the build to complete.</li> </ol>"},{"location":"assets/packer.html#software-packages-included","title":"Software packages included","text":"<p>The templates install various software packages:</p> <p>common tools</p> <p>Some common tools are installed to enable installing other software, performing maintenance tasks, and compile some C++ software:</p> <ul> <li>git</li> <li>curl</li> <li>jq</li> <li>unzip</li> <li>dos2unix</li> <li>AWS CLI v2</li> <li>AWS Systems Manager Agent</li> <li>Amazon Corretto</li> <li>mount.nfs, to be able to mount FSx volumes over NFS</li> <li>python3</li> <li>python3 packages: 'pip', 'requests', 'boto3' and 'botocore'</li> <li>clang</li> <li>cmake3</li> <li>scons</li> <li>Development libraries for compiling the Amazon GameLift Server SDK for C++</li> <li>Development libraries for compiling the Godot 4 game engine (if available in the OS's package manager)</li> </ul> <p>mold</p> <p>The 'mold' linker is installed to enable faster linking.</p> <p>FSx automounter service</p> <p>The FSx automounter systemd service is a service written in Python that automatically mounts FSx for OpenZFS volumes on instance bootup. The service uses resource tags on FSx volumes to determine if and where to mount volumes on.</p> <p>You can use the following tags on FSx volumes: * 'automount-fsx-volume-name' tag: specifies the name of the local mount point. The mount point specified will be prefixed with 'fsx_' by the service. * 'automount-fsx-volume-on' tag: This tag contains a space-delimited list of EC2 instance names on which the volume will be automatically mounted by this service (if it is running on that instance).</p> <p>For example, if the FSx automounter service is running on an EC2 instance with Name tag 'ubuntu-builder', and an FSx volume has tag <code>automount-fsx-volume-on</code>=<code>al2023-builder ubuntu-builder</code> and tag <code>automount-fsx-volume-name</code>=<code>workspace</code>, then the automounter will automatically mount that volume on <code>/mnt/fsx_workspace</code>.</p> <p>Note that the automounter service makes use of the ListTagsForResource FSx API call, which is rate-limited. If you intend to scale up hundreds of EC2 instances that are running this service, then we recommend automatically mounting FSx volumes using <code>/etc/fstab</code>.</p> <p>mount_ephemeral service</p> <p>The mount_ephemeral service is a systemd service written as a simple bash script that mounts NVMe attached instance storage volume automatically as temporary storage. It does this by formatting <code>/dev/nvme1n1</code> as xfs and then mounting it on <code>/tmp</code>. This service runs on instance bootup.</p> <p>create_swap service</p> <p>The create_swap service is a systemd service written as a simple bash script that creates a 1GB swap file on <code>/swapfile</code>. This service runs on instance bootup.</p> <p>sccache</p> <p>'sccache' is installed to cache c/c++ compilation artefacts, which can speed up builds by avoiding unneeded work.</p> <p>sccache is installed as a systemd service, and configured to use <code>/mnt/fsx_cache/sccache</code> as its cache folder. The service expects this folder to be available or set up by another service.</p> <p>octobuild</p> <p>'Octobuild' is installed to act as a compilation cache for Unreal Engine.</p> <p>Octobuild is configured (in octobuild.conf) to use <code>/mnt/fsx_cache/octobuild_cache</code> as its cache folder, and expects this folder to be available or set up by another service.</p> <p>NOTE: Octobuild is not supported on aarch64, and therefore not installed there.</p>"},{"location":"assets/packer.html#processor-architectures-and-naming-conventions","title":"Processor architectures and naming conventions","text":"<p>Within this folder, the processor architecture naming conventions as reported by <code>uname -m</code> are used, hence why there are scripts here with names containing \"x86_64\" or \"aarch64\". The packer template <code>.hcl</code> files are named following the naming conventions of the operating system that they are based on. Unfortunately, because some operating systems don't use the same terminology in their naming conventions throughout, this means that you'll see this lack of consistency here has well.</p>"},{"location":"assets/packer.html#unreal-engine-build-agent-windows","title":"Unreal Engine Build Agent (Windows)","text":"<p>Documentation for UE Build Agents for Windows.</p>"},{"location":"assets/packer.html#perforce-helix-core-packer-template","title":"Perforce Helix Core Packer Template","text":""},{"location":"assets/packer.html#overview","title":"Overview","text":"<p>This Packer template automates the deployment and configuration of a Helix Core Server (P4D) on a Linux environment, specifically tailored for use with SELinux and systemd. It performs various tasks such as checking and setting up the necessary user and group, handling SELinux context, installing and configuring Perforce Software's Server Deployment Package (SDP), and setting up the Helix Core service with systemd.</p> <p>Steps in the Packer template automation include: </p> <ol> <li>Pre-Flight Checks: Ensures the script is run with root privileges.</li> <li>Environment Setup: Defines paths and necessary constants for the installation.</li> <li>SELinux Handling: Checks if SELinux is enabled and installs required packages.</li> <li>User and Group Verification: Ensures the 'perforce' user and group exist.</li> <li>Directory Creation and Ownership: Ensures necessary directories exist and have correct ownership.</li> <li>Helix Binaries and SDP Installation: Downloads and extracts SDP, checks for Helix binaries, and downloads them if missing.</li> <li>Systemd Service Configuration: Sets up a systemd service for the p4d server.</li> <li>SSL Configuration: Updates SSL certificate configuration with the EC2 instance DNS name.</li> <li>SELinux Context Management: Updates SELinux context for p4d.</li> <li>Crontab Initialization: Sets up crontab for the 'perforce' user.</li> <li>SDP Verification: Runs a script to verify the SDP installation.</li> </ol>"},{"location":"assets/packer.html#prerequisites","title":"Prerequisites","text":"<ul> <li>A Linux system with DNF package manager (e.g., Fedora, RHEL, CentOS).</li> <li>Root access to the system.</li> <li>SELinux in Enforcing or Permissive mode (optional but recommended).</li> <li>Access to the internet for downloading necessary packages and binaries.</li> </ul>"},{"location":"assets/packer.html#how-to-use","title":"How to Use","text":"<ol> <li>Download the Script: Clone or download this repository to your system.</li> <li>Provide Execution Permission: Give execute permission to the script using <code>chmod +x &lt;script_name&gt;.sh</code>.</li> <li>Run the Script: Execute the script as root:</li> </ol> <p><code>sudo ./&lt;script_name&gt;.sh</code></p> <ol> <li>Follow the On-Screen Instructions: The script is mostly automated, but monitor the output for any errors or required manual inputs.</li> </ol>"},{"location":"assets/packer.html#important-notes","title":"Important Notes","text":"<ul> <li>This script is designed for a specific use-case and might require modifications for different environments or requirements.</li> <li>Ensure you have a backup of your system before running the script, as it makes significant changes to users, groups, and services.</li> <li>The script assumes an internet connection for downloading packages and binaries.</li> </ul>"},{"location":"assets/packer.html#packer-ci","title":"Packer CI","text":"<p>This README provides steps for running basic CI on Packer templates contained in the project.</p>"},{"location":"assets/packer.html#configuration","title":"Configuration","text":"<p>In the root of the <code>./packer</code> directory is a configuration yaml file (<code>.config.yml</code>) that contains the location of each of the Packer templates in the project. When new Packer templates are added, the config file should be updated as well.</p>"},{"location":"assets/packer.html#configyml","title":"<code>.config.yml</code>","text":"<p>``` title=\".confi.yml\"</p>"},{"location":"assets/packer.html#packer-template-locations-relative-to-packer-directory","title":"Packer template locations relative to ./packer directory","text":"<p>packer_templates:   - description: 'Amazon Linux 2023 Jenkins Builder (x86)'     file_name: 'amazon-linux-2023-x86_64.pkr.hcl'     dir: 'build-agents/linux'   - description: 'Amazon Linux 2023 Jenkins Builder (ARM)'     file_name: 'amazon-linux-2023-arm64.pkr.hcl'     dir: 'build-agents/linux'   - description: 'Ubuntu 22.04 LTS Jenkins Builder (x86)'     file_name: 'ubuntu-jammy-22.04-amd64-server.pkr.hcl'     dir: 'build-agents/linux'   - description: 'Ubuntu 22.04 LTS Jenkins Builder (ARM)'     file_name: 'ubuntu-jammy-22.04-arm64-server.pkr.hcl'     dir: 'build-agents/linux'   - description: 'Windows Server 2022 Jenkins Builder (x86)'     file_name: 'windows.pkr.hcl'     dir: 'build-agents/windows'   - description: 'Perforce Helix Core Server (x86)'     file_name: 'perforce.pkr.hcl'     dir: 'perforce/helix-core'</p> <pre><code>\n```bash\ndocker build -t packer-ci -f .ci/Dockerfile . \\             \n--build-arg AWS_REGION=us-east-1 \\\n--build-arg AWS_VPC_ID=vpc-086839c0e28ad1f29 \\\n--build-arg AWS_SUBNET_ID=subnet-0e6bb0e5c155610c0 \\\n--build-arg AWS_PROFILE=default \\\n--build-arg PUBLIC_KEY=Key_Pair_Linux\n</code></pre>"},{"location":"assets/packer.html#packer-directory","title":"Packer Directory","text":"<pre><code>.\n\u251c\u2500\u2500 build-agents\n\u2502   \u251c\u2500\u2500 linux\n\u2502   \u2502   \u251c\u2500\u2500 amazon-linux-2023-arm64.pkr.hcl\n\u2502   \u2502   \u251c\u2500\u2500 amazon-linux-2023-x86_64.pkr.hcl\n\u2502   \u2502   \u251c\u2500\u2500 create_swap.service\n\u2502   \u2502   \u251c\u2500\u2500 create_swap.sh\n\u2502   \u2502   \u251c\u2500\u2500 example.pkrvars.hcl\n\u2502   \u2502   \u251c\u2500\u2500 fsx_automounter.py\n\u2502   \u2502   \u251c\u2500\u2500 fsx_automounter.service\n\u2502   \u2502   \u251c\u2500\u2500 install_common.al2023.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_common.ubuntu.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_mold.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_octobuild.al2023.x86_64.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_octobuild.ubuntu.x86_64.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_sccache.sh\n\u2502   \u2502   \u251c\u2500\u2500 mount_ephemeral.service\n\u2502   \u2502   \u251c\u2500\u2500 mount_ephemeral.sh\n\u2502   \u2502   \u251c\u2500\u2500 octobuild.conf\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 sccache.service\n\u2502   \u2502   \u251c\u2500\u2500 ubuntu-jammy-22.04-amd64-server.pkr.hcl\n\u2502   \u2502   \u2514\u2500\u2500 ubuntu-jammy-22.04-arm64-server.pkr.hcl\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 windows\n\u2502       \u251c\u2500\u2500 base_setup.ps1\n\u2502       \u251c\u2500\u2500 example.pkrvars.hcl\n\u2502       \u251c\u2500\u2500 install_vs_tools.ps1\n\u2502       \u251c\u2500\u2500 setup_jenkins_agent.ps1\n\u2502       \u251c\u2500\u2500 userdata.ps1\n\u2502       \u2514\u2500\u2500 windows.pkr.hcl\n\u251c\u2500\u2500 .ci\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 packer-ci.md\n\u2502   \u251c\u2500\u2500 packer-validate.sh\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 setup.sh\n\u251c\u2500\u2500 .config.yml\n\u2514\u2500\u2500 perforce\n    \u2514\u2500\u2500 helix-core\n        \u251c\u2500\u2500 example.pkrvars.hcl\n        \u251c\u2500\u2500 p4_configure.sh\n        \u251c\u2500\u2500 p4_setup.sh\n        \u251c\u2500\u2500 perforce.pkr.hcl\n        \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"assets/playbooks.html","title":"Cloud Game Development Toolkit Playbooks","text":"<p>This directory contains automation scripts for configuring workloads after deployment. We've selected Ansible as our IT automation platform, but we envision this directory will contain SSM run documents, Chef recipes, and potentially just plain, old bash scripts in the long term.</p>"},{"location":"ci/index.html","title":"CI and Testing","text":"<p>This project uses Github Actions to automate continuous integration (CI) testing using utilities contained in the <code>.ci/</code> directories within the project's assets, modules, and samples. Dockerfiles are included to simplify running these CI workflows locally in your development environment or in a cloud CI environment. </p>"},{"location":"ci/index.html#example-ci-directory-packerci","title":"Example CI directory: <code>packer/.ci/</code>","text":"<pre><code>.ci/\n\u251c\u2500\u2500 Dockerfile &lt;------------------ Dockerfile for running Packer CI\n\u251c\u2500\u2500 packer-validate.sh &lt;---------- Script for linting with Packer Validate\n\u251c\u2500\u2500 README.md &lt;------------------- Instructions for Packer CI\n\u2514\u2500\u2500 setup.sh &lt;-------------------- Commands to setup environment (i.e install Packer)\n</code></pre>"},{"location":"ci/packer.html","title":"Packer CI","text":"<p>This README provides steps for running basic CI on Packer templates contained in the project.</p>"},{"location":"ci/packer.html#configuration","title":"Configuration","text":"<p>In the root of the <code>./packer</code> directory is a configuration yaml file (<code>.config.yml</code>) that contains the location of each of the Packer templates in the project. When new Packer templates are added, the config file should be updated as well.</p>"},{"location":"ci/packer.html#configyml","title":"<code>.config.yml</code>","text":"<p>``` title=\".confi.yml\"</p>"},{"location":"ci/packer.html#packer-template-locations-relative-to-packer-directory","title":"Packer template locations relative to ./packer directory","text":"<p>packer_templates:   - description: 'Amazon Linux 2023 Jenkins Builder (x86)'     file_name: 'amazon-linux-2023-x86_64.pkr.hcl'     dir: 'build-agents/linux'   - description: 'Amazon Linux 2023 Jenkins Builder (ARM)'     file_name: 'amazon-linux-2023-arm64.pkr.hcl'     dir: 'build-agents/linux'   - description: 'Ubuntu 22.04 LTS Jenkins Builder (x86)'     file_name: 'ubuntu-jammy-22.04-amd64-server.pkr.hcl'     dir: 'build-agents/linux'   - description: 'Ubuntu 22.04 LTS Jenkins Builder (ARM)'     file_name: 'ubuntu-jammy-22.04-arm64-server.pkr.hcl'     dir: 'build-agents/linux'   - description: 'Windows Server 2022 Jenkins Builder (x86)'     file_name: 'windows.pkr.hcl'     dir: 'build-agents/windows'   - description: 'Perforce Helix Core Server (x86)'     file_name: 'perforce.pkr.hcl'     dir: 'perforce/helix-core'</p> <pre><code>\n```bash\ndocker build -t packer-ci -f .ci/Dockerfile . \\             \n--build-arg AWS_REGION=us-east-1 \\\n--build-arg AWS_VPC_ID=vpc-086839c0e28ad1f29 \\\n--build-arg AWS_SUBNET_ID=subnet-0e6bb0e5c155610c0 \\\n--build-arg AWS_PROFILE=default \\\n--build-arg PUBLIC_KEY=Key_Pair_Linux\n</code></pre>"},{"location":"ci/packer.html#packer-directory","title":"Packer Directory","text":"<pre><code>.\n\u251c\u2500\u2500 build-agents\n\u2502   \u251c\u2500\u2500 linux\n\u2502   \u2502   \u251c\u2500\u2500 amazon-linux-2023-arm64.pkr.hcl\n\u2502   \u2502   \u251c\u2500\u2500 amazon-linux-2023-x86_64.pkr.hcl\n\u2502   \u2502   \u251c\u2500\u2500 create_swap.service\n\u2502   \u2502   \u251c\u2500\u2500 create_swap.sh\n\u2502   \u2502   \u251c\u2500\u2500 example.pkrvars.hcl\n\u2502   \u2502   \u251c\u2500\u2500 fsx_automounter.py\n\u2502   \u2502   \u251c\u2500\u2500 fsx_automounter.service\n\u2502   \u2502   \u251c\u2500\u2500 install_common.al2023.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_common.ubuntu.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_mold.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_octobuild.al2023.x86_64.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_octobuild.ubuntu.x86_64.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_sccache.sh\n\u2502   \u2502   \u251c\u2500\u2500 mount_ephemeral.service\n\u2502   \u2502   \u251c\u2500\u2500 mount_ephemeral.sh\n\u2502   \u2502   \u251c\u2500\u2500 octobuild.conf\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 sccache.service\n\u2502   \u2502   \u251c\u2500\u2500 ubuntu-jammy-22.04-amd64-server.pkr.hcl\n\u2502   \u2502   \u2514\u2500\u2500 ubuntu-jammy-22.04-arm64-server.pkr.hcl\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 windows\n\u2502       \u251c\u2500\u2500 base_setup.ps1\n\u2502       \u251c\u2500\u2500 example.pkrvars.hcl\n\u2502       \u251c\u2500\u2500 install_vs_tools.ps1\n\u2502       \u251c\u2500\u2500 setup_jenkins_agent.ps1\n\u2502       \u251c\u2500\u2500 userdata.ps1\n\u2502       \u2514\u2500\u2500 windows.pkr.hcl\n\u251c\u2500\u2500 .ci\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 packer-ci.md\n\u2502   \u251c\u2500\u2500 packer-validate.sh\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 setup.sh\n\u251c\u2500\u2500 .config.yml\n\u2514\u2500\u2500 perforce\n    \u2514\u2500\u2500 helix-core\n        \u251c\u2500\u2500 example.pkrvars.hcl\n        \u251c\u2500\u2500 p4_configure.sh\n        \u251c\u2500\u2500 p4_setup.sh\n        \u251c\u2500\u2500 perforce.pkr.hcl\n        \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"modules/index.html","title":"Modules","text":""},{"location":"modules/index.html#introduction","title":"Introduction","text":"<p>These modules simplify the deployment of common game development workloads on AWS. Some have pre-requisites that will be outlined in their respective documentation. They are designed to easily integrate with each other, and provide relevant outputs to simplify permissions, networking, and access.</p>"},{"location":"modules/index.html#contribution","title":"Contribution","text":"<p>We recommend starting with the Terraform module documentation for a crash course in the way the CGD Toolkit is designed.</p> <p>Please follow these guidelines when developing a new module. These are also outlined in the pull-request template for Module additions.</p>"},{"location":"modules/index.html#1-provider-configurations","title":"1. Provider Configurations","text":"<p>Modules should not define its own provider configurations. Required provider versions should be outlined in a <code>required_versions</code> block inside of a <code>terraform</code> block:</p> <pre><code>terraform {  \n    required_providers {    \n        aws = {      \n            source  = \"hashicorp/aws\"      \n            version = \"&gt;= 5.30.0\"    \n        }  \n    }\n}\n</code></pre>"},{"location":"modules/index.html#2-dependency-inversion","title":"2. Dependency Inversion","text":"<p>It is fine if your module needs to declare significant networking or compute resources to run - the Cloud Game Development Toolkit is intended to be highly opinionated. At the same time, we require that modules support a significant level of dependency injection through variables to support diverse use cases. This is a simple consideration that is easier to incorporate from the beginning of module development rather than retroactively.</p> <p>For example, the Jenkins module can provision its own Elastic Container Service cluster, or it can deploy the Jenkins service onto an existing cluster passed via the <code>cluster_name</code> variable.</p>"},{"location":"modules/index.html#3-assumptions-and-guarantees","title":"3. Assumptions and Guarantees","text":"<p>If your module requires certain input formats in order to function Terraform refers to these as \"assumptions.\"</p> <p>If your module provides certain outputs in a consistent format that other configurations should be able to rely on Terraform calls these \"guarantees.\"</p> <p>We recommend outlining your module's assumptions and guarantees prior to implementation by using Terraform custom conditions. These can be used to validate input variables, data blocks, resource attributes, and much more. They are incredibly powerful.</p>"},{"location":"modules/index.html#4-third-party-software","title":"4. Third Party Software","text":"<p>The modules contained in the CGD Toolkit are designed to simplify infrastructure deployments of common game development workload. Naturally, modules may deploy third party applications - in these situations we require that deployments depend on existing licenses and distribution channels.</p> <p>If your module relies on a container or image that is not distributed through the CGD Toolkit we require a disclaimer and the usage of end-user credentials passed as a variable to the module. This repository is not to be used to redistribute software that may be subject to licensing or contractual agreements.</p> <p>If your module relies on a custom Amazon Machine Image (AMI) or container we ask that you provide a Packer template or Dockerfile in the assets directory and include instructions to create the image prior to infrastructure deployment.</p>"},{"location":"modules/jenkins/index.html","title":"Jenkins Module","text":"<p>Jenkins is an open source automation server. It helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration, and continuous delivery.</p> <p>This module deploys the following resources:</p> <ul> <li>An Elastic Container Service (ECS) cluster backed by AWS Fargate.</li> <li>An ECS service running the latest Jenkins container (jenkins/jenkins:lts-jdk17) available.</li> <li>An Elastic File System (EFS) for the Jenkins service to use as a persistent datastore.</li> <li>An Elastic Load Balancer (ELB) for TLS termination of the Jenkins service</li> <li>A configurable number of EC2 Autoscaling groups to serve as a flexible pool of build nodes for the Jenkins service</li> <li>Supporting resources including KMS keys for encryption and IAM roles to ensure security best practices</li> </ul>"},{"location":"modules/jenkins/index.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws &gt;= 5.30 random &gt;=3.6"},{"location":"modules/jenkins/index.html#providers","title":"Providers","text":"Name Version aws &gt;= 5.30 random &gt;=3.6"},{"location":"modules/jenkins/index.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/jenkins/index.html#resources","title":"Resources","text":"Name Type aws_autoscaling_group.jenkins_build_farm_asg resource aws_cloudwatch_log_group.jenkins_service_log_group resource aws_ecs_cluster.jenkins_cluster resource aws_ecs_cluster_capacity_providers.jenkins_cluster_fargate_rpvodiers resource aws_ecs_service.jenkins_service resource aws_ecs_task_definition.jenkins_task_definition resource aws_efs_access_point.jenkins_efs_access_point resource aws_efs_file_system.jenkins_efs_file_system resource aws_efs_mount_target.jenkins_efs_mount_target resource aws_fsx_openzfs_file_system.jenkins_build_farm_fsxz_file_system resource aws_iam_instance_profile.build_farm_instance_profile resource aws_iam_policy.build_farm_fsxz_policy resource aws_iam_policy.build_farm_s3_policy resource aws_iam_policy.ec2_fleet_plugin_policy resource aws_iam_policy.jenkins_default_policy resource aws_iam_role.build_farm_role resource aws_iam_role.jenkins_default_role resource aws_iam_role.jenkins_task_execution_role resource aws_iam_role_policy_attachment.ec2_fleet_plugin_policy_attachment resource aws_launch_template.jenkins_build_farm_launch_template resource aws_lb.jenkins_alb resource aws_lb_listener.jenkins_alb_https_listener resource aws_lb_target_group.jenkins_alb_target_group resource aws_s3_bucket.artifact_buckets resource aws_s3_bucket.jenkins_alb_access_logs_bucket resource aws_security_group.jenkins_alb_sg resource aws_security_group.jenkins_build_farm_sg resource aws_security_group.jenkins_build_storage_sg resource aws_security_group.jenkins_efs_security_group resource aws_security_group.jenkins_service_sg resource aws_vpc_security_group_egress_rule.jenkins_alb_outbound_service resource aws_vpc_security_group_egress_rule.jenkins_service_outbound_ipv4 resource aws_vpc_security_group_egress_rule.jenkins_service_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.jenkins_build_farm_inbound_ssh_service resource aws_vpc_security_group_ingress_rule.jenkins_build_vpc_all_traffic resource aws_vpc_security_group_ingress_rule.jenkins_efs_inbound_service resource aws_vpc_security_group_ingress_rule.jenkins_service_inbound_alb resource random_string.artifact_buckets resource random_string.build_farm resource random_string.fsxz resource random_string.jenkins resource random_string.jenkins_alb_access_logs_bucket_suffix resource aws_caller_identity.current data source aws_ecs_cluster.jenkins_cluster data source aws_iam_policy_document.build_farm_fsxz_policy data source aws_iam_policy_document.build_farm_s3_policy data source aws_iam_policy_document.ec2_fleet_plugin_policy data source aws_iam_policy_document.ec2_trust_relationship data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.jenkins_default_policy data source aws_region.current data source aws_vpc.build_farm_vpc data source"},{"location":"modules/jenkins/index.html#inputs","title":"Inputs","text":"Name Description Type Default Required artifact_buckets List of Amazon S3 buckets you wish to create to store build farm artifacts. <pre>map(    object({      name                 = string      enable_force_destroy = optional(bool, true)  }))</pre> <code>null</code> no build_farm_compute Each object in this map corresponds to an ASG used by Jenkins as build agents. <pre>map(object(    {      ami = string      #TODO: Support mixed instances / spot with custom policies      instance_type     = string      ebs_optimized     = optional(bool, true)      enable_monitoring = optional(bool, true)    }  ))</pre> <code>{}</code> no build_farm_fsx_openzfs_storage Each object in this map corresponds to an FSx OpenZFS file system used by the Jenkins build agents. <pre>map(object(    {      storage_capacity    = number      throughput_capacity = number      storage_type        = optional(string, \"SSD\") # \"SSD\", \"HDD\"      deployment_type     = optional(string, \"SINGLE_AZ_1\")      route_table_ids     = optional(list(string), null)    }  ))</pre> <code>{}</code> no build_farm_subnets The subnets to deploy the build farms into. <code>list(string)</code> n/a yes certificate_arn The TLS certificate ARN for the Jenkins service load balancer. <code>string</code> n/a yes cluster_name The ARN of the cluster to deploy the Jenkins service into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no container_cpu The CPU allotment for the Jenkins container. <code>number</code> <code>1024</code> no container_memory The memory allotment for the Jenkins container. <code>number</code> <code>4096</code> no container_name The name of the Jenkins service container. <code>string</code> <code>\"jenkins-container\"</code> no container_port The container port used by the Jenkins service container. <code>number</code> <code>8080</code> no create_ec2_fleet_plugin_policy Optional creation of IAM Policy required for Jenkins EC2 Fleet plugin. Default is set to false. <code>bool</code> <code>false</code> no create_jenkins_default_policy Optional creation of Jenkins Default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_jenkins_default_role Optional creation of Jenkins Default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_jenkins_role ARN of the custom IAM Role you wish to use with Jenkins. <code>string</code> <code>null</code> no enable_jenkins_alb_access_logs Enables access logging for the Jenkins ALB. Defaults to false. <code>bool</code> <code>false</code> no enable_jenkins_alb_deletion_protection Enables deletion protection for the Jenkins ALB. Defaults to false. <code>bool</code> <code>false</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_artifact_buckets List of ARNs of the S3 buckets used to store artifacts created by the build farm. <code>list(string)</code> <code>[]</code> no existing_security_groups A list of existing security group IDs to attach to the Jenkins service load balancer. <code>list(string)</code> <code>null</code> no internal Set this flag to true if you do not want the Jenkins service load balancer to have a public IP. <code>bool</code> <code>false</code> no jenkins_agent_secret_arns A list of secretmanager ARNs (wildcards allowed) that contain any secrets which need to be accessed by the Jenkins service. <code>list(string)</code> <code>null</code> no jenkins_alb_access_logs_bucket ID of the S3 bucket for Jenkins ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no jenkins_alb_access_logs_prefix Log prefix for Jenkins ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no jenkins_alb_subnets A list of subnets to deploy the Jenkins load balancer into. Public subnets are recommended. <code>list(string)</code> n/a yes jenkins_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Jenkins. <code>string</code> <code>365</code> no jenkins_efs_performance_mode The performance mode of the EFS file system used by the Jenkins service. Defaults to general purpose. <code>string</code> <code>\"generalPurpose\"</code> no jenkins_efs_throughput_mode The throughput mode of the EFS file system used by the Jenkins service. Defaults to bursting. <code>string</code> <code>\"bursting\"</code> no jenkins_service_desired_container_count The desired number of containers running the Jenkins service. <code>number</code> <code>1</code> no jenkins_service_subnets A list of subnets to deploy the Jenkins service into. Private subnets are recommended. <code>list(string)</code> n/a yes name The name attached to Jenkins module resources. <code>string</code> <code>\"jenkins\"</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IAC_MANAGEMENT\": \"CGD-Toolkit\",  \"IAC_MODULE\": \"Jenkins\",  \"IAC_PROVIDER\": \"Terraform\"}</pre> no vpc_id The ID of the existing VPC you would like to deploy the Jenkins service and build farms into. <code>string</code> n/a yes"},{"location":"modules/jenkins/index.html#outputs","title":"Outputs","text":"Name Description alb_security_group Security group associated with the Jenkins load balancer build_farm_security_group Security group associated with the build farm autoscaling groups jenkins_alb_dns_name The DNS name of the Jenkins application load balancer. jenkins_alb_zone_id The zone ID of the Jenkins ALB. service_security_group Security group associated with the ECS service hosting jenkins"},{"location":"modules/perforce/helix-authentication-service.html","title":"Helix Authentication Service (HAS)","text":"<p>Perforce Helix Authentication Service enables you to integrate certain Perforce products with your organization's Identity Provider (IdP). </p> <p>This module deploys Perforce Helix Authentication Service on AWS as a fully managed ECS Service using Fargate.</p>"},{"location":"modules/perforce/helix-authentication-service.html#instructiosn-for-deployment","title":"Instructiosn for Deployment","text":"<p>They Go Here</p>"},{"location":"modules/perforce/helix-authentication-service.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws &gt;= 5.30 awscc &gt;= 1.2.0 random &gt;=3.6"},{"location":"modules/perforce/helix-authentication-service.html#providers","title":"Providers","text":"Name Version aws &gt;= 5.30 awscc &gt;= 1.2.0 random &gt;=3.6"},{"location":"modules/perforce/helix-authentication-service.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/helix-authentication-service.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.helix_authentication_service_log_group resource aws_ecs_cluster.helix_authentication_service_cluster resource aws_ecs_cluster_capacity_providers.helix_authentication_service_cluster_fargate_providers resource aws_ecs_service.helix_authentication_service resource aws_ecs_task_definition.helix_authentication_service_task_definition resource aws_iam_policy.helix_authentication_service_default_policy resource aws_iam_policy.helix_authentication_service_secrets_manager_policy resource aws_iam_role.helix_authentication_service_default_role resource aws_iam_role.helix_authentication_service_task_execution_role resource aws_lb.helix_authentication_service_alb resource aws_lb_listener.helix_authentication_service_alb_https_listener resource aws_lb_target_group.helix_authentication_service_alb_target_group resource aws_s3_bucket.helix_authentication_service_alb_access_logs_bucket resource aws_security_group.helix_authentication_service_alb_sg resource aws_security_group.helix_authentication_service_sg resource aws_vpc_security_group_egress_rule.helix_authentication_service_alb_outbound_service resource aws_vpc_security_group_egress_rule.helix_authentication_service_outbound_ipv4 resource aws_vpc_security_group_egress_rule.helix_authentication_service_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.helix_authentication_service_inbound_alb resource awscc_secretsmanager_secret.helix_authentication_service_admin_password resource awscc_secretsmanager_secret.helix_authentication_service_admin_username resource random_string.helix_authentication_service resource random_string.helix_authentication_service_alb_access_logs_bucket_suffix resource aws_ecs_cluster.helix_authentication_service_cluster data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.helix_authentication_service_default_policy data source aws_iam_policy_document.helix_authentication_service_secrets_manager_policy data source aws_region.current data source"},{"location":"modules/perforce/helix-authentication-service.html#inputs","title":"Inputs","text":"Name Description Type Default Required certificate_arn The TLS certificate ARN for the Helix Authentication Service load balancer. <code>string</code> n/a yes cluster_name The name of the cluster to deploy the Helix Authentication Service into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no container_cpu The CPU allotment for the Helix Authentication Service container. <code>number</code> <code>1024</code> no container_memory The memory allotment for the Helix Authentication Service container. <code>number</code> <code>4096</code> no container_name The name of the Helix Authentication Service container. <code>string</code> <code>\"helix-auth-container\"</code> no container_port The container port that Helix Authentication Service runs on. <code>number</code> <code>3000</code> no create_helix_authentication_service_default_policy Optional creation of Helix Authentication Service default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_helix_authentication_service_default_role Optional creation of Helix Authentication Service default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_helix_authentication_service_role ARN of the custom IAM Role you wish to use with Helix Authentication Service. <code>string</code> <code>null</code> no desired_container_count The desired number of containers running the Helix Authentication Service. <code>number</code> <code>1</code> no enable_helix_authentication_service_alb_access_logs Enables access logging for the Helix Authentication Service ALB. Defaults to false. <code>bool</code> <code>false</code> no enable_helix_authentication_service_alb_deletion_protection Enables deletion protection for the Helix Authentication Service ALB. Defaults to false. <code>bool</code> <code>false</code> no enable_web_based_administration Flag for enabling web based administration of Helix Authentication Service. <code>bool</code> <code>false</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_security_groups A list of existing security group IDs to attach to the Helix Authentication Service load balancer. <code>list(string)</code> <code>[]</code> no fqdn The fully qualified domain name of Helix Authentication Service. <code>string</code> <code>\"localhost\"</code> no helix_authentication_service_admin_password_secret_arn Optionally provide the ARN of an AWS Secret for the Helix Authentication Service Administrator password. <code>string</code> <code>null</code> no helix_authentication_service_admin_username_secret_arn Optionally provide the ARN of an AWS Secret for the Helix Authentication Service Administrator username. <code>string</code> <code>null</code> no helix_authentication_service_alb_access_logs_bucket ID of the S3 bucket for Helix Authentication Service ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no helix_authentication_service_alb_access_logs_prefix Log prefix for Helix Authentication Service ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no helix_authentication_service_alb_subnets A list of subnets to deploy the Helix Authentication Service load balancer into. Public subnets are recommended. <code>list(string)</code> n/a yes helix_authentication_service_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Helix Authentication Service. <code>string</code> <code>365</code> no helix_authentication_service_subnets A list of subnets to deploy the Helix Authentication Service into. Private subnets are recommended. <code>list(string)</code> n/a yes internal Set this flag to true if you do not want the Helix Authentication Service load balancer to have a public IP. <code>bool</code> <code>false</code> no name The name attached to Helix Authentication Service module resources. <code>string</code> <code>\"helix-auth-svc\"</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IAC_MANAGEMENT\": \"CGD-Toolkit\",  \"IAC_MODULE\": \"helix-authentication-service\",  \"IAC_PROVIDER\": \"Terraform\"}</pre> no vpc_id The ID of the existing VPC you would like to deploy Helix Authentication Service into. <code>string</code> n/a yes"},{"location":"modules/perforce/helix-authentication-service.html#outputs","title":"Outputs","text":"Name Description alb_dns_name The DNS name of the Helix Authentication Service ALB alb_security_group_id Security group associated with the Helix Authentication Service load balancer alb_zone_id The hosted zone ID of the Helix Authentication Service ALB cluster_name Name of the ECS cluster hosting helix_authentication_service service_security_group_id Security group associated with the ECS service running Helix Authentication Service"},{"location":"modules/perforce/helix-core.html","title":"Perforce Helix Core","text":"<p>Perforce Helix Core is a scalable version control system that helps teams manage code alongside large, digital assets and collaborate more effectively in one central, secure location. With AWS, teams can quickly deploy Helix Core and accelerate innovation.</p> <p>This module deploys Perforce Helix Core on AWS using an Amazon Machine Image (AMI) that is included in the Cloud Game Development Toolkit. You can provision this AMI using Hashicorp Packer. To get started, navigate to <code>/assets/packer/perforce/helix-core</code> and consult the documentation.</p>"},{"location":"modules/perforce/helix-core.html#overview","title":"overview","text":""},{"location":"modules/perforce/helix-core.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws &gt;= 5.30 awscc &gt;= 1.2.0 random &gt;=3.6"},{"location":"modules/perforce/helix-core.html#providers","title":"Providers","text":"Name Version aws &gt;= 5.30 awscc &gt;= 1.2.0 random &gt;=3.6"},{"location":"modules/perforce/helix-core.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/helix-core.html#resources","title":"Resources","text":"Name Type aws_ebs_volume.depot resource aws_ebs_volume.logs resource aws_ebs_volume.metadata resource aws_eip.helix_core_eip resource aws_iam_instance_profile.helix_core_instance_profile resource aws_iam_policy.helix_core_default_policy resource aws_iam_role.helix_core_default_role resource aws_instance.helix_core_instance resource aws_security_group.helix_core_security_group resource aws_volume_attachment.depot_attachment resource aws_volume_attachment.logs_attachment resource aws_volume_attachment.metadata_attachment resource aws_vpc_security_group_egress_rule.helix_core_internet resource awscc_secretsmanager_secret.helix_core_super_user_password resource awscc_secretsmanager_secret.helix_core_super_user_username resource random_string.helix_core resource aws_ami.helix_core_ami data source aws_iam_policy_document.ec2_trust_relationship data source aws_iam_policy_document.helix_core_default_policy data source aws_subnet.instance_subnet data source"},{"location":"modules/perforce/helix-core.html#inputs","title":"Inputs","text":"Name Description Type Default Required FQDN The FQDN that should be used to generate the self-signed SSL cert on the Helix Core instance. <code>string</code> <code>null</code> no create_default_sg Whether to create a default security group for the Helix Core instance. <code>bool</code> <code>true</code> no create_helix_core_default_role Optional creation of Helix Core default IAM Role with SSM managed instance core policy attached. Default is set to true. <code>bool</code> <code>true</code> no custom_helix_core_role ARN of the custom IAM Role you wish to use with Helix Core. <code>string</code> <code>null</code> no depot_volume_size The size of the depot volume in GiB. Defaults to 128 GiB. <code>number</code> <code>128</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_security_groups A list of existing security group IDs to attach to the Helix Core load balancer. <code>list(string)</code> <code>[]</code> no helix_authentication_service_url The URL for the Helix Authentication Service. <code>string</code> <code>null</code> no helix_core_super_user_password_secret_arn If you would like to manage your own super user credentials through AWS Secrets Manager provide the ARN for the super user's password here. <code>string</code> <code>null</code> no helix_core_super_user_username_secret_arn If you would like to manage your own super user credentials through AWS Secrets Manager provide the ARN for the super user's username here. Otherwise, the default of 'perforce' will be used. <code>string</code> <code>null</code> no instance_subnet_id The subnet where the Helix Core instance will be deployed. <code>string</code> n/a yes instance_type The instance type for Perforce Helix Core. Defaults to c6in.large. <code>string</code> <code>\"c6in.large\"</code> no internal Set this flag to true if you do not want the Helix Core instance to have a public IP. <code>bool</code> <code>false</code> no logs_volume_size The size of the logs volume in GiB. Defaults to 32 GiB. <code>number</code> <code>32</code> no metadata_volume_size The size of the metadata volume in GiB. Defaults to 32 GiB. <code>number</code> <code>32</code> no name The name attached to swarm module resources. <code>string</code> <code>\"helix-core\"</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no server_type The Perforce Helix Core server type. <code>string</code> n/a yes storage_type The type of backing store [EBS, FSxZ] <code>string</code> n/a yes tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IAC_MANAGEMENT\": \"CGD-Toolkit\",  \"IAC_MODULE\": \"helix-core\",  \"IAC_PROVIDER\": \"Terraform\"}</pre> no vpc_id The VPC where Helix Core should be deployed <code>string</code> n/a yes"},{"location":"modules/perforce/helix-core.html#outputs","title":"Outputs","text":"Name Description helix_core_eip_id The ID of the Elastic IP associated with your Helix Core instance. helix_core_eip_private_ip The private IP of your Helix Core instance. helix_core_eip_public_ip The public IP of your Helix Core instance. helix_core_super_user_password_secret_arn The ARN of the AWS Secrets Manager secret holding your Helix Core super user's password. helix_core_super_user_username_secret_arn The ARN of the AWS Secrets Manager secret holding your Helix Core super user's username. security_group_id The default security group of your Helix Core instance."},{"location":"modules/perforce/helix-swarm.html","title":"Perforce Helix Swarm","text":""},{"location":"modules/perforce/helix-swarm.html#overview","title":"Overview","text":"<p>Perforce Helix Swarm is a free code review tool for projects hosted in Perforce Helix Core.</p>"},{"location":"modules/perforce/helix-swarm.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws &gt;= 5.30 random &gt;=3.6"},{"location":"modules/perforce/helix-swarm.html#providers","title":"Providers","text":"Name Version aws &gt;= 5.30 random &gt;=3.6"},{"location":"modules/perforce/helix-swarm.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/helix-swarm.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.swarm_redis_service_log_group resource aws_cloudwatch_log_group.swarm_service_log_group resource aws_ecs_cluster.swarm_cluster resource aws_ecs_cluster_capacity_providers.swarm_cluster_fargate_providers resource aws_ecs_service.swarm_service resource aws_ecs_task_definition.swarm_task_definition resource aws_efs_access_point.redis_efs_access_point resource aws_efs_access_point.swarm_efs_access_point resource aws_efs_file_system.swarm_efs_file_system resource aws_efs_mount_target.swarm_efs_mount_target resource aws_iam_policy.swarm_default_policy resource aws_iam_policy.swarm_efs_policy resource aws_iam_policy.swarm_ssm_policy resource aws_iam_role.swarm_default_role resource aws_iam_role.swarm_task_execution_role resource aws_lb.swarm_alb resource aws_lb_listener.swarm_alb_https_listener resource aws_lb_target_group.swarm_alb_target_group resource aws_s3_bucket.swarm_alb_access_logs_bucket resource aws_security_group.swarm_alb_sg resource aws_security_group.swarm_efs_security_group resource aws_security_group.swarm_elasticache_sg resource aws_security_group.swarm_service_sg resource aws_vpc_security_group_egress_rule.swarm_alb_outbound_service resource aws_vpc_security_group_egress_rule.swarm_service_outbound_ipv4 resource aws_vpc_security_group_egress_rule.swarm_service_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.swarm_efs_inbound_service resource aws_vpc_security_group_ingress_rule.swarm_elasticache_inbound_service resource aws_vpc_security_group_ingress_rule.swarm_service_inbound_alb resource random_string.swarm resource random_string.swarm_alb_access_logs_bucket_suffix resource aws_ecs_cluster.swarm_cluster data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.swarm_default_policy data source aws_iam_policy_document.swarm_efs_policy data source aws_iam_policy_document.swarm_ssm_policy data source aws_region.current data source"},{"location":"modules/perforce/helix-swarm.html#inputs","title":"Inputs","text":"Name Description Type Default Required certificate_arn The TLS certificate ARN for the Helix Swarm service load balancer. <code>string</code> n/a yes cluster_name The name of the cluster to deploy the Helix Swarm service into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no create_swarm_default_policy Optional creation of Helix Swarm default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_swarm_default_role Optional creation of Helix Swarm Default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_swarm_role ARN of the custom IAM Role you wish to use with Helix Swarm. <code>string</code> <code>null</code> no enable_elastic_filesystem Flag to enable/disable elastic filesystem for persistent storage. Defaults to false. <code>bool</code> <code>false</code> no enable_elasticache_serverless Flag to enable/disable Redis elasticache. Defaults to false. <code>bool</code> <code>false</code> no enable_swarm_alb_access_logs Enables access logging for the Helix Swarm ALB. Defaults to false. <code>bool</code> <code>false</code> no enable_swarm_alb_deletion_protection Enables deletion protection for the Helix Swarm ALB. Defaults to false. <code>bool</code> <code>false</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_redis_host The hostname where the Redis cache that Swarm should use is running. <code>string</code> <code>null</code> no existing_security_groups A list of existing security group IDs to attach to the Helix Swarm service load balancer. <code>list(string)</code> <code>[]</code> no fqdn The fully qualified domain name that Swarm should use for internal URLs. <code>string</code> <code>null</code> no internal Set this flag to true if you do not want the Helix Swarm service load balancer to have a public IP. <code>bool</code> <code>false</code> no name The name attached to swarm module resources. <code>string</code> <code>\"swarm\"</code> no p4d_port The P4D_PORT environment variable where Swarm should look for Helix Core. Defaults to 'ssl:perforce:1666' <code>string</code> <code>\"ssl:perforce:1666\"</code> no p4d_super_user_arn The ARN of the parameter or secret where the p4d super user username is stored. <code>string</code> n/a yes p4d_super_user_password_arn The ARN of the parameter or secret where the p4d super user password is stored. <code>string</code> n/a yes p4d_swarm_password_arn The ARN of the parameter or secret where the swarm user password is stored. <code>string</code> n/a yes p4d_swarm_user_arn The ARN of the parameter or secret where the swarm user username is stored. <code>string</code> n/a yes project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no redis_container_cpu CPU allotment for Helix Swarm Redis container. <code>number</code> <code>1024</code> no redis_container_memory Memory allotment for Helix Swarm Redis container. <code>number</code> <code>2048</code> no redis_container_name The name of the Redis container. <code>string</code> <code>\"swarm-redis\"</code> no redis_container_port The port where the Redis cache that Swarm should use is running. <code>number</code> <code>6379</code> no redis_image The Redis image and version that Helix Swarm should use. <code>string</code> <code>\"redis\"</code> no swarm_alb_access_logs_bucket ID of the S3 bucket for Helix Swarm ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no swarm_alb_access_logs_prefix Log prefix for Helix Swarm ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no swarm_alb_subnets A list of subnets to deploy the Helix Swarm load balancer into. Public subnets are recommended. <code>list(string)</code> n/a yes swarm_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Helix Swarm. <code>string</code> <code>365</code> no swarm_container_cpu The CPU allotment for the swarm container. <code>number</code> <code>1024</code> no swarm_container_memory The memory allotment for the swarm container. <code>number</code> <code>2048</code> no swarm_container_name The name of the swarm container. <code>string</code> <code>\"helix-swarm-container\"</code> no swarm_container_port The container port that swarm runs on. <code>number</code> <code>80</code> no swarm_desired_container_count The desired number of containers running the Helix Swarm service. <code>number</code> <code>1</code> no swarm_efs_performance_mode The performance mode of the EFS file system used by the Helix Swarm service. Defaults to general purpose. <code>string</code> <code>\"generalPurpose\"</code> no swarm_efs_throughput_mode The throughput mode of the EFS file system used by the Helix Swarm service. Defaults to bursting. <code>string</code> <code>\"bursting\"</code> no swarm_service_subnets A list of subnets to deploy the Helix Swarm service into. Private subnets are recommended. <code>list(string)</code> n/a yes tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IAC_MANAGEMENT\": \"CGD-Toolkit\",  \"IAC_MODULE\": \"swarm\",  \"IAC_PROVIDER\": \"Terraform\"}</pre> no task_cpu The CPU allotment for the Helix Swarm task. <code>number</code> <code>2048</code> no task_memory The memory allotment for the Helix Swarm task. <code>number</code> <code>4096</code> no vpc_id The ID of the existing VPC you would like to deploy swarm into. <code>string</code> n/a yes"},{"location":"modules/perforce/helix-swarm.html#outputs","title":"Outputs","text":"Name Description alb_dns_name The DNS name of the Swarm ALB alb_security_group_id Security group associated with the swarm load balancer alb_zone_id The hosted zone ID of the Swarm ALB cluster_name Name of the ECS cluster hosting Swarm service_security_group_id Security group associated with the ECS service running swarm"},{"location":"samples/index.html","title":"Overview","text":"<p>Samples represent a reference implementation that can be deployed to solve for a specific use-case or workload. These are Terraform configurations and integrations with other common AWS workloads and services. Each sample will provide its own documentation and instructions that follows the template below:</p>"},{"location":"samples/index.html#1-predeployment","title":"1) Predeployment","text":"<p>In the predeployment phase the user is instructed to provision or take note of any necessary pre-existing resources. Creating SSL certificates or keypairs, provisioning Amazon Machine Images (AMIs) with Packer, or documenting existing resource IDs and names all fall into this phase.</p>"},{"location":"samples/index.html#2-deployment","title":"2) Deployment","text":"<p>In the deployment phase the user is instructed to run <code>terraform apply</code> on one or more Terraform configurations with the appropriate variables.</p>"},{"location":"samples/index.html#3-postdeployment","title":"3) Postdeployment","text":"<p>Finally, the postdeployment phase includes any Ansible playbooks or remote execution instructions for configuring the applications that have been deployed. These may be automated or manual steps.</p>"},{"location":"samples/simple-build-pipeline.html","title":"Simple Build Pipeline Sample","text":""}]}