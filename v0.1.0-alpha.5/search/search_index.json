{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to the Cloud Game Development Toolkit","text":"<p>The Cloud Game Development Toolkit (a.k.a. CGD Toolkit) is a collection of templates and configurations for deploying game development infrastructure and tools on AWS.</p> <p>Info</p> <p>This project is under active development and community contributions are welcomed!. If you would like to see something in this repository please create a feature request in the Issues tab. If you'd like to contribute, raise a pull request. You'll find our contribution guidelines here.</p>"},{"location":"index.html#introduction","title":"Introduction","text":"<p>The CGD Toolkit consists of three key components:</p> <ul> <li> <p> Assets</p> <p>Foundational resources such as image templates, configurations scripts, and CI/CD pipeline definitions for game development.</p> <p> Assets</p> </li> <li> <p> Modules</p> <p>Infrastructure as code (we use Terraform) for deploying common game dev workloads with best-practices by default. These typically consume assets.</p> <p> Modules</p> </li> <li> <p> Samples</p> <p>Opinionated implementations to address common use cases for expedited game studio setup and battle-tested scenarios from the community.</p> <p> Samples</p> </li> </ul>"},{"location":"contributing.html","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"contributing.html#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"contributing.html#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p>"},{"location":"contributing.html#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"contributing.html#building-and-testing-project-documetation","title":"Building and Testing Project Documetation","text":"<p>This project uses Material for MkDocs to generate versioned documentation automatically. Content for the documentation is auto-generated by referencing the markdown contained in the project's <code>README.md</code> files. </p> <p>Local development: - To build the project documentation, run: <code>make docs-local-docker VERSION=&lt;Semantic Release Version&gt;</code>. This will build the project using <code>./docs/Dockerfile</code> and host the documentation using <code>mkdocs serve</code> on port <code>8000</code>. </p> <p>Live documentation (GitHub Pages): - A Github workflow is used to build and deploy the documentation to the gh-pages branch: <code>.github/workflows/docs.yml</code></p>"},{"location":"contributing.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"contributing.html#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"contributing.html#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"assets/index.html","title":"Assets","text":"<p>Assets are reusable scripts, pipeline definitions, Dockerfiles, Packer templates, and other resources that might prove useful or are dependencies of any of the modules.</p> <p>Info</p> <p>Don't see an asset listed? Create a feature request for a new asset or learn how to contribute new assets to the project below</p> Asset Type Description Packer Templates Packer templates provide an easy way to build machine images for commonly used game dev infrastructure. Currently the project includes Packer templates for UE5 build agents for Linux and Windows, as well as a Packer template for building a Perforce Helix Core version control AMI. Jenkins Pipelines Jenkins Pipelines for common game dev automation workflows Ansible Playbooks Automation scripts for reusable system level configurations. Dockerfiles (Coming Soon!) Dockerfiles for creating Docker images of commonly used game dev infrastructure. These are primarily used in scenarios where there aren't openly available pre-built images that address a use case, or significant customization is needed that warrants building an image"},{"location":"assets/dockerfiles.html","title":"Dockerfiles","text":""},{"location":"assets/jenkins-pipelines.html","title":"Jenkins Pipelines","text":""},{"location":"assets/playbooks.html","title":"Cloud Game Development Toolkit Playbooks","text":"<p>This directory contains automation scripts for configuring workloads after deployment. We've selected Ansible as our IT automation platform, but we envision this directory will contain SSM run documents, Chef recipes, and potentially just plain, old bash scripts in the long term.</p>"},{"location":"assets/packer/index.html","title":"Packer Templates","text":"<p>Packer is a tool for simplifying and automating Amazon Machine Image (AMI) creation with code. It enables developers to create identical images for multiple platforms. The Packer templates provided in the Cloud Game Development Toolkit can be used to provision EC2 instances with common development tools preinstalled.</p> <p>Info</p> <p>Don't see a Packer template that solves your needs? Create a feature request for a new template or learn how to contribute new assets to the project below</p> Asset Type Description Linux Build Agents Provision C++ compilation machines on Amazon Linux 2023 and Ubuntu machines on both x86 and ARM based architectures with useful tools like compiler caches such as Octobuild preinstalled. Windows Build Agents Create Windows 2022 based instances capable of Unreal Engine compilation out of the box. Perforce Helix Core An Amazon Machine Image used for provisioning Helix Core on AWS. This AMI is required for deployment of the Perforce Helix Core module"},{"location":"assets/packer/helix-core.html","title":"Perforce Helix Core Packer Template","text":"<p>This Packer template creates an Amazon Machine Image for installing and configuring a Perforce [Helix Core] server on Linux.</p> <p>The <code>p4_configure.sh</code> script contains the majority of Helix Core setup. It performs the following operations:</p> <ol> <li>Pre-Flight Checks: Ensures the script is run with root privileges.</li> <li>Environment Setup: Defines paths and necessary constants for the installation.</li> <li>SELinux Handling: Checks if SELinux is enabled and installs required packages.</li> <li>User and Group Verification: Ensures the 'perforce' user and group exist.</li> <li>Directory Creation and Ownership: Ensures necessary directories exist and have correct ownership.</li> <li>Helix Binaries and SDP Installation: Downloads and extracts SDP, checks for Helix binaries, and downloads them if missing.</li> <li>Systemd Service Configuration: Sets up a systemd service for the p4d server.</li> <li>SSL Configuration: Updates SSL certificate configuration with the EC2 instance DNS name.</li> <li>SELinux Context Management: Updates SELinux context for p4d.</li> <li>Crontab Initialization: Sets up crontab for the 'perforce' user.</li> <li>SDP Verification: Runs a script to verify the SDP installation.</li> <li>Helix Authentication Extension: Installs the Helix Authentication Extension and validates successful communication with Helix Authentication Service.</li> </ol>"},{"location":"assets/packer/helix-core.html#how-to-use","title":"How to Use","text":"<p>Building this AMI is as easy as running:</p> <pre><code>packer build /assets/packer/perforce/helix-core/perforce.pkr.hcl\n</code></pre> <p>Packer will attempt to leverage the default VPC available in the AWS account and Region specified by your CLI credentials. It will provision an instance in a public subnet and communicate with that instance over the public internet. If a default VPC is not provided the above command will fail. This Packer template can take a number of variables as specified in <code>example.pkrvars.hcl</code>. Variables can be passed individually through the <code>-var</code> command line flag or through a configuration file with the <code>-var-file</code> command line flag.</p> <p>An instance that is provisioned with this AMI will not automatically deploy a Helix Core server. Instead, the required installation and configuration scripts are loaded onto this AMI by Packer, and then invoked at boot through EC2 user data. The Perforce Helix Core module does this through Terraform, but you can also manually provision an instance off of this AMI and specify the user data yourself:</p> <pre><code>#!/bin/bash\n/home/ec2-user/cloud-game-development-toolkit/p4_configure.sh \\\n   &lt;volume label for hxdepot&gt; \\\n   &lt;volume label for hxmetadata&gt; \\\n   &lt;volume label for hxlogs&gt; \\\n   &lt;p4d server type&gt; \\\n   &lt;super user username ARN from AWS Secrets Manager&gt; \\\n   &lt;super user password ARN from AWS Secrets Manager&gt; \\\n   &lt;fully qualified domain name of Helix Core&gt; \\\n   &lt;URL for Helix Authentication Service&gt;\n</code></pre> <p>As you can see, there are quite a few configurables that need to be passed to the <code>p4_configure.sh</code> script. We recommend using the Perforce Helix Core module for this reason.</p>"},{"location":"assets/packer/helix-core.html#important-notes","title":"Important Notes","text":"<ul> <li>This script is designed for a specific use-case and might require modifications for different environments or requirements.</li> <li>Ensure you have a backup of your system before running the script, as it makes significant changes to users, groups, and services.</li> <li>The script assumes an internet connection for downloading packages and binaries.</li> </ul>"},{"location":"assets/packer/build-agents/linux.html","title":"Linux Build Agents","text":""},{"location":"assets/packer/build-agents/linux.html#packer-templates-for-unreal-engine-linux-build-agents","title":"Packer templates for Unreal Engine Linux build agents","text":"<p>The following templates provide Unreal Engine Linux build agents:</p> Operating system CPU architecture file location Ubuntu Jammy 22.04 x86_64 (a.k.a. amd64) <code>x86_64/ubuntu-jammy-22.04-amd64-server.pkr.hcl</code> Ubuntu Jammy 22.04 aarch64 (a.k.a. arm64) <code>aarch64/ubuntu-jammy-22.04-arm64-server.pkr.hcl</code> Amazon Linux 2023 x86_64 (a.k.a. amd64) <code>x86_64/amazon-linux-2023-x86_64.pkr.hcl</code> Amazon Linux 2023 aarch64 (a.k.a. arm64) <code>aarch64/amazon-linux-2023-arm64.pkr.hcl</code>"},{"location":"assets/packer/build-agents/linux.html#usage","title":"Usage","text":"<ol> <li>Make a copy of <code>example.pkrvars.hcl</code> and adjust the input variables as needed</li> <li>Ensure you have active AWS credentials</li> <li>Invoke <code>packer build --var-file=&lt;your .pkrvars.hcl file&gt; &lt;path to .pkr.hcl file&gt;</code>, then wait for the build to complete.</li> </ol>"},{"location":"assets/packer/build-agents/linux.html#software-packages-included","title":"Software packages included","text":"<p>The templates install various software packages:</p> <p>common tools</p> <p>Some common tools are installed to enable installing other software, performing maintenance tasks, and compile some C++ software:</p> <ul> <li>git</li> <li>curl</li> <li>jq</li> <li>unzip</li> <li>dos2unix</li> <li>AWS CLI v2</li> <li>AWS Systems Manager Agent</li> <li>Amazon Corretto</li> <li>mount.nfs, to be able to mount FSx volumes over NFS</li> <li>python3</li> <li>python3 packages: 'pip', 'requests', 'boto3' and 'botocore'</li> <li>clang</li> <li>cmake3</li> <li>scons</li> <li>Development libraries for compiling the Amazon GameLift Server SDK for C++</li> <li>Development libraries for compiling the Godot 4 game engine (if available in the OS's package manager)</li> </ul> <p>mold</p> <p>The 'mold' linker is installed to enable faster linking.</p> <p>FSx automounter service</p> <p>The FSx automounter systemd service is a service written in Python that automatically mounts FSx for OpenZFS volumes on instance bootup. The service uses resource tags on FSx volumes to determine if and where to mount volumes on.</p> <p>You can use the following tags on FSx volumes: * 'automount-fsx-volume-name' tag: specifies the name of the local mount point. The mount point specified will be prefixed with 'fsx_' by the service. * 'automount-fsx-volume-on' tag: This tag contains a space-delimited list of EC2 instance names on which the volume will be automatically mounted by this service (if it is running on that instance).</p> <p>For example, if the FSx automounter service is running on an EC2 instance with Name tag 'ubuntu-builder', and an FSx volume has tag <code>automount-fsx-volume-on</code>=<code>al2023-builder ubuntu-builder</code> and tag <code>automount-fsx-volume-name</code>=<code>workspace</code>, then the automounter will automatically mount that volume on <code>/mnt/fsx_workspace</code>.</p> <p>Note that the automounter service makes use of the ListTagsForResource FSx API call, which is rate-limited. If you intend to scale up hundreds of EC2 instances that are running this service, then we recommend automatically mounting FSx volumes using <code>/etc/fstab</code>.</p> <p>mount_ephemeral service</p> <p>The mount_ephemeral service is a systemd service written as a simple bash script that mounts NVMe attached instance storage volume automatically as temporary storage. It does this by formatting <code>/dev/nvme1n1</code> as xfs and then mounting it on <code>/tmp</code>. This service runs on instance bootup.</p> <p>create_swap service</p> <p>The create_swap service is a systemd service written as a simple bash script that creates a 1GB swap file on <code>/swapfile</code>. This service runs on instance bootup.</p> <p>sccache</p> <p>'sccache' is installed to cache c/c++ compilation artefacts, which can speed up builds by avoiding unneeded work.</p> <p>sccache is installed as a systemd service, and configured to use <code>/mnt/fsx_cache/sccache</code> as its cache folder. The service expects this folder to be available or set up by another service.</p> <p>octobuild</p> <p>'Octobuild' is installed to act as a compilation cache for Unreal Engine.</p> <p>Octobuild is configured (in octobuild.conf) to use <code>/mnt/fsx_cache/octobuild_cache</code> as its cache folder, and expects this folder to be available or set up by another service.</p> <p>NOTE: Octobuild is not supported on aarch64, and therefore not installed there.</p>"},{"location":"assets/packer/build-agents/linux.html#processor-architectures-and-naming-conventions","title":"Processor architectures and naming conventions","text":"<p>Within this folder, the processor architecture naming conventions as reported by <code>uname -m</code> are used, hence why there are scripts here with names containing \"x86_64\" or \"aarch64\". The packer template <code>.hcl</code> files are named following the naming conventions of the operating system that they are based on. Unfortunately, because some operating systems don't use the same terminology in their naming conventions throughout, this means that you'll see this lack of consistency here has well.</p>"},{"location":"assets/packer/build-agents/windows.html","title":"Packer Templates - Windows Build Agents README","text":""},{"location":"ci/index.html","title":"CI and Testing","text":"<p>This project uses Github Actions to automate continuous integration (CI) testing using utilities contained in the <code>.ci/</code> directories within the project's assets, modules, and samples. Dockerfiles are included to simplify running these CI workflows locally in your development environment or in a cloud CI environment. </p>"},{"location":"ci/index.html#example-ci-directory-packerci","title":"Example CI directory: <code>packer/.ci/</code>","text":"<pre><code>.ci/\n\u251c\u2500\u2500 Dockerfile &lt;------------------ Dockerfile for running Packer CI\n\u251c\u2500\u2500 packer-validate.sh &lt;---------- Script for linting with Packer Validate\n\u251c\u2500\u2500 README.md &lt;------------------- Instructions for Packer CI\n\u2514\u2500\u2500 setup.sh &lt;-------------------- Commands to setup environment (i.e install Packer)\n</code></pre>"},{"location":"ci/packer.html","title":"Packer CI","text":"<p>This README provides steps for running basic CI on Packer templates contained in the project.</p>"},{"location":"ci/packer.html#configuration","title":"Configuration","text":"<p>In the root of the <code>./packer</code> directory is a configuration yaml file (<code>.config.yml</code>) that contains the location of each of the Packer templates in the project. When new Packer templates are added, the config file should be updated as well.</p>"},{"location":"ci/packer.html#configyml","title":"<code>.config.yml</code>","text":".confi.yml<pre><code># Packer template locations relative to ./packer directory\npacker_templates:\n  - description: 'Amazon Linux 2023 Jenkins Builder (x86)'\n    file_name: 'amazon-linux-2023-x86_64.pkr.hcl'\n    dir: 'build-agents/linux'\n  - description: 'Amazon Linux 2023 Jenkins Builder (ARM)'\n    file_name: 'amazon-linux-2023-arm64.pkr.hcl'\n    dir: 'build-agents/linux'\n  - description: 'Ubuntu 22.04 LTS Jenkins Builder (x86)'\n    file_name: 'ubuntu-jammy-22.04-amd64-server.pkr.hcl'\n    dir: 'build-agents/linux'\n  - description: 'Ubuntu 22.04 LTS Jenkins Builder (ARM)'\n    file_name: 'ubuntu-jammy-22.04-arm64-server.pkr.hcl'\n    dir: 'build-agents/linux'\n  - description: 'Windows Server 2022 Jenkins Builder (x86)'\n    file_name: 'windows.pkr.hcl'\n    dir: 'build-agents/windows'\n  - description: 'Perforce Helix Core Server (x86)'\n    file_name: 'perforce.pkr.hcl'\n    dir: 'perforce/helix-core'\n</code></pre> <pre><code>docker build -t packer-ci -f .ci/Dockerfile . \\             \n--build-arg AWS_REGION=us-east-1 \\\n--build-arg AWS_VPC_ID=vpc-086839c0e28ad1f29 \\\n--build-arg AWS_SUBNET_ID=subnet-0e6bb0e5c155610c0 \\\n--build-arg AWS_PROFILE=default \\\n--build-arg PUBLIC_KEY=Key_Pair_Linux\n</code></pre>"},{"location":"ci/packer.html#packer-directory","title":"Packer Directory","text":"<pre><code>.\n\u251c\u2500\u2500 build-agents\n\u2502   \u251c\u2500\u2500 linux\n\u2502   \u2502   \u251c\u2500\u2500 amazon-linux-2023-arm64.pkr.hcl\n\u2502   \u2502   \u251c\u2500\u2500 amazon-linux-2023-x86_64.pkr.hcl\n\u2502   \u2502   \u251c\u2500\u2500 create_swap.service\n\u2502   \u2502   \u251c\u2500\u2500 create_swap.sh\n\u2502   \u2502   \u251c\u2500\u2500 example.pkrvars.hcl\n\u2502   \u2502   \u251c\u2500\u2500 fsx_automounter.py\n\u2502   \u2502   \u251c\u2500\u2500 fsx_automounter.service\n\u2502   \u2502   \u251c\u2500\u2500 install_common.al2023.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_common.ubuntu.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_mold.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_octobuild.al2023.x86_64.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_octobuild.ubuntu.x86_64.sh\n\u2502   \u2502   \u251c\u2500\u2500 install_sccache.sh\n\u2502   \u2502   \u251c\u2500\u2500 mount_ephemeral.service\n\u2502   \u2502   \u251c\u2500\u2500 mount_ephemeral.sh\n\u2502   \u2502   \u251c\u2500\u2500 octobuild.conf\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 sccache.service\n\u2502   \u2502   \u251c\u2500\u2500 ubuntu-jammy-22.04-amd64-server.pkr.hcl\n\u2502   \u2502   \u2514\u2500\u2500 ubuntu-jammy-22.04-arm64-server.pkr.hcl\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 windows\n\u2502       \u251c\u2500\u2500 base_setup.ps1\n\u2502       \u251c\u2500\u2500 example.pkrvars.hcl\n\u2502       \u251c\u2500\u2500 install_vs_tools.ps1\n\u2502       \u251c\u2500\u2500 setup_jenkins_agent.ps1\n\u2502       \u251c\u2500\u2500 userdata.ps1\n\u2502       \u2514\u2500\u2500 windows.pkr.hcl\n\u251c\u2500\u2500 .ci\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 packer-ci.md\n\u2502   \u251c\u2500\u2500 packer-validate.sh\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 setup.sh\n\u251c\u2500\u2500 .config.yml\n\u2514\u2500\u2500 perforce\n    \u2514\u2500\u2500 helix-core\n        \u251c\u2500\u2500 example.pkrvars.hcl\n        \u251c\u2500\u2500 p4_configure.sh\n        \u251c\u2500\u2500 p4_setup.sh\n        \u251c\u2500\u2500 perforce.pkr.hcl\n        \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"modules/index.html","title":"Modules","text":""},{"location":"modules/index.html#introduction","title":"Introduction","text":"<p>These modules simplify the deployment of common game development workloads on AWS. Some have pre-requisites that will be outlined in their respective documentation. They are designed to easily integrate with each other, and provide relevant outputs to simplify permissions, networking, and access.</p>"},{"location":"modules/index.html#contribution","title":"Contribution","text":"<p>We recommend starting with the Terraform module documentation for a crash course in the way the CGD Toolkit is designed.</p> <p>Please follow these guidelines when developing a new module. These are also outlined in the pull-request template for Module additions.</p>"},{"location":"modules/index.html#1-provider-configurations","title":"1. Provider Configurations","text":"<p>Modules should not define its own provider configurations. Required provider versions should be outlined in a <code>required_versions</code> block inside of a <code>terraform</code> block:</p> <pre><code>terraform {  \n    required_providers {    \n        aws = {      \n            source  = \"hashicorp/aws\"      \n            version = \"&gt;= 5.30.0\"    \n        }  \n    }\n}\n</code></pre>"},{"location":"modules/index.html#2-dependency-inversion","title":"2. Dependency Inversion","text":"<p>It is fine if your module needs to declare significant networking or compute resources to run - the Cloud Game Development Toolkit is intended to be highly opinionated. At the same time, we require that modules support a significant level of dependency injection through variables to support diverse use cases. This is a simple consideration that is easier to incorporate from the beginning of module development rather than retroactively.</p> <p>For example, the Jenkins module can provision its own Elastic Container Service cluster, or it can deploy the Jenkins service onto an existing cluster passed via the <code>cluster_name</code> variable.</p>"},{"location":"modules/index.html#3-assumptions-and-guarantees","title":"3. Assumptions and Guarantees","text":"<p>If your module requires certain input formats in order to function Terraform refers to these as \"assumptions.\"</p> <p>If your module provides certain outputs in a consistent format that other configurations should be able to rely on Terraform calls these \"guarantees.\"</p> <p>We recommend outlining your module's assumptions and guarantees prior to implementation by using Terraform custom conditions. These can be used to validate input variables, data blocks, resource attributes, and much more. They are incredibly powerful.</p>"},{"location":"modules/index.html#4-third-party-software","title":"4. Third Party Software","text":"<p>The modules contained in the CGD Toolkit are designed to simplify infrastructure deployments of common game development workload. Naturally, modules may deploy third party applications - in these situations we require that deployments depend on existing licenses and distribution channels.</p> <p>If your module relies on a container or image that is not distributed through the CGD Toolkit we require a disclaimer and the usage of end-user credentials passed as a variable to the module. This repository is not to be used to redistribute software that may be subject to licensing or contractual agreements.</p> <p>If your module relies on a custom Amazon Machine Image (AMI) or container we ask that you provide a Packer template or Dockerfile in the assets directory and include instructions to create the image prior to infrastructure deployment.</p>"},{"location":"modules/jenkins/jenkins.html","title":"Jenkins Module","text":"<p>Jump to Terraform docs</p> <p>Jenkins is an open source automation server that simplifies repeatable software development tasks such as building, testing, and deploying applications. This module deploys Jenkins on an Elastic Container Service (ECS) cluster backed by AWS Fargate using the latest Jenkins container image (jenkins/jenkins:lts-jdk17). The deployment includes an Elastic File System (EFS) volume for persisting plugins and configurations, along with an Elastic Load Balancer (ELB) deployment for TLS termination. The module also includes the deployment of an EC2 Autoscaling group to serve as a flexible pool of build nodes, however the user must configure Jenkins to use the build farm.</p> <p>This module deploys the following resources:</p> <ul> <li>An Elastic Container Service (ECS) cluster backed by AWS Fargate.</li> <li>An ECS service running the latest Jenkins container image (jenkins/jenkins:lts-jdk17) available.</li> <li>An Elastic File System (EFS) for the Jenkins service to use as a persistent datastore.</li> <li>A user defined number of ZFS File Systems</li> <li>An Elastic Load Balancer (ELB) for TLS termination of the Jenkins service</li> <li>A configurable number of EC2 Autoscaling groups to serve as a flexible pool of build nodes for the Jenkins service</li> <li>Supporting resources including KMS keys for encryption and IAM roles to ensure security best practices</li> </ul>"},{"location":"modules/jenkins/jenkins.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/jenkins/jenkins.html#prerequisites","title":"Prerequisites","text":"<p>There are a few prerequisites to the deployment of Jenkins which are not directly provided in the module. The first is a public certificate used by the Jenkins ALB for SSL termination, instructions are included for provisioning the certificates using the AWS Certificates Manager (ACM) but certificates created else where can also be uploaded into the service for later use. The second is an AWS Secrets Manager secret which is used to store sensitive information, such as SSH keys, which can then be made available to Jenkins through a Plugin.</p>"},{"location":"modules/jenkins/jenkins.html#create-a-public-certificate","title":"Create a Public Certificate","text":"How to Create a Public Certificate Using Amazon Certificate Manager <ol> <li> <p>Sign in to the AWS Management Console and open the Amazon Certificate Manager (ACM) console and choose Request a certificate.</p> </li> <li> <p>In the Domain names section, type your domain name.</p> <p>a. You can use a fully qualified domain name (FQDN), such as www.example.com, or a bare or apex domain name such as example.com. You can also use an asterisk (*) as a wild card in the leftmost position to protect several site names in the same domain. For example, *.example.com protects corp.example.com, and images.example.com. The wild-card name will appear in the Subject field and in the Subject Alternative Name extension of the ACM certificate.</p> <p>b. When you request a wild-card certificate, the asterisk (*) must be in the leftmost position of the domain name and can protect only one subdomain level. For example, *.example.com can protect login.example.com, and test.example.com, but it cannot protect test.login.example.com. Also note that *.example.com protects only the subdomains of example.com, it does not protect the bare or apex domain (example.com). To protect both, see the next step.</p> <p>Note</p> <p>In compliance with RFC 5280, the length of the domain name (technically, the Common Name) that you enter in this step cannot exceed 64 octets (characters), including periods. Each subsequent Subject Alternative Name (SAN) that you provide, as in the next step, can be up to 253 octets in length.</p> <p>c. To add another name, choose Add another name to this certificate and type the name in the text box. This is useful for protecting both a bare or apex domain (such as example.com) and its subdomains such as *.example.com).</p> </li> <li> <p>In the Validation method section, choose either DNS validation \u2013 recommended or Email validation, depending on your needs.</p> <p>Note</p> <p>If you are able to edit your DNS configuration, we recommend that you use DNS domain validation rather than email validation. DNS validation has multiple benefits over email validation. See DNS validation.</p> <p>a. Before ACM issues a certificate, it validates that you own or control the domain names in your certificate request. You can use either email validation or DNS validation.</p> <p>b. If you choose email validation, ACM sends validation email to three contact addresses registered in the WHOIS database, and up to five common system administration addresses for each domain name. You or an authorized representative must reply to one of these email messages. For more information, see Email validation.</p> <p>c. If you use DNS validation, you simply add a CNAME record provided by ACM to your DNS configuration. For more information about DNS validation, see DNS validation.</p> </li> <li> <p>In the Key algorithm section, chose one of the three available algorithms:</p> <ul> <li>RSA 2048 (default)</li> <li>ECDSA P 256</li> <li>ECDSA P 384</li> </ul> <p>a. For information to help you choose an algorithm, see Key algorithms and the AWS blog post How to evaluate and use ECDSA certificates in AWS Certificate Manager.</p> </li> </ol> <p>5.In the Tags page, you can optionally tag your certificate. Tags are key-value pairs that serve as metadata for identifying and organizing AWS resources. For a list of ACM tag parameters and for instructions on how to add tags to certificates after creation, see Tagging AWS Certificate Manager certificates. When you finish adding tags, choose Request.</p> <ol> <li> <p>After the request is processed, the console returns you to your certificate list, where information about the new certificate is displayed.     a. A certificate enters status Pending validation upon being requested, unless it fails for any of the reasons given in the troubleshooting topic Certificate request fails. ACM makes repeated attempts to validate a certificate for 72 hours and then times out. If a certificate shows status Failed or Validation timed out, delete the request, correct the issue with DNS validation or Email validation, and try again. If validation succeeds, the certificate enters status Issued.</p> <p>Note</p> <p>Depending on how you have ordered the list, a certificate you are looking for might not be immediately visible. You can click the black triangle at right to change the ordering. You can also navigate through multiple pages of certificates using the page numbers at upper-right.</p> </li> </ol> <p>Amazon Certificate Manager Documentation</p>"},{"location":"modules/jenkins/jenkins.html#optional-upload-secrets-to-aws-secrets-manager","title":"(Optional) Upload Secrets to AWS Secrets Manager","text":"<p>AWS Secrets Manager can be used to store sensitive information such as SSH keys and access tokens, which can then be made available to Jenkins. We recommend using the service to store the private key for the Jenkins agents which the Jenkins orchestrator uses to communicate over SSH.</p> <p>Warning</p> <p>To grant Jenkins access to the secrets stored in the AWS Secrets Manager, the <code>AWS Secrets Manager Credentials Provider</code> Jenkins plugin is recommended. There are requirements around tagging your secrets for the plugin to work properly. See the AWS Secrets Manager Credentials Provider Plugin Documentation for additional details. Step-by-step instructions for uploading SSH keys into AWS Secrets Manager can be found in the Configure Pluings section.</p> How to Upload Secrets to AWS Secrets Manager <ol> <li> <p>Open the Secrets Manager console.</p> </li> <li> <p>Choose Store a new secret.</p> </li> <li> <p>On the Choose secret type page, do the following:</p> <ol> <li> <p>For Secret type, choose Other type of secret.</p> </li> <li> <p>In Key/value pairs, either enter your secret in JSON Key/value pairs, or choose the Plaintext tab and enter the secret in any format (you must choose Plaintext if storing SSH keys). You can store up to 65536 bytes in the secret.</p> </li> <li> <p>For Encryption key, choose the AWS KMS key that Secrets Manager uses to encrypt the secret value. For more information, see Secret encryption and decryption.</p> <ul> <li> <p>For most cases, choose aws/secretsmanager to use the AWS managed key for Secrets Manager. There is no cost for using this key.</p> </li> <li> <p>If you need to access the secret from another AWS account, or if you want to use your own KMS key so that you can rotate it or apply a key policy to it, choose a customer managed key from the list or choose Add new key to create one. For information about the costs of using a customer managed key, see Pricing.</p> </li> <li> <p>You must have Permissions for the KMS key. For information about cross-account access, see Access AWS Secrets Manager secrets from a different account.</p> </li> </ul> </li> <li> <p>Choose Next</p> </li> </ol> </li> <li> <p>On the Configure secret page, do the following:</p> <ol> <li> <p>Enter a descriptive Secret name and Description. Secret names must contain 1-512 Unicode characters.</p> </li> <li> <p>(Optional) In the Tags section, add tags to your secret. For tagging strategies, see Tag AWS Secrets Manager secrets. Don't store sensitive information in tags because they aren't encrypted.</p> </li> <li> <p>(Optional) In Resource permissions, to add a resource policy to your secret, choose Edit permissions. For more information, see Attach a permissions policy to an AWS Secrets Manager secret.</p> </li> <li> <p>(Optional) In Replicate secret, to replicate your secret to another AWS Region, choose Replicate secret. You can replicate your secret now or come back and replicate it later. For more information, see Replicate secrets across Regions.</p> </li> <li> <p>Choose Next.</p> </li> </ol> </li> <li> <p>(Optional) On the Configure rotation page, you can turn on automatic rotation. You can also keep rotation off for now and then turn it on later. For more information, see Rotate secrets. Choose Next.</p> </li> <li> <p>On the Review page, review your secret details, and then choose Store.</p> </li> </ol> <p>Secrets Manager returns to the list of secrets. If your new secret doesn't appear, choose the refresh button.</p> <p>AWS Secrets Manager Documentation</p>"},{"location":"modules/jenkins/jenkins.html#optional-create-amazon-machine-image-ami-for-jenkins-agent-using-packer","title":"(Optional) Create Amazon Machine Image (AMI) for Jenkins Agent Using Packer","text":"<p>The CGD Toolkit provides packer templates for generating Amazon Machine Images (AMIs) for use as Jenkins Agents. The Toolkit provides both Windows and Linux options</p> How to Generate SSH Keys <ol> <li> <p>Open your preferred Command Line App.</p> </li> <li> <p>Paste the text below, replacing the email used in the example with your email</p> </li> </ol> <pre><code># This command creates a new SSH key, using the provided email as a label\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre> <ol> <li> <p>When prompted to \"Enter a file in which to save the key\", enter a path in which to save the generated key, or press Enter to accept the default location.</p> </li> <li> <p>When prompted to \"Enter passphrase\", enter a password for your key, or leave empty for no password.</p> </li> </ol> <p>Warning</p> <p>If using the AWS Secrets Manager Credentials Provider, leave the passphrase empty. The AWS Secrets Manager Credentials Provider plugin does NOT support passphrases.</p> How to Create Linux AMI using Packer <ol> <li>Copy the existing example Packer configuration file located at assets/packer/build-agents/linux/example.pkvars.hcl</li> <li>Replace placeholder values<ul> <li>region - AWS Region code to deploy the AMI into. i.e. \"us-west-2\"</li> <li>vpc_id - The ID of the VPC you wish to use to create the AMI.</li> <li>subnet_id - The ID of the subnet you wish to use to create the AMI.</li> <li>profile - The name of the AWS CLI profile you wish to use to create the AMI.</li> <li>public_key - The public ssh key to be used by Jenkins when communicating with its agents.</li> </ul> </li> <li> <p>Download the Packer dependencies     <pre><code># This command will download all necessary dependencies to build the AMI\npacker init\n</code></pre></p> <p>Note</p> <p>If you do not have Hashicorp Packer installed, see Packer Installation Instructions.</p> </li> <li> <p>Build the Image     <pre><code># This command builds the Linux x86_64 AMI using the configurations provided in the .pkvars.hcl you created above\npacker build -var-file your-vars.pkvars.hcl amazon-linux-2023-x86_64.pkr.hcl\n</code></pre></p> </li> <li> <p>Notate the AMI id (ami-#################) returned from the previous command.     <pre><code>==&gt; Wait completed after 10 minutes 23 seconds\n\n==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; jenkins-linux-packer.amazon-ebs.al2023: AMIs were created:\nus-east-1: ami-08862...e2e\n</code></pre></p> </li> </ol>"},{"location":"modules/jenkins/jenkins.html#installation","title":"Installation","text":"<pre><code>terraform init\n</code></pre>"},{"location":"modules/jenkins/jenkins.html#deploy","title":"Deploy","text":"<pre><code># Review architecture to be deployed\nterraform plan -var-file=deployment-variables.tfvars\n\n# Deploy terraform module\n# It is recommended that you use a .tfvars file to pass variables to the module when deploying\nterraform deploy -var-file=deployment-variables.tfvars\n</code></pre>"},{"location":"modules/jenkins/jenkins.html#accessing-jenkins","title":"Accessing Jenkins","text":"<p>Once deployed, the Jenkins service can be accessed through its associated load balancer. The service is accessible through the ALB on ports 443 and 80 by default. The module does NOT allowlist any IP addresses by default. Users must edit the ALB security group to grant access to the Jenkins Service.</p> <p>You can find the DNS address for the ALB created by the module by running the following command:</p> <pre><code># This command pulls the DNS address of the ALB associated with the Jenkins ECS service\nterraform output jenkins_alb_dns_name\n</code></pre>"},{"location":"modules/jenkins/jenkins.html#configuring-jenkins","title":"Configuring Jenkins","text":"<p>When accessing Jenkins for the first time, an administrators password is required. This password is auto-generated and available through the ECS logs. The administrative user will be replaced with a new user upon completion of the setup, see Creating Users.</p>"},{"location":"modules/jenkins/jenkins.html#retrieve-the-jenkins-administrator-password","title":"Retrieve the Jenkins Administrator Password","text":"<ol> <li>Open the AWS console and navigate to the Elastic Container Service (ECS) console.</li> <li>In the <code>Clusters</code> tab, select the name of the cluster you created</li> <li>Select the name of the jenkins service you created</li> <li>Select the <code>Logs</code> tab</li> <li>Scroll through the logs until you find the password, below is an example of what the password section looks like. Note that each line is shown as its own log entry in the console.</li> </ol>"},{"location":"modules/jenkins/jenkins.html#jenkins-initial-configuration","title":"Jenkins Initial Configuration","text":"<ol> <li>Open the Jenkins console on your preferred browser, see Accessing Jenkins for details.</li> <li>Paste the password you retrieved from the logs in the previous step into the text box and click Continue</li> <li>You will then be prompted to select the plugins you wish to install.    a. If you are unsure of which plugins to install, select <code>Install suggested plugins</code>.    b. Otherwise, select <code>Select plugins to install</code> and choose your preferred plugins.</li> <li>You are then prompted to create your first admin user.    a. Enter a username for your new user    b. Enter a password    c. Enter your full name    d. Enter you email    e. Click <code>Save and Continue</code></li> <li>For the Jenkins URL, accept the default value by clicking <code>Save and Finish</code>.</li> <li>Click <code>Start using Jenkins</code></li> </ol>"},{"location":"modules/jenkins/jenkins.html#configuring-plugins","title":"Configuring Plugins","text":"<p>There are 2 plugins recommended for the solutions: The EC2 Fleet Plugin and the AWS Secrets Manager Credentials Provider Plugin. The <code>EC2 Fleet</code> Plugin is used to integrate Jenkins with AWS and allows EC2 instances to be used as build nodes through an autoscaling group. The <code>AWS Secrets Manager Credentials Provider</code> Plugin will allow users to store their credentials in AWS Secrets Manager and seamlessly access them in Jenkins.</p>"},{"location":"modules/jenkins/jenkins.html#install-the-plugins","title":"Install the Plugins","text":"<ol> <li>Open the Jenkins console.</li> <li>On the left-hand side, select the <code>Manage Jenkins</code> tab.</li> <li>Then, under the <code>System Configuration</code> section, select <code>Plugins</code>.</li> <li>On the left-hand side, select <code>Available plugins</code>.</li> <li>Using the search bar at the top of the page, search for <code>EC2 Fleet</code>.</li> <li>Select the <code>EC2 Fleet</code> plugin.</li> <li>Using the search bar at the top of the page, search for <code>AWS Secret Manager Credentials Provider</code>.</li> <li>Select the <code>AWS Secret Manager Credentials Provider</code> plugin.</li> <li>Click <code>install</code> on the top-right corner of the page.</li> <li>Once the installation is complete, Select <code>Go back to the top page</code> at the bottom of the page</li> </ol>"},{"location":"modules/jenkins/jenkins.html#create-the-necessary-credentials","title":"Create the Necessary Credentials","text":"Using the Jenkins ConsoleUsing the AWS Secrets Manager Plugin <ol> <li>From the Jenkins homepage, on the left-hand side, select <code>Manage Jenkins</code></li> <li>Under the <code>Security</code> section, select <code>Credentials</code></li> <li>Under <code>Stores scoped to Jenkins</code>, select <code>System</code></li> <li>Select <code>Global credentials (unrestricted)</code></li> <li>In the top right corner, click the <code>Add Credentials</code> button</li> <li>For the <code>Kind</code> dropdown, select <code>SSH Username with private key</code></li> <li>For <code>ID</code> enter a name for your credentials</li> <li>For <code>Description</code> add a description of your credential</li> <li>For <code>Username</code>, enter the username to be used for the SSH connection</li> <li>For <code>Private Key</code>, select the <code>Enter directly</code> radio button.</li> <li>In the next section displayed, select the <code>Add</code> button</li> <li>Paste the Private Key created earlier into the text box.</li> <li>For <code>Passphrase</code> enter the Passphrase for the SSH key, if no passphrase was entered when creating the keys, leave this blank</li> <li>Note the <code>ID</code> of your newly created credentials. This will be referenced in the next section.</li> </ol> <ol> <li> <p>Open the Secrets Manager console.</p> </li> <li> <p>Choose Store a new secret.</p> </li> <li> <p>On the Choose secret type page, do the following:</p> <ol> <li> <p>For Secret type, choose Other type of secret.</p> </li> <li> <p>Select the <code>Plaintext</code> test tab, select all text in the textbox and delete it.</p> </li> <li> <p>Paste your Private key into the textbox</p> </li> <li> <p>Choose Next</p> </li> </ol> </li> <li> <p>On the Configure secret page, do the following:</p> <ol> <li> <p>Enter a descriptive Secret name and Description. Note that the name chosen here will also be used as the name of the credentials within Jenkins.</p> </li> <li> <p>In the Tags section, add the 2 required tags for the AWS Secrets Manager Credentials Provider Plugin</p> <ul> <li><code>jenkins:credentials:type</code> = <code>sshUserPrivateKey</code></li> <li><code>jenkins:credentials:username</code> = <code>&lt;username&gt;</code></li> </ul> <p>Info</p> <p>The username will depend on the image being used for the build agent.</p> <ul> <li>Amazon Linux -&gt; <code>ec2-user</code></li> <li>Ubuntu -&gt; <code>ubuntu</code></li> <li>Windows -&gt; <code>Administrator</code></li> </ul> </li> <li> <p>Choose Next.</p> </li> </ol> </li> <li> <p>On the Review page, review your secret details, and then choose Store.</p> </li> </ol>"},{"location":"modules/jenkins/jenkins.html#connect-jenkins-to-the-build-farm","title":"Connect Jenkins to the Build Farm","text":"<ol> <li>From the Jenkins homepage, on the left-hand side, choose <code>Manage Jenkins</code>.</li> <li>Under the <code>System Configuration</code> section, choose <code>Clouds</code></li> <li>Select <code>New Cloud</code></li> <li>Enter a name for your cloud configuration</li> <li>Select <code>Amazon EC2 Fleet</code></li> <li>Click <code>Create</code></li> <li>On the <code>New Cloud</code> configuration page, change the following settings.<ol> <li>Region - Select the region in which you deployed the module</li> <li>EC2 Fleet - Select the autoscaling group created by the module</li> <li>Launcher - Select <code>Launch agents via SSH</code></li> <li>Launcher -&gt; Credentials - Select the credentials created in the previous step</li> <li>Launcher -&gt; Host Key Verification Strategy - Select <code>Non verifying Verification Strategy</code></li> <li>Connect to instaces via private IP instead of public IP - Select the <code>Private IP</code> check box</li> <li>Max Idle Minutes Before Scaledown - Set this variable to <code>5</code> (minutes). Feel free to change this based on your needs.</li> </ol> </li> </ol>"},{"location":"modules/jenkins/terraform-docs.html","title":"Terraform Module Docs","text":""},{"location":"modules/jenkins/terraform-docs.html#jenkins","title":"Jenkins","text":""},{"location":"modules/jenkins/terraform-docs.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.59.0 random 3.6.2"},{"location":"modules/jenkins/terraform-docs.html#providers","title":"Providers","text":"Name Version aws 5.59.0 random 3.6.2"},{"location":"modules/jenkins/terraform-docs.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/jenkins/terraform-docs.html#resources","title":"Resources","text":"Name Type aws_autoscaling_group.jenkins_build_farm_asg resource aws_cloudwatch_log_group.jenkins_service_log_group resource aws_ecs_cluster.jenkins_cluster resource aws_ecs_cluster_capacity_providers.jenkins_cluster_fargate_rpvodiers resource aws_ecs_service.jenkins_service resource aws_ecs_task_definition.jenkins_task_definition resource aws_efs_access_point.jenkins_efs_access_point resource aws_efs_file_system.jenkins_efs_file_system resource aws_efs_mount_target.jenkins_efs_mount_target resource aws_fsx_openzfs_file_system.jenkins_build_farm_fsxz_file_system resource aws_fsx_openzfs_volume.jenkins_build_farm_fsxz_volume resource aws_iam_instance_profile.build_farm_instance_profile resource aws_iam_policy.build_farm_fsxz_policy resource aws_iam_policy.build_farm_s3_policy resource aws_iam_policy.ec2_fleet_plugin_policy resource aws_iam_policy.jenkins_default_policy resource aws_iam_role.build_farm_role resource aws_iam_role.jenkins_default_role resource aws_iam_role.jenkins_task_execution_role resource aws_iam_role_policy_attachment.ec2_fleet_plugin_policy_attachment resource aws_launch_template.jenkins_build_farm_launch_template resource aws_lb.jenkins_alb resource aws_lb_listener.jenkins_alb_https_listener resource aws_lb_target_group.jenkins_alb_target_group resource aws_s3_bucket.artifact_buckets resource aws_s3_bucket.jenkins_alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_s3_bucket_public_access_block.artifacts_bucket_public_block resource aws_s3_bucket_versioning.artifact_bucket_versioning resource aws_security_group.jenkins_alb_sg resource aws_security_group.jenkins_build_farm_sg resource aws_security_group.jenkins_build_storage_sg resource aws_security_group.jenkins_efs_security_group resource aws_security_group.jenkins_service_sg resource aws_vpc_security_group_egress_rule.jenkins_alb_outbound_service resource aws_vpc_security_group_egress_rule.jenkins_build_farm_outbound_ipv4 resource aws_vpc_security_group_egress_rule.jenkins_build_farm_outbound_ipv6 resource aws_vpc_security_group_egress_rule.jenkins_service_outbound_ipv4 resource aws_vpc_security_group_egress_rule.jenkins_service_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.jenkins_build_farm_inbound_ssh_service resource aws_vpc_security_group_ingress_rule.jenkins_build_vpc_all_traffic resource aws_vpc_security_group_ingress_rule.jenkins_efs_inbound_service resource aws_vpc_security_group_ingress_rule.jenkins_service_inbound_alb resource random_string.artifact_buckets resource random_string.build_farm resource random_string.fsxz resource random_string.jenkins resource random_string.jenkins_alb_access_logs_bucket_suffix resource aws_caller_identity.current data source aws_ecs_cluster.jenkins_cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.build_farm_fsxz_policy data source aws_iam_policy_document.build_farm_s3_policy data source aws_iam_policy_document.ec2_fleet_plugin_policy data source aws_iam_policy_document.ec2_trust_relationship data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.jenkins_default_policy data source aws_region.current data source aws_vpc.build_farm_vpc data source"},{"location":"modules/jenkins/terraform-docs.html#inputs","title":"Inputs","text":"Name Description Type Default Required artifact_buckets List of Amazon S3 buckets you wish to create to store build farm artifacts. <pre>map(    object({      name                 = string      enable_force_destroy = optional(bool, true)      enable_versioning    = optional(bool, true)      tags                 = optional(map(string), {})    })  )</pre> <code>null</code> no build_farm_compute Each object in this map corresponds to an ASG used by Jenkins as build agents. <pre>map(object(    {      ami = string      #TODO: Support mixed instances / spot with custom policies      instance_type     = string      ebs_optimized     = optional(bool, true)      enable_monitoring = optional(bool, true)    }  ))</pre> <code>{}</code> no build_farm_fsx_openzfs_storage Each object in this map corresponds to an FSx OpenZFS file system used by the Jenkins build agents. <pre>map(object(    {      storage_capacity    = number      throughput_capacity = number      storage_type        = optional(string, \"SSD\") # \"SSD\", \"HDD\"      deployment_type     = optional(string, \"SINGLE_AZ_1\")      route_table_ids     = optional(list(string), null)      tags                = optional(map(string), null)    }  ))</pre> <code>{}</code> no build_farm_subnets The subnets to deploy the build farms into. <code>list(string)</code> n/a yes certificate_arn The TLS certificate ARN for the Jenkins service load balancer. <code>string</code> n/a yes cluster_name The ARN of the cluster to deploy the Jenkins service into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no container_cpu The CPU allotment for the Jenkins container. <code>number</code> <code>1024</code> no container_memory The memory allotment for the Jenkins container. <code>number</code> <code>4096</code> no container_name The name of the Jenkins service container. <code>string</code> <code>\"jenkins-container\"</code> no container_port The container port used by the Jenkins service container. <code>number</code> <code>8080</code> no create_ec2_fleet_plugin_policy Optional creation of IAM Policy required for Jenkins EC2 Fleet plugin. Default is set to false. <code>bool</code> <code>false</code> no create_jenkins_default_policy Optional creation of Jenkins Default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_jenkins_default_role Optional creation of Jenkins Default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_jenkins_role ARN of the custom IAM Role you wish to use with Jenkins. <code>string</code> <code>null</code> no enable_jenkins_alb_access_logs Enables access logging for the Jenkins ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_jenkins_alb_deletion_protection Enables deletion protection for the Jenkins ALB. Defaults to true. <code>bool</code> <code>true</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_artifact_buckets List of ARNs of the S3 buckets used to store artifacts created by the build farm. <code>list(string)</code> <code>[]</code> no existing_security_groups A list of existing security group IDs to attach to the Jenkins service load balancer. <code>list(string)</code> <code>null</code> no internal Set this flag to true if you do not want the Jenkins service load balancer to have a public IP. <code>bool</code> <code>false</code> no jenkins_agent_secret_arns A list of secretmanager ARNs (wildcards allowed) that contain any secrets which need to be accessed by the Jenkins service. <code>list(string)</code> <code>null</code> no jenkins_alb_access_logs_bucket ID of the S3 bucket for Jenkins ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no jenkins_alb_access_logs_prefix Log prefix for Jenkins ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no jenkins_alb_subnets A list of subnet ids to deploy the Jenkins load balancer into. Public subnets are recommended. <code>list(string)</code> n/a yes jenkins_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Jenkins. <code>string</code> <code>365</code> no jenkins_efs_performance_mode The performance mode of the EFS file system used by the Jenkins service. Defaults to general purpose. <code>string</code> <code>\"generalPurpose\"</code> no jenkins_efs_throughput_mode The throughput mode of the EFS file system used by the Jenkins service. Defaults to bursting. <code>string</code> <code>\"bursting\"</code> no jenkins_service_desired_container_count The desired number of containers running the Jenkins service. <code>number</code> <code>1</code> no jenkins_service_subnets A list of subnets to deploy the Jenkins service into. Private subnets are recommended. <code>list(string)</code> n/a yes name The name attached to Jenkins module resources. <code>string</code> <code>\"jenkins\"</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IAC_MANAGEMENT\": \"CGD-Toolkit\",  \"IAC_MODULE\": \"Jenkins\",  \"IAC_PROVIDER\": \"Terraform\"}</pre> no vpc_id The ID of the existing VPC you would like to deploy the Jenkins service and build farms into. <code>string</code> n/a yes"},{"location":"modules/jenkins/terraform-docs.html#outputs","title":"Outputs","text":"Name Description alb_security_group Security group associated with the Jenkins load balancer build_farm_security_group Security group associated with the build farm autoscaling groups jenkins_alb_dns_name The DNS name of the Jenkins application load balancer. jenkins_alb_zone_id The zone ID of the Jenkins ALB. service_security_group Security group associated with the ECS service hosting jenkins"},{"location":"modules/perforce/helix-authentication-service/helix-authentication-service.html","title":"Helix Authentication Service (HAS)","text":"<p>Jump to Terraform docs</p> <p>Perforce Helix Authentication Service enables you to integrate certain Perforce products with your organization's Identity Provider (IdP).</p> <p>This module creates the following resources:</p> <ul> <li>An Elastic Container Service (ECS) cluster backed by AWS Fargate. This can also be created externally and passed in via the <code>cluster_name</code> variable.</li> <li>An ECS service running the latest Helix Authentication Service container (perforce/helix-auth-svc) available.</li> <li>AWS Secrets Manager secrets for an administrative user that has access to the Helix Authentication Service's web UI. These credentials are needed to configure external identity providers through the UI.</li> <li>An Application Load Balancer for TLS termination of the Helix Authentication Service.</li> <li>Supporting resources such as Cloudwatch log groups, IAM roles, and security groups.</li> </ul>"},{"location":"modules/perforce/helix-authentication-service/helix-authentication-service.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/perforce/helix-authentication-service/helix-authentication-service.html#prerequisites","title":"Prerequisites","text":"<p>The Helix Authentication Service can be configured at deployment time or through the web UI following deployment. If you opt to configure Helix Authentication Service through the web-based UI you will need to create an administrative user for initial login. You can either create and upload these credentials to AWS Secrets Manager yourself, or you opt to have the module create these credentials for you.</p> <p>Should you choose to create this administrative user yourself you will need to specify the ARN for the username and password as module variables. You can create the secret using the AWS CLI:</p> <pre><code>aws secretsmanager create-secret \\\n    --name HelixAuthAdmin \\\n    --description \"Helix Authentication Service Admin\" \\\n    --secret-string \"{\\\"username\\\":\\\"admin\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\"\n</code></pre> <p>And then provide the relevant ARNs as variables when you define the Helix Authentication module in your Terraform configurations:</p> <pre><code>module \"perforce_helix_authentication_service\" {\n    source = \"modules/perforce/helix-authentication-service\"\n    ...\n    helix_authentication_service_admin_username_secret_arn = \"arn:aws:secretsmanager:us-west-2:123456789012:secret:HelixAuthAdmin-a1b2c3:username::\"\n    helix_authentication_service_admin_password_secret_arn = \"arn:aws:secretsmanager:us-west-2:123456789012:secret:HelixAuthAdmin-a1b2c3:password::\"\n}\n</code></pre> <p>If you do not provide these the module will create a random Super User and create the secret for you. The ARN of this secret is then available as an output to be referenced elsewhere, and can be accessed from the AWS Secrets Manager console.</p>"},{"location":"modules/perforce/helix-authentication-service/terraform-docs.html","title":"Terraform Module Docs","text":""},{"location":"modules/perforce/helix-authentication-service/terraform-docs.html#perforce-helix-authentication-service-module","title":"Perforce Helix Authentication Service Module","text":""},{"location":"modules/perforce/helix-authentication-service/terraform-docs.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.59.0 awscc 1.6.0 random 3.6.2"},{"location":"modules/perforce/helix-authentication-service/terraform-docs.html#providers","title":"Providers","text":"Name Version aws 5.59.0 awscc 1.6.0 random 3.6.2"},{"location":"modules/perforce/helix-authentication-service/terraform-docs.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/helix-authentication-service/terraform-docs.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.helix_authentication_service_log_group resource aws_ecs_cluster.helix_authentication_service_cluster resource aws_ecs_cluster_capacity_providers.helix_authentication_service_cluster_fargate_providers resource aws_ecs_service.helix_authentication_service resource aws_ecs_task_definition.helix_authentication_service_task_definition resource aws_iam_policy.helix_authentication_service_default_policy resource aws_iam_policy.helix_authentication_service_secrets_manager_policy resource aws_iam_role.helix_authentication_service_default_role resource aws_iam_role.helix_authentication_service_task_execution_role resource aws_lb.helix_authentication_service_alb resource aws_lb_listener.helix_authentication_service_alb_https_listener resource aws_lb_target_group.helix_authentication_service_alb_target_group resource aws_s3_bucket.helix_authentication_service_alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_security_group.helix_authentication_service_alb_sg resource aws_security_group.helix_authentication_service_sg resource aws_vpc_security_group_egress_rule.helix_authentication_service_alb_outbound_service resource aws_vpc_security_group_egress_rule.helix_authentication_service_outbound_ipv4 resource aws_vpc_security_group_egress_rule.helix_authentication_service_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.helix_authentication_service_inbound_alb resource awscc_secretsmanager_secret.helix_authentication_service_admin_password resource awscc_secretsmanager_secret.helix_authentication_service_admin_username resource random_string.helix_authentication_service resource random_string.helix_authentication_service_alb_access_logs_bucket_suffix resource aws_ecs_cluster.helix_authentication_service_cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.helix_authentication_service_default_policy data source aws_iam_policy_document.helix_authentication_service_secrets_manager_policy data source aws_region.current data source"},{"location":"modules/perforce/helix-authentication-service/terraform-docs.html#inputs","title":"Inputs","text":"Name Description Type Default Required certificate_arn The TLS certificate ARN for the Helix Authentication Service load balancer. <code>string</code> n/a yes cluster_name The name of the cluster to deploy the Helix Authentication Service into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no container_cpu The CPU allotment for the Helix Authentication Service container. <code>number</code> <code>1024</code> no container_memory The memory allotment for the Helix Authentication Service container. <code>number</code> <code>4096</code> no container_name The name of the Helix Authentication Service container. <code>string</code> <code>\"helix-auth-container\"</code> no container_port The container port that Helix Authentication Service runs on. <code>number</code> <code>3000</code> no create_helix_authentication_service_default_policy Optional creation of Helix Authentication Service default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_helix_authentication_service_default_role Optional creation of Helix Authentication Service default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_helix_authentication_service_role ARN of the custom IAM Role you wish to use with Helix Authentication Service. <code>string</code> <code>null</code> no desired_container_count The desired number of containers running the Helix Authentication Service. <code>number</code> <code>1</code> no enable_helix_authentication_service_alb_access_logs Enables access logging for the Helix Authentication Service ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_helix_authentication_service_alb_deletion_protection Enables deletion protection for the Helix Authentication Service ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_web_based_administration Flag for enabling web based administration of Helix Authentication Service. <code>bool</code> <code>false</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_security_groups A list of existing security group IDs to attach to the Helix Authentication Service load balancer. <code>list(string)</code> <code>[]</code> no fqdn The fully qualified domain name of Helix Authentication Service. <code>string</code> <code>\"localhost\"</code> no helix_authentication_service_admin_password_secret_arn Optionally provide the ARN of an AWS Secret for the Helix Authentication Service Administrator password. <code>string</code> <code>null</code> no helix_authentication_service_admin_username_secret_arn Optionally provide the ARN of an AWS Secret for the Helix Authentication Service Administrator username. <code>string</code> <code>null</code> no helix_authentication_service_alb_access_logs_bucket ID of the S3 bucket for Helix Authentication Service ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no helix_authentication_service_alb_access_logs_prefix Log prefix for Helix Authentication Service ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no helix_authentication_service_alb_subnets A list of subnets to deploy the Helix Authentication Service load balancer into. Public subnets are recommended. <code>list(string)</code> n/a yes helix_authentication_service_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Helix Authentication Service. <code>string</code> <code>365</code> no helix_authentication_service_subnets A list of subnets to deploy the Helix Authentication Service into. Private subnets are recommended. <code>list(string)</code> n/a yes internal Set this flag to true if you do not want the Helix Authentication Service load balancer to have a public IP. <code>bool</code> <code>false</code> no name The name attached to Helix Authentication Service module resources. <code>string</code> <code>\"helix-auth-svc\"</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IAC_MANAGEMENT\": \"CGD-Toolkit\",  \"IAC_MODULE\": \"helix-authentication-service\",  \"IAC_PROVIDER\": \"Terraform\"}</pre> no vpc_id The ID of the existing VPC you would like to deploy Helix Authentication Service into. <code>string</code> n/a yes"},{"location":"modules/perforce/helix-authentication-service/terraform-docs.html#outputs","title":"Outputs","text":"Name Description alb_dns_name The DNS name of the Helix Authentication Service ALB alb_security_group_id Security group associated with the Helix Authentication Service load balancer alb_zone_id The hosted zone ID of the Helix Authentication Service ALB cluster_name Name of the ECS cluster hosting helix_authentication_service service_security_group_id Security group associated with the ECS service running Helix Authentication Service"},{"location":"modules/perforce/helix-core/helix-core.html","title":"Perforce Helix Core","text":"<p>Jump to Terraform docs</p> <p>Perforce Helix Core is a scalable version control system that helps teams manage code alongside large, digital assets and collaborate more effectively in one central, secure location. With AWS, teams can quickly deploy Helix Core and accelerate innovation.</p> <p>This module provisions Perforce Helix Core on an EC2 Instance with three dedicated EBS volumes for Helix Core depots, metadata, and logs. It can also be configured to automatically install the required plugins to integrate with Perforce Helix Authentication Service. This allows end users to quickly set up single-sign-on for their Perforce Helix Core server.</p>"},{"location":"modules/perforce/helix-core/helix-core.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/perforce/helix-core/helix-core.html#prerequisites","title":"Prerequisites","text":"<p>This module deploys Perforce Helix Core on AWS using an Amazon Machine Image (AMI) that is included in the Cloud Game Development Toolkit. You must provision this AMI using Hashicorp Packer prior to deploying this module. To get started consult the documentation for the Perforce Helix Core AMI.</p>"},{"location":"modules/perforce/helix-core/helix-core.html#optional","title":"Optional","text":"<p>You can optionally define the Helix Core super user's credentials prior to deployment. To do so, create a secret for the Helix Core super user's username and password:</p> <pre><code>aws secretsmanager create-secret \\\n    --name HelixCoreSuperUser \\\n    --description \"Helix Core Super User\" \\\n    --secret-string \"{\\\"username\\\":\\\"admin\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\"\n</code></pre> <p>You can then provide the relevant ARN as variables when you define the Helix Core module in your Terraform configurations:</p> <pre><code>module \"perforce_helix_core\" {\n    source = \"modules/perforce/helix-core\"\n    ...\n    helix_core_super_user_username_arn = \"arn:aws:secretsmanager:us-west-2:123456789012:secret:HelixCoreSuperUser-a1b2c3:username::\"\n    helix_core_super_user_password_arn = \"arn:aws:secretsmanager:us-west-2:123456789012:secret:HelixCoreSuperUser-a1b2c3:password::\"\n}\n</code></pre> <p>If you do not provide these the module will create a random Super User and create the secret for you. The ARN of this secret is then available as an output to be referenced elsewhere.</p>"},{"location":"modules/perforce/helix-core/terraform-docs.html","title":"Helix Core - Terraform Module Docs","text":""},{"location":"modules/perforce/helix-core/terraform-docs.html#perforce-helix-core-module","title":"Perforce Helix Core Module","text":""},{"location":"modules/perforce/helix-core/terraform-docs.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.59.0 awscc 1.6.0 random 3.6.2"},{"location":"modules/perforce/helix-core/terraform-docs.html#providers","title":"Providers","text":"Name Version aws 5.59.0 awscc 1.6.0 random 3.6.2"},{"location":"modules/perforce/helix-core/terraform-docs.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/helix-core/terraform-docs.html#resources","title":"Resources","text":"Name Type aws_ebs_volume.depot resource aws_ebs_volume.logs resource aws_ebs_volume.metadata resource aws_eip.helix_core_eip resource aws_iam_instance_profile.helix_core_instance_profile resource aws_iam_policy.helix_core_default_policy resource aws_iam_role.helix_core_default_role resource aws_instance.helix_core_instance resource aws_security_group.helix_core_security_group resource aws_volume_attachment.depot_attachment resource aws_volume_attachment.logs_attachment resource aws_volume_attachment.metadata_attachment resource aws_vpc_security_group_egress_rule.helix_core_internet resource awscc_secretsmanager_secret.helix_core_super_user_password resource awscc_secretsmanager_secret.helix_core_super_user_username resource random_string.helix_core resource aws_ami.helix_core_ami data source aws_iam_policy_document.ec2_trust_relationship data source aws_iam_policy_document.helix_core_default_policy data source aws_subnet.instance_subnet data source"},{"location":"modules/perforce/helix-core/terraform-docs.html#inputs","title":"Inputs","text":"Name Description Type Default Required FQDN The FQDN that should be used to generate the self-signed SSL cert on the Helix Core instance. <code>string</code> <code>null</code> no create_default_sg Whether to create a default security group for the Helix Core instance. <code>bool</code> <code>true</code> no create_helix_core_default_role Optional creation of Helix Core default IAM Role with SSM managed instance core policy attached. Default is set to true. <code>bool</code> <code>true</code> no custom_helix_core_role ARN of the custom IAM Role you wish to use with Helix Core. <code>string</code> <code>null</code> no depot_volume_size The size of the depot volume in GiB. Defaults to 128 GiB. <code>number</code> <code>128</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_security_groups A list of existing security group IDs to attach to the Helix Core load balancer. <code>list(string)</code> <code>[]</code> no helix_authentication_service_url The URL for the Helix Authentication Service. <code>string</code> <code>null</code> no helix_core_super_user_password_secret_arn If you would like to manage your own super user credentials through AWS Secrets Manager provide the ARN for the super user's password here. <code>string</code> <code>null</code> no helix_core_super_user_username_secret_arn If you would like to manage your own super user credentials through AWS Secrets Manager provide the ARN for the super user's username here. Otherwise, the default of 'perforce' will be used. <code>string</code> <code>null</code> no instance_subnet_id The subnet where the Helix Core instance will be deployed. <code>string</code> n/a yes instance_type The instance type for Perforce Helix Core. Defaults to c6in.large. <code>string</code> <code>\"c6in.large\"</code> no internal Set this flag to true if you do not want the Helix Core instance to have a public IP. <code>bool</code> <code>false</code> no logs_volume_size The size of the logs volume in GiB. Defaults to 32 GiB. <code>number</code> <code>32</code> no metadata_volume_size The size of the metadata volume in GiB. Defaults to 32 GiB. <code>number</code> <code>32</code> no name The name attached to swarm module resources. <code>string</code> <code>\"helix-core\"</code> no project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no server_type The Perforce Helix Core server type. <code>string</code> n/a yes storage_type The type of backing store [EBS, FSxZ] <code>string</code> n/a yes tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IAC_MANAGEMENT\": \"CGD-Toolkit\",  \"IAC_MODULE\": \"helix-core\",  \"IAC_PROVIDER\": \"Terraform\"}</pre> no vpc_id The VPC where Helix Core should be deployed <code>string</code> n/a yes"},{"location":"modules/perforce/helix-core/terraform-docs.html#outputs","title":"Outputs","text":"Name Description helix_core_eip_id The ID of the Elastic IP associated with your Helix Core instance. helix_core_eip_private_ip The private IP of your Helix Core instance. helix_core_eip_public_ip The public IP of your Helix Core instance. helix_core_super_user_password_secret_arn The ARN of the AWS Secrets Manager secret holding your Helix Core super user's password. helix_core_super_user_username_secret_arn The ARN of the AWS Secrets Manager secret holding your Helix Core super user's username. security_group_id The default security group of your Helix Core instance."},{"location":"modules/perforce/helix-swarm/helix-swarm.html","title":"Perforce Helix Swarm","text":"<p>Jump to Terraform docs</p> <p>Perforce Helix Swarm is a free code review tool for projects hosted in Perforce Helix Core. This module deploys Helix Swarm as a service on AWS Elastic Container Service using the publicly available image from Dockerhub.</p> <p>Helix Swarm also relies on a Redis cache. The module runs Redis as a service alongside Helix Swarm as part of the same task definition.</p> <p>This module deploys the following resources:</p> <ul> <li>An Elastic Container Service (ECS) cluster backed by AWS Fargate. This can also be created externally and passed in via the <code>cluster_name</code> variable.</li> <li>An ECS service running the latest Helix Swarm container (perforce/helix-swarm) available and a Redis sidecar.</li> <li>An Application Load Balancer for TLS termination of the Helix Swarm service.</li> <li>Supporting resources such as Cloudwatch log groups, IAM roles, and security groups.</li> </ul>"},{"location":"modules/perforce/helix-swarm/helix-swarm.html#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"modules/perforce/helix-swarm/helix-swarm.html#prerequisites","title":"Prerequisites","text":"<p>Perforce Helix Swarm needs to be able to connect to a Perforce Helix Core server. Helix Swarm leverages the same authentication mechanism as Helix Core, and needs to install required plugins on the upstream Helix Core instance during setup. This happens automatically, but Swarm requires an administrative user's credentials to be able to initially connect. These credentials are provided to the module through variables specifying AWS Secrets Manager secrets, and then pulled into the Helix Swarm container during startup. See the <code>p4d_super_user_arn</code>, <code>p4d_super_user_password_arn</code>, <code>p4d_swarm_user_arn</code>, and <code>p4d_swarm_password_arn</code> variables below for more details.</p> <p>The Helix Core module creates an administrative user on initial deployment, and stores the credentials in AWS Secrets manager. The ARN of the credentials secret is then made available as a Terraform output from the module, and can be referenced elsewhere.</p> <p>Should you need to manually create the administrative user secret the following AWS CLI command may prove useful:</p> <pre><code>aws secretsmanager create-secret \\\n    --name HelixSwarmSuperUser \\\n    --description \"Helix Swarm Super User\" \\\n    --secret-string \"{\\\"username\\\":\\\"swarm\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\"\n</code></pre> <p>You can then provide these credentials as variables when you define the Helix Swarm module in your terraform configurations:</p> <pre><code>module \"perforce_helix_swarm\" {\n    source = \"modules/perforce/helix-swarm\"\n    ...\n    p4d_super_user_arn = \"arn:aws:secretsmanager:us-west-2:123456789012:secret:HelixSwarmSuperUser-a1b2c3:username::\"\n    p4d_super_user_password_arn = \"arn:aws:secretsmanager:us-west-2:123456789012:secret:HelixSwarmSuperUser-a1b2c3:password::\"\n}\n</code></pre>"},{"location":"modules/perforce/helix-swarm/terraform-docs.html","title":"Terraform Module Docs","text":""},{"location":"modules/perforce/helix-swarm/terraform-docs.html#perforce-helix-swarm","title":"Perforce Helix Swarm","text":""},{"location":"modules/perforce/helix-swarm/terraform-docs.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 5.59.0 random 3.6.2"},{"location":"modules/perforce/helix-swarm/terraform-docs.html#providers","title":"Providers","text":"Name Version aws 5.59.0 random 3.6.2"},{"location":"modules/perforce/helix-swarm/terraform-docs.html#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/perforce/helix-swarm/terraform-docs.html#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.helix_swarm_redis_service_log_group resource aws_cloudwatch_log_group.helix_swarm_service_log_group resource aws_ecs_cluster.helix_swarm_cluster resource aws_ecs_cluster_capacity_providers.helix_swarm_cluster_fargate_providers resource aws_ecs_service.helix_swarm_service resource aws_ecs_task_definition.helix_swarm_task_definition resource aws_efs_access_point.helix_swarm_efs_access_point resource aws_efs_access_point.redis_efs_access_point resource aws_efs_file_system.helix_swarm_efs_file_system resource aws_efs_mount_target.helix_swarm_efs_mount_target resource aws_iam_policy.helix_swarm_default_policy resource aws_iam_policy.helix_swarm_efs_policy resource aws_iam_policy.helix_swarm_ssm_policy resource aws_iam_role.helix_swarm_default_role resource aws_iam_role.helix_swarm_task_execution_role resource aws_lb.helix_swarm_alb resource aws_lb_listener.swarm_alb_https_listener resource aws_lb_target_group.helix_swarm_alb_target_group resource aws_s3_bucket.helix_swarm_alb_access_logs_bucket resource aws_s3_bucket_lifecycle_configuration.access_logs_bucket_lifecycle_configuration resource aws_s3_bucket_policy.alb_access_logs_bucket_policy resource aws_s3_bucket_public_access_block.access_logs_bucket_public_block resource aws_security_group.helix_swarm_alb_sg resource aws_security_group.helix_swarm_efs_security_group resource aws_security_group.helix_swarm_service_sg resource aws_vpc_security_group_egress_rule.helix_swarm_alb_outbound_service resource aws_vpc_security_group_egress_rule.helix_swarm_service_outbound_ipv4 resource aws_vpc_security_group_egress_rule.helix_swarm_service_outbound_ipv6 resource aws_vpc_security_group_ingress_rule.helix_swarm_efs_inbound_service resource aws_vpc_security_group_ingress_rule.helix_swarm_service_inbound_alb resource random_string.helix_swarm resource random_string.helix_swarm_alb_access_logs_bucket_suffix resource aws_ecs_cluster.helix_swarm_cluster data source aws_elb_service_account.main data source aws_iam_policy_document.access_logs_bucket_alb_write data source aws_iam_policy_document.ecs_tasks_trust_relationship data source aws_iam_policy_document.helix_swarm_default_policy data source aws_iam_policy_document.helix_swarm_efs_policy data source aws_iam_policy_document.helix_swarm_ssm_policy data source aws_region.current data source"},{"location":"modules/perforce/helix-swarm/terraform-docs.html#inputs","title":"Inputs","text":"Name Description Type Default Required certificate_arn The TLS certificate ARN for the Helix Swarm service load balancer. <code>string</code> n/a yes cluster_name The name of the cluster to deploy the Helix Swarm service into. Defaults to null and a cluster will be created. <code>string</code> <code>null</code> no create_helix_swarm_default_policy Optional creation of Helix Swarm default IAM Policy. Default is set to true. <code>bool</code> <code>true</code> no create_helix_swarm_default_role Optional creation of Helix Swarm Default IAM Role. Default is set to true. <code>bool</code> <code>true</code> no custom_helix_swarm_role ARN of the custom IAM Role you wish to use with Helix Swarm. <code>string</code> <code>null</code> no enable_elastic_filesystem Flag to enable/disable elastic filesystem for persistent storage. Defaults to false. <code>bool</code> <code>false</code> no enable_helix_swarm_alb_access_logs Enables access logging for the Helix Swarm ALB. Defaults to true. <code>bool</code> <code>true</code> no enable_helix_swarm_alb_deletion_protection Enables deletion protection for the Helix Swarm ALB. Defaults to true. <code>bool</code> <code>true</code> no environment The current environment (e.g. dev, prod, etc.) <code>string</code> <code>\"dev\"</code> no existing_redis_host The hostname where the Redis cache that Swarm should use is running. <code>string</code> <code>null</code> no existing_security_groups A list of existing security group IDs to attach to the Helix Swarm service load balancer. <code>list(string)</code> <code>[]</code> no fqdn The fully qualified domain name that Swarm should use for internal URLs. <code>string</code> <code>null</code> no helix_swarm_alb_access_logs_bucket ID of the S3 bucket for Helix Swarm ALB access log storage. If access logging is enabled and this is null the module creates a bucket. <code>string</code> <code>null</code> no helix_swarm_alb_access_logs_prefix Log prefix for Helix Swarm ALB access logs. If null the project prefix and module name are used. <code>string</code> <code>null</code> no helix_swarm_alb_subnets A list of subnets to deploy the Helix Swarm load balancer into. Public subnets are recommended. <code>list(string)</code> n/a yes helix_swarm_cloudwatch_log_retention_in_days The log retention in days of the cloudwatch log group for Helix Swarm. <code>string</code> <code>365</code> no helix_swarm_container_cpu The CPU allotment for the swarm container. <code>number</code> <code>1024</code> no helix_swarm_container_memory The memory allotment for the swarm container. <code>number</code> <code>2048</code> no helix_swarm_container_name The name of the swarm container. <code>string</code> <code>\"helix-swarm-container\"</code> no helix_swarm_container_port The container port that swarm runs on. <code>number</code> <code>80</code> no helix_swarm_desired_container_count The desired number of containers running the Helix Swarm service. <code>number</code> <code>1</code> no helix_swarm_efs_performance_mode The performance mode of the EFS file system used by the Helix Swarm service. Defaults to general purpose. <code>string</code> <code>\"generalPurpose\"</code> no helix_swarm_efs_throughput_mode The throughput mode of the EFS file system used by the Helix Swarm service. Defaults to bursting. <code>string</code> <code>\"bursting\"</code> no helix_swarm_service_subnets A list of subnets to deploy the Helix Swarm service into. Private subnets are recommended. <code>list(string)</code> n/a yes internal Set this flag to true if you do not want the Helix Swarm service load balancer to have a public IP. <code>bool</code> <code>false</code> no name The name attached to swarm module resources. <code>string</code> <code>\"swarm\"</code> no p4d_port The P4D_PORT environment variable where Swarm should look for Helix Core. Defaults to 'ssl:perforce:1666' <code>string</code> <code>\"ssl:perforce:1666\"</code> no p4d_super_user_arn The ARN of the parameter or secret where the p4d super user username is stored. <code>string</code> n/a yes p4d_super_user_password_arn The ARN of the parameter or secret where the p4d super user password is stored. <code>string</code> n/a yes p4d_swarm_password_arn The ARN of the parameter or secret where the swarm user password is stored. <code>string</code> n/a yes p4d_swarm_user_arn The ARN of the parameter or secret where the swarm user username is stored. <code>string</code> n/a yes project_prefix The project prefix for this workload. This is appeneded to the beginning of most resource names. <code>string</code> <code>\"cgd\"</code> no redis_container_cpu CPU allotment for Helix Swarm Redis container. <code>number</code> <code>1024</code> no redis_container_memory Memory allotment for Helix Swarm Redis container. <code>number</code> <code>2048</code> no redis_container_name The name of the Redis container. <code>string</code> <code>\"swarm-redis\"</code> no redis_container_port The port where the Redis cache that Swarm should use is running. <code>number</code> <code>6379</code> no redis_image The Redis image and version that Helix Swarm should use. <code>string</code> <code>\"redis\"</code> no tags Tags to apply to resources. <code>map(any)</code> <pre>{  \"IAC_MANAGEMENT\": \"CGD-Toolkit\",  \"IAC_MODULE\": \"swarm\",  \"IAC_PROVIDER\": \"Terraform\"}</pre> no task_cpu The CPU allotment for the Helix Swarm task. <code>number</code> <code>2048</code> no task_memory The memory allotment for the Helix Swarm task. <code>number</code> <code>4096</code> no vpc_id The ID of the existing VPC you would like to deploy swarm into. <code>string</code> n/a yes"},{"location":"modules/perforce/helix-swarm/terraform-docs.html#outputs","title":"Outputs","text":"Name Description alb_dns_name The DNS name of the Swarm ALB alb_security_group_id Security group associated with the swarm load balancer alb_zone_id The hosted zone ID of the Swarm ALB cluster_name Name of the ECS cluster hosting Swarm service_security_group_id Security group associated with the ECS service running swarm"},{"location":"samples/index.html","title":"Overview","text":"<p>Samples represent a reference implementation that can be deployed to solve for a specific use-case or workload. These are Terraform configurations and integrations with other common AWS workloads and services. Each sample will provide its own documentation and instructions that follows the template below:</p>"},{"location":"samples/index.html#1-predeployment","title":"1) Predeployment","text":"<p>In the predeployment phase the user is instructed to provision or take note of any necessary pre-existing resources. Creating SSL certificates or keypairs, provisioning Amazon Machine Images (AMIs) with Packer, or documenting existing resource IDs and names all fall into this phase.</p>"},{"location":"samples/index.html#2-deployment","title":"2) Deployment","text":"<p>In the deployment phase the user is instructed to run <code>terraform apply</code> on one or more Terraform configurations with the appropriate variables.</p>"},{"location":"samples/index.html#3-postdeployment","title":"3) Postdeployment","text":"<p>Finally, the postdeployment phase includes any Ansible playbooks or remote execution instructions for configuring the applications that have been deployed. These may be automated or manual steps.</p>"},{"location":"samples/simple-build-pipeline.html","title":"Simple Build Pipeline Sample","text":"<p>The Simple Build Pipeline is the best place to get started when first exploring the Cloud Game Development Toolkit. It encapsulates all of the available modules alongside best practice deployments of core AWS services. The Simple Build Pipeline provisions a well-architected Virtual Private Cloud (VPC), a skeleton for managing DNS and SSL certificates with Route 53 and AWS Certificate Manager(ACM), Jenkins for continuous integration and deployment, Perforce Helix Core for version control, Perforce Helix Swarm for code review, and Perforce Helix Authentication Service for external identity provider integrations.</p> <p>We recommend that you fork the Cloud Game Development Toolkit and then use this Simple Build Pipeline as the starting point for managing your studio's infrastructure.</p>"},{"location":"samples/simple-build-pipeline.html#predeployment","title":"Predeployment","text":"<p>There are a few prerequisites that need to be completed prior to deploying this sample architecture. We'll walk through those here.</p> <ol> <li>Domain Name System (DNS) Resolution</li> </ol> <p>The Simple Build Pipeline will deploy a number of web-based applications into your AWS account. The Cloud Game Development Toolkit attempts to follow a \"secure-by-default\" design pattern, so HTTPS is the standard protocol for all applications deployed by the Toolkit.</p> <p>All applications deployed by the Simple Build Pipeline use Route 53, Amazon's highly available and scalable DNS service, for routing traffic from the internet. All you need to provide is the root hosted zone you would like the Simple Build Pipeline to use, and your Jenkins and Perforce applications will be provisioned as sub-domains. Route53 will need to be able to validate ownership of this domain, so make sure you are using a domain that you have complete control over.</p> <p>If you do not have a domain yet you can register one through Route53.</p> <p>If you already have a domain with a different domain registrar you can leverage Route53 for DNS services. Please review the documentation for migrating to Route53 as your DNS provider.</p> <p>Regardless, the Simple Build Pipeline requires the fully qualified domain name (FQDN) that you would like resources to be provisioned under. If you own the domain: \"example.com\" the simple build pipeline would deploy Jenkins to \"jenkins.example.com\" and Helix Swarm to \"swarm.helix.example.com\" - this can be modified from the <code>dns.tf</code> file.</p> <ol> <li>Jenkins Build Farm</li> </ol> <p>The Jenkins module provisions the Jenkins coordinator as a service on Amazon Elastic Container Service (ECS). It also provisions any number of EC2 autoscaling groups to be used as build nodes by Jenkins, and any number of Amazon FSx for OpenZFS filesystems to be used as shared storage by those build nodes.</p> <p>By default, the Simple Build Pipelines will not provision any autoscaling groups or any filesystems. These are configured through the <code>local.tf</code> file.</p> <p>To add an autoscaling group you need to specify an instance type and an Amazon Machine Image (AMI). The instance type specifies the hardware of your build nodes, and the AMI contains all of the tooling and software you would like those machines to contain on startup. The Cloud Game Development Toolkit provides a number of Packer templates for useful game development AMIs. We recommend reviewing this documentation before adding build nodes to your Simple Build Pipeline.</p> <p>Your build nodes may also need access to credentials or other secrets. These can be uploaded to AWS Secrets Manager and then passed to your Jenkins build nodes with the <code>jenkins_agent_secret_arns</code> local variable.</p>"},{"location":"samples/simple-build-pipeline.html#deployment","title":"Deployment","text":"<p>Deployment of the Simple Build Pipeline is relatively straightforward once you have completed the prerequisites. The necessary variables can be passed to Terraform configuration through the variables in the Simple Build Pipeline's local.tf file.</p> <pre><code># local.tf\n\nlocals {\n\n  fully_qualified_domain_name = \"www.example.com\"\n\n  build_farm_compute = {\n      graviton_builders : {\n          ami = ami-0a1b2c3d4e5f\n          instance_type = c7g.large\n      }\n      windows_builders : {\n          ami = ami-9z8y7x6w5v\n          instance_type = c7a.large\n      }\n  }\n\n  build_farm_fsx_openzfs_storage = {\n      cache : {\n        storage_type        = \"SSD\"\n        throughput_capacity = 160\n        storage_capacity    = 256\n        deployment_type     = \"MULTI_AZ_1\"\n        route_table_ids     = [aws_route_table.private_rt.id]\n      }\n      workspace : {\n        storage_type        = \"SSD\"\n        throughput_capacity = 160\n        storage_capacity    = 564\n        deployment_type     = \"MULTI_AZ_1\"\n        route_table_ids     = [aws_route_table.private_rt.id]\n      }\n  }\n\n\n}\n</code></pre> <p>Once you have defined your variables deploying the sample is as easy as running the following:</p> <pre><code>terraform apply\n</code></pre> <p>The deployment can take close to ten minutes. Creating the certificates and performing DNS validation against them generally is the last thing to complete. This happens automatically.</p>"},{"location":"samples/simple-build-pipeline.html#postdeployment","title":"Postdeployment","text":"<p>After the Simple Build Pipeline deploys you still need to configure the underlying applications.</p> <ol> <li>Jenkins</li> </ol> <p>Jenkins requires a couple of plugins to be able to send build jobs to the provisioned autoscaling groups. These plugins are outlined in the Jenkins documentation. In the future we will automate the installation of these plugins.</p> <p>To gain access to your new Jenkins deployment you will need to modify the <code>jenkins ALB security group</code> to allowlist your IP address. We recommend doing this with Terraform.</p> <p>Navigate to <code>main.tf</code> and add the following block at the bottom. Make sure to replace \"IP_PLACEHOLDER\" with your IP address.</p> <pre><code>resource \"aws_vpc_security_group_ingress_rule\" \"jenkins_inbound_personal\" {\n  security_group_id            = module.jenkins.alb_security_group\n  ip_protocol                  = \"TCP\"\n  from_port                    = 443\n  to_port                      = 443\n  cidr_blocks                  = [\"IP_PLACEHOLDER/32\"]\n  description                  = \"Grants personal access to Jenkins.\"\n}\n</code></pre> <p>You will need to run <code>terraform apply</code> to deploy this change.</p> <p>Now that you are able to access Jenkins you'll need to configure the plugins, cloud based agents, and credentials that Jenkins has access to. Please consult the Jenkins documentation for these steps.</p> <ol> <li>Helix Authentication Service</li> </ol> <p>In order to use your external identity provider with Helix Core and Helix Swarm you will need to configure a OIDC or SAML connection in the Helix Authentication Service. The Helix Authentication Service module provides a web-based UI to do this.</p> <p>To gain access to this UI you will need to modify the <code>helix authentication service ALB security group</code> to allowlist your IP address. We recommend doing this with Terraform.</p> <p>Navigate to <code>main.tf</code> and add the following block at the bottom. Make sure to replace \"IP_PLACEHOLDER\" with your IP address.</p> <pre><code>resource \"aws_vpc_security_group_ingress_rule\" \"helix_auth_service_inbound_personal\" {\n  security_group_id            = module.helix_authentication_service.alb_security_group_id\n  ip_protocol                  = \"TCP\"\n  from_port                    = 443\n  to_port                      = 443\n  cidr_blocks                  = [\"IP_PLACEHOLDER/32\"]\n  description                  = \"Grants personal access Helix Authentication Service.\"\n}\n</code></pre> <p>You will need to run <code>terraform apply</code> to deploy this change.</p> <p>You should now be able to access the Helix Authentication Service's web based UI. Please consult the Helix Authentication Service documentation for guidance on logging in and configuring your external IDP.</p> <ol> <li>Helix Core and Helix Swarm</li> </ol> <p>Helix Core and Helix Swarm are configured to leverage the Helix Authentication Service for sign-in out of the box. However, they are not exposed to the public internet by default. You will need to create rules on the <code>helix core security group</code> and the <code>helix swarm security group</code> that grant personal access. As above, we recommend doing this with Terraform:</p> <p>Navigate to <code>main.tf</code> and add the following block at the bottom. Make sure to replace \"IP_PLACEHOLDER\" with your IP address.</p> <pre><code>resource \"aws_vpc_security_group_ingress_rule\" \"core_inbound_personal\" {\n  security_group_id            = module.perforce_helix_core.security_group_id\n  ip_protocol                  = \"TCP\"\n  from_port                    = 1666\n  to_port                      = 1666\n  cidr_blocks                  = [\"IP_PLACEHOLDER/32\"]\n  description                  = \"Enables personal access to Helix Core.\"\n}\n\nresource \"aws_vpc_security_group_ingress_rule\" \"swarm_inbound_personal\" {\n  security_group_id            = module.perforce_helix_swarm.alb_security_group_id\n  ip_protocol                  = \"TCP\"\n  from_port                    = 443\n  to_port                      = 443\n  cidr_blocks                  = [\"IP_PLACEHOLDER/32\"]\n  description                  = \"Enables personal access to Helix Swarm.\"\n}\n</code></pre> <p>Now that you have access to Helix Core and Helix Swarm you should be able to log in with the super user credentials specified or created during deployment. This will enable you to provision other users that leverage Helix Authentication Service for single-sign-on. For more information please consult the Helix Core documentation.</p>"}]}